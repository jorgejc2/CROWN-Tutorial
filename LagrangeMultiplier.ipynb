{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking inspiration from this [post](https://stackoverflow.com/questions/77508682/correct-way-to-do-lagrange-dual-optimization-pytorch), we will use the PyTorch Adam Optimizer to solve the Lagrangain dual problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from gurobipy import GRB, quicksum, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an optimizer on the linear system of equations:\n",
    "$$\n",
    "x_0 + x_1 - 5 = 0 (eq.1)\n",
    "$$\n",
    "$$\n",
    "2x_0 - x_1 + 3 = 0 (eq.2)\n",
    "$$\n",
    "where the loss is defined to be:\n",
    "$$\n",
    "(eq.1)^2 + (eq.2)^2\n",
    "$$\n",
    "and the Adam optimizer is being used to minimize this loss. \n",
    "The exact solutions are \n",
    "$$\n",
    "x_0 = \\frac{2}{3}\n",
    "$$\n",
    "$$\n",
    "x_1 = \\frac{13}{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 34.0\n",
      "Step 100, Loss: 20.551959991455078\n",
      "Step 200, Loss: 11.665807723999023\n",
      "Step 300, Loss: 6.137031555175781\n",
      "Step 400, Loss: 2.9643564224243164\n",
      "Step 500, Loss: 1.3036264181137085\n",
      "Step 600, Loss: 0.5183063745498657\n",
      "Step 700, Loss: 0.18529455363750458\n",
      "Step 800, Loss: 0.05931048095226288\n",
      "Step 900, Loss: 0.016934897750616074\n",
      "Step 1000, Loss: 0.004297736566513777\n",
      "Step 1100, Loss: 0.0009654018795117736\n",
      "Step 1200, Loss: 0.00019103451631963253\n",
      "Step 1300, Loss: 3.309983731014654e-05\n",
      "Step 1400, Loss: 4.998842086934019e-06\n",
      "Optimized solution: [0.6665166 4.33259  ]\n"
     ]
    }
   ],
   "source": [
    "# Define your system of equations as a function\n",
    "def equations(x):\n",
    "    eq1 = x[0] + x[1] - 5\n",
    "    eq2 = 2*x[0] - x[1] + 3\n",
    "    return eq1, eq2\n",
    "\n",
    "# Initialize the variables \n",
    "x = torch.tensor([0.,0.], requires_grad=True)\n",
    "\n",
    "# Define the Adam optimizer\n",
    "optimizer = Adam([x], lr=0.01, maximize=False)\n",
    "\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-6\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "    # Compute the system of equations\n",
    "    eq1, eq2 = equations(x)\n",
    "    \n",
    "    # Define the loss as the sum of squared equations\n",
    "    loss = eq1**2 + eq2**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the variables\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an optimizer on the quadratic equation:\n",
    "$$\n",
    "y = x_t^2\n",
    "$$\n",
    "where the loss is defined to be:\n",
    "$$\n",
    "y\n",
    "$$\n",
    "and the Adam optimizer is being used to minimize this loss. \n",
    "The exact solution is\n",
    "$$\n",
    "x_t = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t: tensor([[0.9906]], requires_grad=True)\n",
      "Step 0, Loss: 0.981256365776062\n",
      "Step 100, Loss: 0.04736644774675369\n",
      "Step 200, Loss: 0.00020352439605630934\n",
      "Step 300, Loss: 1.983038089292677e-08\n",
      "Optimized solution: [[9.470147e-05]]\n"
     ]
    }
   ],
   "source": [
    "# test on a simple parabola\n",
    "x_t = torch.rand(1,1, requires_grad=True)\n",
    "print(f\"x_t: {x_t}\")\n",
    "optimizer = torch.optim.Adam([x_t], lr=0.01, maximize=False)\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "\n",
    "    loss = x_t**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x_t.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t: tensor([[0.2674]], requires_grad=True)\n",
      "Step 0, Loss: -0.07150442898273468\n",
      "Optimized solution: [[7.715425e-06]]\n"
     ]
    }
   ],
   "source": [
    "# test on a simple parabola\n",
    "x_t = torch.rand(1,1, requires_grad=True)\n",
    "print(f\"x_t: {x_t}\")\n",
    "optimizer = torch.optim.Adam([x_t], lr=0.01, maximize=True)\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "\n",
    "    loss = -x_t**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x_t.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Gurobi will be needed to validate answers later on, here is a simple example using the Gurobi module.\n",
    "\n",
    "minimize $5x + 4y$\n",
    "\n",
    "subject to\n",
    "$$x + y \\geq 8$$\n",
    "$$2x + y \\geq 10$$\n",
    "$$x + 4y \\geq 11$$\n",
    "$$x \\geq 0$$\n",
    "$$y \\geq 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2024-10-28\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 3 rows, 2 columns and 6 nonzeros\n",
      "Model fingerprint: 0x6c7cdc94\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 4e+00]\n",
      "  Objective range  [4e+00, 5e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [8e+00, 1e+01]\n",
      "Presolve time: 0.00s\n",
      "Presolved: 3 rows, 2 columns, 6 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    0.0000000e+00   1.850000e+01   0.000000e+00      0s\n",
      "       2    3.4000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  3.400000000e+01\n",
      "Objective Function Value: 34.000000\n",
      "x: 2\n",
      "y: 6\n"
     ]
    }
   ],
   "source": [
    "from gurobipy import *\n",
    "opt_mod = Model(name = \"simple_linear_program_1\")\n",
    "\n",
    "# add variables\n",
    "x = opt_mod.addVar(name='x', vtype=GRB.CONTINUOUS, lb=0)\n",
    "y = opt_mod.addVar(name='y', vtype=GRB.CONTINUOUS, lb=0)\n",
    "\n",
    "# set the objective function\n",
    "obj_fn = 5*x + 4*y\n",
    "opt_mod.setObjective(obj_fn, GRB.MINIMIZE)\n",
    "\n",
    "# adding the constraints\n",
    "c1 = opt_mod.addConstr(x + y >= 8, name='c1')\n",
    "c2 = opt_mod.addConstr(2*x + y >= 10, name='c2')\n",
    "c3 = opt_mod.addConstr(x + 4*y >= 11, name='c3')\n",
    "\n",
    "# now optimize the problem and save it to a file\n",
    "opt_mod.optimize()\n",
    "opt_mod.write(\"simpe_linear_model_one.lp\")\n",
    "\n",
    "# output the result\n",
    "print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "# Get values of the decision variables\n",
    "for v in opt_mod.getVars():\n",
    "    print('%s: %g' % (v.VarName, v.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use the Adam optimizer to solve a Lagrangian dual problem. The objective function be to minimized is: \n",
    "$$\n",
    "2x_1 + 4x_2 = 0\n",
    "$$\n",
    "And the constraints are: \n",
    "$$\n",
    "-x_1 - 5 = 0\n",
    "$$\n",
    "$$\n",
    "-x_2 - 5 = 0\n",
    "$$\n",
    "The Lagrangian thus becomes:\n",
    "$$\n",
    "L(x, \\lambda) = 2x_1 + 4x_2 + \\lambda_1(-x_1 - 5) + \\lambda_2(-x_2 - 5)\n",
    "$$\n",
    "\n",
    "For my personal purposes, I do not need to modify $x$, solely $\\lambda$, therefore this is not a dual optimization problem but a simple maximization of $\\lambda$. \n",
    "\n",
    "Thus, assuming $x$ is given and does not violate constraints, we will obtain the gradients:\n",
    "$$\n",
    "\\nabla_{x_1}L(x,\\lambda) = 2 - \\lambda_1 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{x_2}L(x,\\lambda) = 4 - \\lambda_2 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{\\lambda_1}L(x,\\lambda) = -x_1 - 5 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{\\lambda_2}L(x,\\lambda) = -x_2 - 5 = 0\n",
    "$$\n",
    "Giving the exact solutions:\n",
    "$$\n",
    "x_1 = -5\n",
    "$$\n",
    "$$\n",
    "x_2 = -5\n",
    "$$\n",
    "$$\n",
    "\\lambda_1 = 2\n",
    "$$\n",
    "$$\n",
    "\\lambda_2 = 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5. -5.  2.  4.]\n"
     ]
    }
   ],
   "source": [
    "# double checking that these are indeed the exact solutions as I stated above\n",
    "A = np.array([[0,0,-1,0],[0,0,0,-1],[-1,0,0,0],[0,-1,0,0]])\n",
    "b = np.array([-2,-4,5,5])\n",
    "x = np.linalg.solve(A,b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[-1.  0.]\n",
      " [ 0. -1.]]\n",
      "b: [[-5.]\n",
      " [-5.]]\n",
      "c: [[2.]\n",
      " [4.]]\n"
     ]
    }
   ],
   "source": [
    "c = np.array([2,4], dtype='float32').reshape(-1,1)\n",
    "n = 2 # input of dimension 2\n",
    "m = 2 # 2 constraints\n",
    "A = np.array([[-1, 0], [0, -1]], dtype='float32')\n",
    "b = np.array([-5,-5], dtype='float32').reshape(-1, 1)\n",
    "print(f\"A: {A}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[-1.  0.]\n",
      " [ 0. -1.]]\n",
      "b: [[-5.]\n",
      " [-5.]]\n",
      "c: [[2.]\n",
      " [4.]]\n",
      "x_t: [[-5.]\n",
      " [-5.]]\n",
      "Init lagrange_multiplier [[0.873101 ]\n",
      " [0.8700457]]\n",
      "lagrangian shape: torch.Size([])\n",
      "Shape objective torch.Size([1, 1]), Shape constraint torch.Size([2, 1])\n",
      "objective: tensor([[-30.]], grad_fn=<MmBackward0>)\n",
      "constraint: tensor([[0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n",
      "lagrangian: -30.0\n",
      "Step 0, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 340000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m lagrangian\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# update values\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mopt_lagrange\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c_t = torch.tensor(c).float()\n",
    "A_t = torch.tensor(A).float()\n",
    "b_t = torch.tensor(b).float().reshape(-1,1)\n",
    "# x_t = torch.rand(n, 1, requires_grad=True)\n",
    "x_t = torch.tensor([-5.0,-5.0], requires_grad=True).float().reshape(-1,1)\n",
    "print(f\"A: {A_t.detach().numpy()}\")\n",
    "print(f\"b: {b_t.detach().numpy()}\")\n",
    "print(f\"c: {c_t.detach().numpy()}\")\n",
    "print(f\"x_t: {x_t.detach().numpy()}\")\n",
    "\n",
    "_lagrange_multiplier = torch.rand(m,1, requires_grad=True)\n",
    "lagrange_multiplier = torch.nn.functional.softplus(_lagrange_multiplier)\n",
    "print(f\"Init lagrange_multiplier {lagrange_multiplier.detach().numpy()}\")\n",
    "\n",
    "# opt_weights = torch.optim.Adam([x_t], lr=0.1)\n",
    "opt_lagrange = torch.optim.Adam([_lagrange_multiplier], lr=0.1, maximize=True)\n",
    "\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "    \n",
    "\n",
    "    objective = c_t.T @ x_t\n",
    "    constraint = A_t @ x_t + b_t\n",
    "    \n",
    "    lagrange_multiplier = torch.nn.functional.softplus(_lagrange_multiplier)\n",
    "    lagrangian = objective + lagrange_multiplier.T @ constraint\n",
    "    lagrangian = lagrangian.squeeze()\n",
    "    if step == 0:\n",
    "        print(f\"lagrangian shape: {lagrangian.shape}\")\n",
    "        print(f\"Shape objective {objective.shape}, Shape constraint {constraint.shape}\")\n",
    "        print(f\"objective: {objective}\")\n",
    "        print(f\"constraint: {constraint}\")\n",
    "        print(f\"lagrangian: {lagrangian}\")\n",
    "\n",
    "        # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {lagrangian.item()}, lambda_1: {_lagrange_multiplier[0]}, lambda_2: {_lagrange_multiplier[1]}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(lagrangian.item()) < loss_threshold:\n",
    "        break\n",
    "        \n",
    "    # zero the gradient\n",
    "    opt_lagrange.zero_grad()\n",
    "\n",
    "    # compute the gradient\n",
    "    lagrangian.backward()\n",
    "\n",
    "    # update values\n",
    "    opt_lagrange.step()\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: -5.0 is leaf True, x2: -5.0 is leaf True, x3: tensor([0.0566], requires_grad=True) is leaf True, x4: tensor([0.8357], requires_grad=True) is leaf True\n",
      "Optimized x1: -5.0\n",
      "Optimized x2: -5.0\n",
      "Optimized x3: 0.05660974979400635\n",
      "Optimized x4: 0.835723876953125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9e002a050>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Adam\n",
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.tensor(-5.0, requires_grad=False)\n",
    "x2_opt = torch.tensor(-5.0, requires_grad=False)\n",
    "x3_opt = torch.rand(1, requires_grad=True)\n",
    "x4_opt = torch.rand(1, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 1000\n",
    "lr = 0.1\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "opt = torch.optim.Adam([x3_opt, x4_opt], lr=lr, maximize=True)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, 0.98)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt).sum()\n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    y.backward()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    # clip lagrange values\n",
    "    x3_opt.data = torch.clip(x3_opt.data, 0)\n",
    "    x4_opt.data = torch.clip(x4_opt.data, 0)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([0.6116], dtype=torch.float64, requires_grad=True) is leaf True, x2: tensor([0.0295], dtype=torch.float64, requires_grad=True) is leaf True, x3: tensor([0.1955], dtype=torch.float64, requires_grad=True) is leaf True, x4: tensor([0.9048], dtype=torch.float64, requires_grad=True) is leaf True\n",
      "Optimized x1: -4.55081800782109\n",
      "Optimized x2: -5.225714517952417\n",
      "Optimized x3: 0.0\n",
      "Optimized x4: 0.29730698177256015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fda17360160>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Adam\n",
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x2_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x3_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x4_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 10000 + 20\n",
    "lr = 0.1\n",
    "\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "opt_duals = torch.optim.Adam([x3_opt, x4_opt], lr=lr, maximize=True)\n",
    "opt_x = torch.optim.Adadelta([x1_opt, x2_opt], lr=lr, maximize=False)\n",
    "scheduler_dual = torch.optim.lr_scheduler.ExponentialLR(opt_duals, 0.98)\n",
    "scheduler_x = torch.optim.lr_scheduler.ExponentialLR(opt_x, 0.98)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt).sum()\n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt_x.zero_grad(set_to_none=True)\n",
    "    opt_duals.zero_grad(set_to_none=True)\n",
    "    y.backward()\n",
    "\n",
    "    if flip:\n",
    "        opt_x.step()\n",
    "        # if step % 20 == 0:\n",
    "        #     scheduler_x.step()\n",
    "    else:\n",
    "        opt_duals.step()\n",
    "        # if step % 20 == 0:\n",
    "        #     scheduler_dual.step()\n",
    "    \n",
    "    # clip lagrange values\n",
    "    x3_opt.data = torch.clip(x3_opt.data, 0)\n",
    "    x4_opt.data = torch.clip(x4_opt.data, 0)\n",
    "\n",
    "    if step != 0 and step % 60 == 0:\n",
    "        flip = not flip\n",
    "    \n",
    "    if loss_graph[1, step] < -29.5 and loss_graph[1,step] > -30.5 and flip == False:\n",
    "        loss_graph[1, step:] = loss_graph[1,step]\n",
    "        break\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([0.7194], requires_grad=True) is leaf True, x2: tensor([0.2687], requires_grad=True) is leaf True, x3: tensor([0.7701], requires_grad=True) is leaf True, x4: tensor([0.8334], requires_grad=True) is leaf True\n",
      "Optimized x1: -4.999998092651367\n",
      "Optimized x2: -4.999998092651367\n",
      "Optimized x3: 1.9999995231628418\n",
      "Optimized x4: 3.9999990463256836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdffc1509d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1, requires_grad=True)\n",
    "x2_opt = torch.rand(1, requires_grad=True)\n",
    "x3_opt = torch.rand(1, requires_grad=True)\n",
    "x4_opt = torch.rand(1, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "def zero_grad(parameters):\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 1000\n",
    "lr = 0.1\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt)\n",
    "    y.backward()\n",
    "\n",
    "    grad1 = x1_opt.grad if x1_opt.grad is not None else 0.0\n",
    "    grad2 = x2_opt.grad if x2_opt.grad is not None else 0.0\n",
    "    grad3 = x3_opt.grad if x3_opt.grad is not None else 0.0\n",
    "    grad4 = x4_opt.grad if x4_opt.grad is not None else 0.0\n",
    "\n",
    "    loss_graph[1, step] = y.item()\n",
    "    \n",
    "    if flip:\n",
    "        # when this is true, we are minimizing L(x,lambda) w.r.t. x\n",
    "        x1_opt.data = (x1_opt.data + lr*grad3).requires_grad_(True)\n",
    "        x2_opt.data = (x2_opt.data + lr*grad4).requires_grad_(True)        \n",
    "\n",
    "    else:\n",
    "        # when this is false, we are maximizing L(x,lambda) w.r.t. lambda\n",
    "        x3_opt.data = torch.clamp(x3_opt.data + lr*grad1, min=0.0).requires_grad_(True)\n",
    "        x4_opt.data = torch.clamp(x4_opt.data + lr*grad2, min=0.0).requires_grad_(True)\n",
    "\n",
    "    # if step != 0 and (step % 100) == 0:\n",
    "    #     print(f\"Step {step}, Loss: {y.item():.4f}, x1: {x1_opt.detach().numpy()[0]:.4f} x2: {x2_opt.detach().numpy()[0]:.4f} lambda_1: {x3_opt.detach().numpy()[0]:.4f}, lambda_2: {x4_opt.detach().numpy()[0]:.4f}, grads: [{x1_opt.grad.detach().numpy()[0]:.4f}, {x2_opt.grad.detach().numpy()[0]:.4f}, {x3_opt.grad.detach().numpy()[0]:.4f}, {x4_opt.grad.detach().numpy()[0]:.4f}]\")\n",
    "\n",
    "    if step != 0 and (step % 100) == 0:\n",
    "        flip = not flip\n",
    "        \n",
    "    # zero out the gradients\n",
    "    zero_grad([x1_opt, x2_opt, x3_opt, x4_opt])\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x1: -0.8333324193954468\n",
      "Optimized x2: -0.8333352208137512\n",
      "Optimized l1: 1.045896053314209\n",
      "Optimized l2: 0.2871112823486328\n",
      "Optimized l3: 0.0\n",
      "Optimized l4: 1.4137334823608398\n",
      "Optimized l5: 0.5862621068954468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdce8c83550>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1, requires_grad=True)\n",
    "x2_opt = torch.rand(1, requires_grad=True)\n",
    "l1_opt = torch.rand(1, requires_grad=True)\n",
    "l2_opt = torch.rand(1, requires_grad=True)\n",
    "l3_opt = torch.rand(1, requires_grad=True)\n",
    "l4_opt = torch.rand(1, requires_grad=True)\n",
    "l5_opt = torch.rand(1, requires_grad=True)\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, l1, l2, l3, l4, l5):\n",
    "    return x1 + 2*x2 + l1*(x1-2) + l2*(-x1-2) + l3*(x2-2) + l4*(-x2-2) + l5*(-3*x1 - x2 - 5)\n",
    "\n",
    "def zero_grad(parameters):\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 10000\n",
    "lr = 0.01\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, l1_opt, l2_opt, l3_opt, l4_opt, l5_opt)\n",
    "    y.backward()\n",
    "\n",
    "    x_grad1 = x1_opt.grad if x1_opt.grad is not None else 0.0\n",
    "    x_grad2 = x2_opt.grad if x2_opt.grad is not None else 0.0\n",
    "    l_grad1 = l1_opt.grad if l1_opt.grad is not None else 0.0\n",
    "    l_grad2 = l2_opt.grad if l2_opt.grad is not None else 0.0\n",
    "    l_grad3 = l3_opt.grad if l3_opt.grad is not None else 0.0\n",
    "    l_grad4 = l4_opt.grad if l4_opt.grad is not None else 0.0\n",
    "    l_grad5 = l5_opt.grad if l5_opt.grad is not None else 0.0\n",
    "\n",
    "    loss_graph[1, step] = y.item()\n",
    "    \n",
    "    if flip:\n",
    "        # when this is true, we are minimizing L(x,lambda) w.r.t. x\n",
    "        x1_opt.data = (x1_opt.data + lr*(-l_grad1 + l_grad2 + l_grad5)).requires_grad_(True)\n",
    "        x2_opt.data = (x2_opt.data + lr*(-l_grad3 + l_grad4 + l_grad5)).requires_grad_(True)        \n",
    "\n",
    "    else:\n",
    "        # when this is false, we are maximizing L(x,lambda) w.r.t. lambda\n",
    "        l1_opt.data = torch.clamp(l1_opt.data + lr*(-x_grad1), min=0.0).requires_grad_(True)\n",
    "        l2_opt.data = torch.clamp(l2_opt.data + lr*(x_grad1), min=0.0).requires_grad_(True)\n",
    "        l3_opt.data = torch.clamp(l3_opt.data + lr*(-x_grad2), min=0.0).requires_grad_(True)\n",
    "        l4_opt.data = torch.clamp(l4_opt.data + lr*(x_grad2), min=0.0).requires_grad_(True)\n",
    "        l5_opt.data = torch.clamp(l5_opt.data + lr*(x_grad1 + x_grad2), min=0.0).requires_grad_(True)\n",
    "\n",
    "    if step != 0 and (step % 100) == 0:\n",
    "        flip = not flip\n",
    "        \n",
    "    # zero out the gradients\n",
    "    zero_grad([x1_opt, x2_opt, l1_opt, l2_opt, l3_opt, l4_opt, l5_opt])\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "l1_optimized = l1_opt.item()\n",
    "l2_optimized = l2_opt.item()\n",
    "l3_optimized = l3_opt.item()\n",
    "l4_optimized = l4_opt.item()\n",
    "l5_optimized = l5_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized l1:\", l1_optimized)\n",
    "print(\"Optimized l2:\", l2_optimized)\n",
    "print(\"Optimized l3:\", l3_optimized)\n",
    "print(\"Optimized l4:\", l4_optimized)\n",
    "print(\"Optimized l5:\", l5_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am checking the results of the Lagrange problem above by computing the upper and lower bounds on x as well as the minimal and maximal pertubation on each logit output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Model fingerprint: 0xf5f961a3\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [3e+00, 4e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Presolve removed 5 rows and 2 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    1.6000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.600000000e+01\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 2e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -3.0000000e+30   3.000000e+30   3.000000e+00      0s\n",
      "       3   -5.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 3 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -5.000000000e+00\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [3e+00, 4e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    7.0000000e+30   2.000000e+30   7.000000e+00      0s\n",
      "       2    1.6000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.600000000e+01\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 3e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -4.0000000e+30   3.000000e+30   4.000000e+00      0s\n",
      "       3   -5.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 3 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -5.000000000e+00\n",
      "logit 1 is bounded s.t. -5.0 <= z(x) <= 16.0 with lb pertubation (-1.0, -2.0) and ub pertubation (2.0, 2.0)\n",
      "logit 2 is bounded s.t. -5.0 <= z(x) <= 16.0 with lb pertubation (-1.0, -2.0) and ub pertubation (2.0, 2.0)\n"
     ]
    }
   ],
   "source": [
    "W_ub = np.array([[4,3],[4,3]])\n",
    "b_ub = np.array([2,2])\n",
    "W_lb = np.array([[1,2],[1,1]])\n",
    "# b_lb = np.array([1,1])\n",
    "b_lb = np.zeros(2)\n",
    "# using Gurobi to solve the same problem as above\n",
    "opt_mod = Model(name = \"simple_linear_program_2\")\n",
    "\n",
    "# add variables\n",
    "inputs = np.array(list(opt_mod.addVars(W_ub.shape[1], name=\"x\", lb=float(\"-inf\"), ub=float(\"inf\")).values()))\n",
    "\n",
    "# adding the constraints\n",
    "c1 = opt_mod.addConstr(inputs[0] - 2 <= 0, name='c1') # these four constraints are the l_inf norm box constraints\n",
    "c2 = opt_mod.addConstr(-inputs[0] - 2 <= 0, name='c2')\n",
    "c3 = opt_mod.addConstr(inputs[1] - 2 <= 0, name='c3')\n",
    "c4 = opt_mod.addConstr(-inputs[1] - 2 <= 0, name='c4')\n",
    "c5 = opt_mod.addConstr(-3*inputs[0] - inputs[1] - 5 <= 0, name='c5') # this constraint is a line constraint cutting through the box\n",
    "\n",
    "worst_case_inputs_ub = []\n",
    "worst_case_inputs_lb = []\n",
    "upper_bounds = []\n",
    "lower_bounds = []\n",
    "\n",
    "# set the objective function for each logit\n",
    "for idx in range(2*W_ub.shape[0]):\n",
    "    i = idx // 2\n",
    "    if idx % 2 == 0:\n",
    "        obj_fn = quicksum([W_ub[i,j]*inputs[j] for j in range(W_ub.shape[1])]) + b_ub[i]\n",
    "        opt_mod.setObjective(obj_fn, GRB.MAXIMIZE)\n",
    "    else:\n",
    "        obj_fn = quicksum([W_lb[i,j]*inputs[j] for j in range(W_lb.shape[1])]) + b_lb[i]\n",
    "        opt_mod.setObjective(obj_fn, GRB.MINIMIZE)\n",
    "\n",
    "    # now optimize the problem and save it to a file\n",
    "    opt_mod.optimize()\n",
    "    # opt_mod.write(\"scenario_one_upperbound_logit_one.lp\")\n",
    "\n",
    "    # output the result\n",
    "    # print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "    if idx % 2 == 0:\n",
    "        upper_bounds.append(opt_mod.ObjVal)\n",
    "    else:\n",
    "        lower_bounds.append(opt_mod.ObjVal)\n",
    "    # Get values of the decision variables\n",
    "    temp_inputs = []\n",
    "    for v in opt_mod.getVars():\n",
    "        # print('%s: %g' % (v.VarName, v.x))\n",
    "        temp_inputs.append(v.x)\n",
    "\n",
    "    if idx % 2 == 0:\n",
    "        worst_case_inputs_ub.append(tuple(temp_inputs)) # append the worst case input that caused this maximal pertubation\n",
    "    else:\n",
    "        worst_case_inputs_lb.append(tuple(temp_inputs)) # append the worst case input that caused this maximal pertubation\n",
    "    \n",
    "for i in range(W_ub.shape[0]):\n",
    "    print(f\"logit {i + 1} is bounded s.t. {lower_bounds[i]} <= z(x) <= {upper_bounds[i]} with lb pertubation {worst_case_inputs_lb[i]} and ub pertubation {worst_case_inputs_ub[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal Result:\n",
      "       message: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n",
      "       success: True\n",
      "        status: 0\n",
      "           fun: 0.0\n",
      "             x: [ 0.000e+00  0.000e+00]\n",
      "           nit: 0\n",
      "         lower:  residual: [ 0.000e+00  0.000e+00]\n",
      "                marginals: [ 2.000e+00  3.000e+00]\n",
      "         upper:  residual: [       inf        inf]\n",
      "                marginals: [ 0.000e+00  0.000e+00]\n",
      "         eqlin:  residual: []\n",
      "                marginals: []\n",
      "       ineqlin:  residual: [ 1.000e+00  2.000e+00]\n",
      "                marginals: [-0.000e+00 -0.000e+00]\n",
      "\n",
      "Dual Result:\n",
      "       message: The problem is unbounded. (HiGHS Status 10: model_status is Unbounded; primal_status is At upper bound)\n",
      "       success: False\n",
      "        status: 3\n",
      "           fun: None\n",
      "             x: None\n",
      "           nit: 0\n",
      "         lower:  residual: None\n",
      "                marginals: None\n",
      "         upper:  residual: None\n",
      "                marginals: None\n",
      "         eqlin:  residual: None\n",
      "                marginals: None\n",
      "       ineqlin:  residual: None\n",
      "                marginals: None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# Primal linear program coefficients\n",
    "c = np.array([2, 3])\n",
    "A = np.array([[1, -1], [3, 1]])\n",
    "b = np.array([1, 2])\n",
    "\n",
    "# Solve the primal linear program\n",
    "result_primal = linprog(c, A_ub=A, b_ub=b, method='highs')\n",
    "\n",
    "# Display the primal result\n",
    "print(\"Primal Result:\")\n",
    "print(result_primal)\n",
    "\n",
    "# Dual linear program coefficients\n",
    "c_dual = -b  # Coefficients are negated for maximization\n",
    "A_dual = -A.T  # Transpose of A with negation\n",
    "b_dual = c  # Dual variables corresponding to the inequality constraints\n",
    "\n",
    "# Solve the dual linear program\n",
    "result_dual = linprog(c_dual, A_ub=A_dual, b_ub=b_dual, method='highs')\n",
    "\n",
    "# Display the dual result\n",
    "print(\"\\nDual Result:\")\n",
    "print(result_dual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [1.25 0.  ]\n",
      " [2.5  0.  ]\n",
      " [3.75 0.  ]\n",
      " [5.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# potentially look into this for solving the lagrange dual, but it requires the gradients to be determined beforehand\n",
    "from scipy.optimize import fsolve, fmin_l_bfgs_b\n",
    "\n",
    "a = 1\n",
    "nbtests = 5\n",
    "minmu = 0\n",
    "maxmu = 5\n",
    "\n",
    "def lagrange(x, mu):\n",
    "    return x**2 + mu * (np.exp(x) + x - a)\n",
    "\n",
    "def lagrange_grad(x, mu):\n",
    "    grad_x = 2*x + mu * (np.exp(x) + 1)\n",
    "    grad_mu = np.exp(x) + x - a\n",
    "    return grad_x, grad_mu\n",
    "\n",
    "def dual(mu):\n",
    "    x = fsolve(lambda x: lagrange_grad(x, mu)[0], x0=1)\n",
    "    obj_val = lagrange(x, mu)\n",
    "    grad = lagrange_grad(x, mu)[1]\n",
    "    return -1.0*obj_val, -1.0*grad\n",
    "\n",
    "pl = np.empty((nbtests, 2))\n",
    "for i, nu in enumerate(np.linspace(minmu,maxmu,nbtests)):\n",
    "    res = fmin_l_bfgs_b(dual, x0=nu, bounds=[(0,None)], factr=1e6)\n",
    "    mu_opt = res[0]\n",
    "    x_opt = fsolve(lambda x: lagrange_grad(x, mu_opt)[0], x0=1)\n",
    "    pl[i] = [nu, *x_opt]\n",
    "print(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last example, we will deal with a single dimension input and output. The input x will have a pertubation of $\\epsilon=2$ and be fed to a ReLU unit, then be added by 5. This will then be the final output. This first example will have no constraints, and if alpha is tuned correctly, it should be zero s.t. alpha-CROWN produces a lower bound of 0. The actualy lower and upper bound is 5 and 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same example as before but with the restriction that $x\\geq1$. We will add a Lagrange Multiplier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximizing $\\lambda$ after minimizing x. \n",
    "For a ReLU activation function, we may have an example where we wish to minimize $\\sigma(x)$ where $\\sigma$ is the ReLU function. If x is convex s.t. $-2\\leq x \\leq 2$, we may choose the lower bound on the input to be x and we will naturally find that the objection function is 0. The main issue is that ReLU is non-convex in nature, therefore we take inspiration from $\\alpha$-CROWN and use the linear lower bound $\\alpha$x to lower bound the ReLU function. This is the linear constraint that is used in the triangular relaxation of ReLU. Letting $alpha=1$ (note that we must have 0\\leq \\alpha \\leq 1) and adding constraints, $x \\geq -1$ and $x \\leq 2$, we now solve the dual program, $L(x, \\lambda)$. First minimize x and then maximize $\\lambda$. x exists in the space $-2 \\leq x \\leq 2$, therefore we know that the smallest value of x is x=-2. Though this x does not fit our constraints, if $\\lambda$ is optimal, we get the optimal objective value. Note that this minimum value will be -1 and not 0 because of the ReLU relaxation. The true minimum value is 0. \n",
    "\n",
    "## Question\n",
    "Ask Zhang if it is reasonable to choose route 1 where clipping and projection is included in the projection as this leads to tigher bounds yet still valid bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Lambda: tensor([[0.8509],\n",
      "        [0.0250]])\n",
      "Initial \u0007lpha: tensor([[1.]])\n",
      "Optimized lambda: tensor([[0.],\n",
      "        [1.]])\n",
      "Optimized alpha: tensor([[1.]])\n",
      "CROWN lower bound: -2.0, Lagrange lower bound: -1.0000001192092896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb1bf784c10>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.tensor([[-2]]).float()\n",
    "lambda_ = torch.rand(2,1, requires_grad=True)\n",
    "c = torch.tensor([[1]]).float()\n",
    "# c = torch.rand(1,1, requires_grad=True)\n",
    "# with torch.no_grad():\n",
    "#     c.data = torch.clamp(c.data, min=0, max=1)\n",
    "G = torch.tensor([[1],[-1]]).float()\n",
    "h = torch.tensor([[2],[1]]).float()\n",
    "\n",
    "print(f\"Initial Lambda: {lambda_.data}\\nInitial \\alpha: {c}\")\n",
    "\n",
    "def constraint(v, G, c):\n",
    "    return -G.T @ v - c\n",
    "\n",
    "def project_onto_lambda(v, G, c):\n",
    "    # using the formula x = z - A.T(AA.T)^-1(Az-b) to project v onto the set Ax=b\n",
    "    # in this example, A = -G.T, b = c\n",
    "    projection = v + G @ torch.linalg.pinv(G.T@G) @ (-G.T @ v - c)\n",
    "    min_val = torch.min(projection)\n",
    "    if min_val < 0.0:\n",
    "        projection -= min_val\n",
    "    return projection\n",
    "    \n",
    "def obj_fn(x, lambda_, A, b, c):\n",
    "    return c.T@x + lambda_.T@(A@x - b)\n",
    "\n",
    "# Number of optimization steps\n",
    "lr = 0.1\n",
    "num_steps = 20\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "lambda_vals = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "lambda_vals = np.vstack((lambda_vals, np.zeros((3, num_steps))))\n",
    "\n",
    "# opt = torch.optim.Adam([lambda_, c], lr=lr, maximize=True)\n",
    "opt = torch.optim.Adam([lambda_], lr=lr, maximize=True)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, 0.98)\n",
    "\n",
    "## Though this converges much more tightly, it is frowned upon\n",
    "# Optimization loop\n",
    "# for step in range(num_steps):\n",
    "\n",
    "#     # y = obj_fn(x, lambda_, G, h, c)\n",
    "#     c.data = torch.clamp(c.data, min=0, max=1)\n",
    "#     projected_lambda = project_onto_lambda(lambda_, G, c)\n",
    "#     lambda_vals[1:3,step] = projected_lambda.detach().clone().numpy().flatten()\n",
    "#     y = obj_fn(x, projected_lambda, G, h, c)\n",
    "\n",
    "#     loss_graph[1, step] = y.item()\n",
    "\n",
    "#     opt.zero_grad(set_to_none=True)\n",
    "\n",
    "#     y.backward()\n",
    "\n",
    "#     opt.step()\n",
    "#     scheduler.step()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # c.data = torch.clamp(c.data, min=0, max=1)\n",
    "#         lambda_vals[3, step] = c.detach().clone().numpy().flatten()\n",
    "\n",
    "## Projections and constraints should not be included in the Adam optimizer\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    y = obj_fn(x, lambda_, G, h, c)\n",
    "    \n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    y.backward()\n",
    "\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lambda_.data = project_onto_lambda(lambda_, G, c)\n",
    "        lambda_vals[1:3,step] = lambda_.detach().clone().numpy().flatten()\n",
    "        \n",
    "    # with torch.no_grad():\n",
    "    #     c.data = torch.clamp(c.data, min=0, max=1)\n",
    "    #     lambda_vals[3, step] = c.detach().clone().numpy().flatten()\n",
    "\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "# lambda_optimized = lambda_.data\n",
    "lambda_optimized = project_onto_lambda(lambda_, G, c).data\n",
    "alpha_optimize = c.data\n",
    "\n",
    "print(\"Optimized lambda:\", lambda_optimized)\n",
    "print(\"Optimized alpha:\", alpha_optimize)\n",
    "\n",
    "print(f\"CROWN lower bound: {obj_fn(x, torch.zeros(2,1).float(), G, h, c).squeeze()}, Lagrange lower bound: {loss_graph[1,-1]}\")\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(f\"Optimization Lambda (converged to {loss_graph[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(f\"\\Lambda_1 (converged to {lambda_vals[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[1,:])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(f\"\\Lambda_2 (converged to {lambda_vals[2,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[2,:])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(f\"\\alpha (converged to {lambda_vals[3,-1]:1.1f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2024-10-28\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 10 nonzeros\n",
      "Model fingerprint: 0xd3257bed\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-01, 5e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [5e+00, 3e+01]\n",
      "Presolve removed 2 rows and 0 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 3 rows, 2 columns, 6 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -1.5200000e+01   1.216983e+01   0.000000e+00      0s\n",
      "       2   -7.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.01 seconds (0.00 work units)\n",
      "Optimal objective -7.000000000e+00\n",
      "Objective Function Value: -7.000000\n",
      "x[0,0]: -2\n",
      "x[1,0]: -5\n",
      "Printing inequality constraint dual variables:\n",
      " [[ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.62790698]\n",
      " [-0.37209302]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# tightest lower bound with  constraints\n",
    "opt_mod = Model(name=\"lower_bound_model\")\n",
    "\n",
    "# constraints\n",
    "G = np.array([[-2/9, 1],[-6/5, 1], [-5/3, -1], [1/8, -1], [5, 1]])\n",
    "h = np.array([[46/9], [6], [25/3], [19/4], [26]])\n",
    "\n",
    "x = opt_mod.addMVar(shape=(2,1), name='x', lb=float('-inf'), ub=float('inf'))\n",
    "\n",
    "# adding inequality constraints\n",
    "ineq_c = opt_mod.addConstr(G @ x <= h, name='c0')\n",
    "\n",
    "opt_mod.setObjective(x[0,0] + x[1,0], GRB.MINIMIZE)\n",
    "\n",
    "opt_mod.optimize()\n",
    "\n",
    "# output the result\n",
    "print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "# Get values of the decision variables\n",
    "for v in opt_mod.getVars():\n",
    "    print('%s: %g' % (v.VarName, v.x))\n",
    "\n",
    "ineq_pi = ineq_c.Pi\n",
    "print(f\"Printing inequality constraint dual variables:\\n {ineq_pi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 10 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-01, 5e+00]\n",
      "  Objective range  [5e-01, 5e-01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [5e+00, 3e+01]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    5.4545455e+29   2.125000e+30   5.454545e-01      0s\n",
      "       2    1.0909091e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.090909091e+01\n",
      "Objective Function Value: 10.909091\n",
      "x[0,0]: 4\n",
      "x[1,0]: 6\n",
      "Printing inequality constraint dual variables:\n",
      " [[0.41779497]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.12765957]]\n"
     ]
    }
   ],
   "source": [
    "opt_mod.setObjective((12/22)*(x[0,0] + x[1,0] + 10), GRB.MAXIMIZE)\n",
    "\n",
    "opt_mod.optimize()\n",
    "\n",
    "# output the result\n",
    "print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "# Get values of the decision variables\n",
    "for v in opt_mod.getVars():\n",
    "    print('%s: %g' % (v.VarName, v.x))\n",
    "\n",
    "ineq_pi = ineq_c.Pi\n",
    "print(f\"Printing inequality constraint dual variables:\\n {ineq_pi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(constraint(torch.tensor([[0.],[0.], [0.62790698], [0.37209302], [0.]]), G, lower_c).detach().clone().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized lambda: tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [5.2201],\n",
      "        [0.0000],\n",
      "        [0.0000]]) tensor([[ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-4.2369]])\n",
      "CROWN lower bound: -10.0, Lagrange lower bound: 16.100540161132812\n",
      "CROWN upper bound: 12.0, Lagrange upper bound: -30.36902618408203\n",
      "last_lower_loss 16.03287696838379\n",
      "last_upper_loss30.233699798583984\n",
      "last_lower_lambda[[0.       ]\n",
      " [0.       ]\n",
      " [5.2065754]\n",
      " [0.       ]\n",
      " [0.       ]]\n",
      "last_upper_lambda[[ 0.     ]\n",
      " [ 0.     ]\n",
      " [ 0.     ]\n",
      " [ 0.     ]\n",
      " [-4.22337]]\n",
      "lower bound penalth0\n",
      "upper bound penalty0\n",
      "w: tensor([[0.0010],\n",
      "        [0.0000],\n",
      "        [0.4016],\n",
      "        [0.4034],\n",
      "        [0.0062]])\n",
      "w: tensor([[0.0005],\n",
      "        [0.0000],\n",
      "        [0.2190],\n",
      "        [0.2200],\n",
      "        [0.0034]])\n",
      "Last projection of lower bound: tensor([[0.0063],\n",
      "        [0.0000],\n",
      "        [2.5982],\n",
      "        [2.6097],\n",
      "        [0.0399]], grad_fn=<ClampBackward1>) with constraint: tensor([[2.8062],\n",
      "        [4.1618]], grad_fn=<SubBackward0>)\n",
      "Last projection of upper bound: tensor([[-7.8277e-05],\n",
      "        [-0.0000e+00],\n",
      "        [-3.2353e-02],\n",
      "        [-3.2496e-02],\n",
      "        [-4.9635e-04]], grad_fn=<ClampBackward1>) with constraint: tensor([[-0.5928],\n",
      "        [-0.6097]], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9676/2430135428.py:5: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w = torch.from_numpy(np.linalg.lstsq(-G.detach().clone().numpy().T, c.detach().clone().numpy())[0].reshape(-1,1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7ddf28b490>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def constraint(v, G, c):\n",
    "    return -G.T @ v - c\n",
    "\n",
    "def project_onto_lambda(v, G, c, d):\n",
    "    w = torch.from_numpy(np.linalg.lstsq(-G.detach().clone().numpy().T, c.detach().clone().numpy())[0].reshape(-1,1))\n",
    "    min_w_entry = torch.min(w)\n",
    "    if min_w_entry < 0:\n",
    "        w = w - min_w_entry\n",
    "    print(f\"w: {w}\")\n",
    "    # print(constraint(w, G, c))\n",
    "    projection = ((w.T@v)/(w.T@w))*w\n",
    "\n",
    "    # using the formula x = z - A.T(AA.T)^-1(Az-b) to project v onto the set Ax=b\n",
    "    # in this example, A = -G.T, b = c\n",
    "    # if np.linalg.det(G.T@G) != 0:\n",
    "    #     projection = v + G @ torch.linalg.inv(G.T@G) @ (-G.T @ v - c)\n",
    "    # else:\n",
    "    #     projection = v + G @ torch.linalg.pinv(G.T@G) @ (-G.T @ v - c)\n",
    "\n",
    "    return projection\n",
    "\n",
    "def obj_fn(x, d, lambda_, A, b, c, ub=False):\n",
    "    if ub:\n",
    "        return -1*(c.T@x + d) + lambda_.T@(A@x - b)\n",
    "    else:\n",
    "        return c.T@x + d + lambda_.T@(A@x - b)\n",
    "    \n",
    "def get_penalty(lambda_, G, c, scale=5, lb=True):\n",
    "    if lb:\n",
    "        return 0 #-scale*(torch.sum(torch.abs(constraint(lambda_, G, c))) )# + torch.sum(torch.abs(torch.clamp(lambda_, max=0.0))))\n",
    "    else:\n",
    "        return 0 #-scale*(torch.sum(torch.abs(constraint(lambda_, G, c))) )# + torch.sum(torch.abs(torch.clamp(lambda_, min=0.0))))\n",
    "\n",
    "lower_x = torch.tensor([[-5], [-5]]).float()\n",
    "upper_x = torch.tensor([[6], [6]]).float()\n",
    "lambda_lower = torch.rand(5,1, requires_grad=True)\n",
    "lambda_upper = torch.rand(5,1, requires_grad=True)\n",
    "lower_c = torch.tensor([[1], [1]]).float()\n",
    "lower_d = torch.tensor([[0]]).float()\n",
    "upper_c = torch.tensor([[12/22],[12/22]]).float()\n",
    "upper_d = torch.tensor([[120/22]]).float()\n",
    "G = torch.tensor([[-2/9, 1],[-6/5, 1], [-5/3, -1], [1/8, -1], [5, 1]]).float()\n",
    "h = torch.tensor([[46/9], [6], [25/3], [19/4], [26]]).float()\n",
    "\n",
    "# print(f\"Initial Lower Lambda: {lambda_lower.data}\\nInitial Upper Lambda: {lambda_upper.data}\\nInitial lower \\alpha: {lower_c}\\n Initial upper \\alpha: {upper_c}\")\n",
    "\n",
    "# Number of optimization steps\n",
    "lr = 0.1\n",
    "num_steps = 100\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "lambda_vals = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "lambda_vals = np.vstack((lambda_vals, np.zeros((3, num_steps))))\n",
    "\n",
    "# opt = torch.optim.Adam([lambda_, c], lr=lr, maximize=True)\n",
    "opt_lower = torch.optim.Adam([lambda_lower], lr=lr, maximize=True)\n",
    "opt_upper = torch.optim.Adam([lambda_upper], lr=lr, maximize=True)\n",
    "scheduler_lower = torch.optim.lr_scheduler.ExponentialLR(opt_lower, 0.98)\n",
    "scheduler_upper = torch.optim.lr_scheduler.ExponentialLR(opt_upper, 0.98)\n",
    "\n",
    "## Projections and constraints should not be included in the Adam optimizer\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    lower_y = obj_fn(lower_x, lower_d, lambda_lower, G, h, lower_c) + get_penalty(lambda_lower, G, lower_c, lb=True)\n",
    "    upper_y = -1*(obj_fn(upper_x, upper_d, lambda_upper, G, h, upper_c) - get_penalty(lambda_upper, G, upper_c, lb=False))\n",
    "\n",
    "    \n",
    "    loss_graph[1, step] = lower_y.item()\n",
    "\n",
    "    if step == num_steps - 1:\n",
    "        last_lower_loss = lower_y.item()\n",
    "        last_upper_loss = upper_y.item()\n",
    "        last_lower_lambda = lambda_lower.detach().clone().numpy()\n",
    "        last_upper_lambda = lambda_upper.detach().clone().numpy()\n",
    "\n",
    "    opt_lower.zero_grad(set_to_none=True)\n",
    "    opt_upper.zero_grad(set_to_none=True)\n",
    "\n",
    "    lower_y.backward()\n",
    "    upper_y.backward()\n",
    "\n",
    "    opt_lower.step()\n",
    "    opt_upper.step()\n",
    "    scheduler_lower.step()\n",
    "    scheduler_upper.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        lambda_upper.data = torch.clamp(lambda_upper.data, max=0.0)\n",
    "        lambda_lower.data = torch.clamp(lambda_lower.data, min=0.0)\n",
    "        lambda_vals[1:3,step] = lambda_lower.detach().clone().numpy().flatten()[:2]\n",
    "        \n",
    "# lambda_lower.data = project_onto_lambda(lambda_lower.data, G, lower_c,0)\n",
    "lambda_lower_optimized = lambda_lower.data\n",
    "lambda_upper_optimized = lambda_upper.data\n",
    "# alpha_optimize = c.data\n",
    "\n",
    "print(\"Optimized lambda:\", lambda_lower_optimized, lambda_upper_optimized)\n",
    "# print(\"Optimized alpha:\", alpha_optimize)\n",
    "\n",
    "print(f\"CROWN lower bound: {obj_fn(lower_x, lower_d, torch.zeros(5,1).float(), G, h, lower_c).squeeze()}, Lagrange lower bound: {obj_fn(lower_x, lower_d, lambda_lower, G, h, lower_c).squeeze()}\")\n",
    "print(f\"CROWN upper bound: {obj_fn(upper_x, upper_d, torch.zeros(5,1).float(), G, h, upper_c).squeeze()}, Lagrange upper bound: {obj_fn(upper_x, upper_d, lambda_upper, G, h, upper_c).squeeze()}\")\n",
    "\n",
    "print(f\"last_lower_loss {last_lower_loss}\\nlast_upper_loss{last_upper_loss}\\nlast_lower_lambda{last_lower_lambda}\\nlast_upper_lambda{last_upper_lambda}\\nlower bound penalth{get_penalty(lambda_lower, G, lower_c, lb=True, scale=1)}\\nupper bound penalty{get_penalty(lambda_upper, G, upper_c, lb=False, scale=1)}\")\n",
    "# assert last_lower_loss <= -7, f\"Last lower loss was {last_lower_loss}\"\n",
    "# assert -1*last_upper_loss >= 1.090909091e+01, f\"Last upper loss was {last_upper_loss}\"\n",
    "\n",
    "p_lower_lambda = torch.clamp(project_onto_lambda(lambda_lower, G, lower_c, 0), min=0.0)\n",
    "p_upper_lambda = torch.clamp(project_onto_lambda(lambda_upper, G, upper_c, 0), max=0.0)\n",
    "print(f\"Last projection of lower bound: {p_lower_lambda} with constraint: {constraint(p_lower_lambda, G, lower_c)}\")\n",
    "print(f\"Last projection of upper bound: {p_upper_lambda} with constraint: {constraint(p_upper_lambda, G, upper_c)}\")\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(f\"Optimization Lambda (converged to {loss_graph[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(f\"\\Lambda_1 (converged to {lambda_vals[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[1,:])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(f\"\\Lambda_2 (converged to {lambda_vals[2,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[2,:])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(f\"\\alpha (converged to {lambda_vals[3,-1]:1.1f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add alpha as an optimizable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized lambda: tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [4.6042],\n",
      "        [0.0000],\n",
      "        [0.0000]]) tensor([[ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-4.2369]])\n",
      "Optimized lower alpha: tensor([[0.],\n",
      "        [0.]])\n",
      "CROWN lower bound: 0.0, Lagrange lower bound: 23.021236419677734\n",
      "CROWN upper bound: 12.0, Lagrange upper bound: -30.36902618408203\n",
      "last_lower_loss 22.953571319580078\n",
      "last_upper_loss30.233699798583984\n",
      "last_lower_lambda[[0.       ]\n",
      " [0.       ]\n",
      " [4.5907145]\n",
      " [0.       ]\n",
      " [0.       ]]\n",
      "last_upper_lambda[[ 0.     ]\n",
      " [ 0.     ]\n",
      " [ 0.     ]\n",
      " [ 0.     ]\n",
      " [-4.22337]]\n",
      "lower bound penalth0\n",
      "upper bound penalty0\n",
      "w: tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "w: tensor([[0.0005],\n",
      "        [0.0000],\n",
      "        [0.2190],\n",
      "        [0.2200],\n",
      "        [0.0034]])\n",
      "Last projection of lower bound: tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], grad_fn=<ClampBackward1>) with constraint: tensor([[nan],\n",
      "        [nan]], grad_fn=<SubBackward0>)\n",
      "Last projection of upper bound: tensor([[-7.8277e-05],\n",
      "        [-0.0000e+00],\n",
      "        [-3.2353e-02],\n",
      "        [-3.2496e-02],\n",
      "        [-4.9635e-04]], grad_fn=<ClampBackward1>) with constraint: tensor([[-0.5928],\n",
      "        [-0.6097]], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9676/3732288400.py:5: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w = torch.from_numpy(np.linalg.lstsq(-G.detach().clone().numpy().T, c.detach().clone().numpy())[0].reshape(-1,1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7ddf021750>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def constraint(v, G, c):\n",
    "    return -G.T @ v - c\n",
    "\n",
    "def project_onto_lambda(v, G, c, d):\n",
    "    w = torch.from_numpy(np.linalg.lstsq(-G.detach().clone().numpy().T, c.detach().clone().numpy())[0].reshape(-1,1))\n",
    "    min_w_entry = torch.min(w)\n",
    "    if min_w_entry < 0:\n",
    "        w = w - min_w_entry\n",
    "    print(f\"w: {w}\")\n",
    "    # print(constraint(w, G, c))\n",
    "    projection = ((w.T@v)/(w.T@w))*w\n",
    "\n",
    "    # using the formula x = z - A.T(AA.T)^-1(Az-b) to project v onto the set Ax=b\n",
    "    # in this example, A = -G.T, b = c\n",
    "    # if np.linalg.det(G.T@G) != 0:\n",
    "    #     projection = v + G @ torch.linalg.inv(G.T@G) @ (-G.T @ v - c)\n",
    "    # else:\n",
    "    #     projection = v + G @ torch.linalg.pinv(G.T@G) @ (-G.T @ v - c)\n",
    "\n",
    "    return projection\n",
    "\n",
    "def obj_fn(x, d, lambda_, A, b, c, ub=False):\n",
    "    if ub:\n",
    "        return -1*(c.T@x + d) + lambda_.T@(A@x - b)\n",
    "    else:\n",
    "        return c.T@x + d + lambda_.T@(A@x - b)\n",
    "    \n",
    "def get_penalty(lambda_, G, c, scale=5, lb=True):\n",
    "    if lb:\n",
    "        return 0 #-scale*(torch.sum(torch.abs(constraint(lambda_, G, c))) + torch.sum(torch.abs(torch.clamp(lambda_, max=0.0))))\n",
    "    else:\n",
    "        return 0 #-scale*(torch.sum(torch.abs(constraint(lambda_, G, c))) + torch.sum(torch.abs(torch.clamp(lambda_, min=0.0))))\n",
    "\n",
    "lower_x = torch.tensor([[-5], [-5]]).float()\n",
    "upper_x = torch.tensor([[6], [6]]).float()\n",
    "lambda_lower = torch.rand(5,1, requires_grad=True)\n",
    "lambda_upper = torch.rand(5,1, requires_grad=True)\n",
    "# lower_c = torch.tensor([[1], [1]]).float()\n",
    "lower_c = torch.rand(2,1,requires_grad=True)\n",
    "lower_d = torch.tensor([[0]]).float()\n",
    "upper_c = torch.tensor([[12/22],[12/22]]).float()\n",
    "upper_d = torch.tensor([[120/22]]).float()\n",
    "G = torch.tensor([[-2/9, 1],[-6/5, 1], [-5/3, -1], [1/8, -1], [5, 1]]).float()\n",
    "h = torch.tensor([[46/9], [6], [25/3], [19/4], [26]]).float()\n",
    "\n",
    "# print(f\"Initial Lower Lambda: {lambda_lower.data}\\nInitial Upper Lambda: {lambda_upper.data}\\nInitial lower \\alpha: {lower_c}\\n Initial upper \\alpha: {upper_c}\")\n",
    "\n",
    "# Number of optimization steps\n",
    "lr = 0.1\n",
    "num_steps = 100\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "lambda_vals = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "lambda_vals = np.vstack((lambda_vals, np.zeros((3, num_steps))))\n",
    "\n",
    "# opt = torch.optim.Adam([lambda_, c], lr=lr, maximize=True)\n",
    "opt_lower = torch.optim.Adam([lambda_lower, lower_c], lr=lr, maximize=True)\n",
    "opt_upper = torch.optim.Adam([lambda_upper], lr=lr, maximize=True)\n",
    "scheduler_lower = torch.optim.lr_scheduler.ExponentialLR(opt_lower, 0.98)\n",
    "scheduler_upper = torch.optim.lr_scheduler.ExponentialLR(opt_upper, 0.98)\n",
    "\n",
    "## Projections and constraints should not be included in the Adam optimizer\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    lower_y = obj_fn(lower_x, lower_d, lambda_lower, G, h, lower_c) + get_penalty(lambda_lower, G, lower_c, lb=True)\n",
    "    upper_y = -1*(obj_fn(upper_x, upper_d, lambda_upper, G, h, upper_c) - get_penalty(lambda_upper, G, upper_c, lb=False))\n",
    "\n",
    "    \n",
    "    loss_graph[1, step] = lower_y.item()\n",
    "\n",
    "    if step == num_steps - 1:\n",
    "        last_lower_loss = lower_y.item()\n",
    "        last_upper_loss = upper_y.item()\n",
    "        last_lower_lambda = lambda_lower.detach().clone().numpy()\n",
    "        last_upper_lambda = lambda_upper.detach().clone().numpy()\n",
    "\n",
    "    opt_lower.zero_grad(set_to_none=True)\n",
    "    opt_upper.zero_grad(set_to_none=True)\n",
    "\n",
    "    lower_y.backward()\n",
    "    upper_y.backward()\n",
    "\n",
    "    opt_lower.step()\n",
    "    opt_upper.step()\n",
    "    scheduler_lower.step()\n",
    "    scheduler_upper.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        lower_c.data = torch.clamp(lower_c.data, min=0.0, max=1.0)\n",
    "        \n",
    "        lambda_upper.data = torch.clamp(lambda_upper.data, max=0.0)\n",
    "        lambda_lower.data = torch.clamp(lambda_lower.data, min=0.0)\n",
    "        lambda_vals[1:3,step] = lambda_lower.detach().clone().numpy().flatten()[:2]\n",
    "        \n",
    "# lambda_lower.data = project_onto_lambda(lambda_lower.data, G, lower_c,0)\n",
    "lambda_lower_optimized = lambda_lower.data\n",
    "lambda_upper_optimized = lambda_upper.data\n",
    "alpha_optimize = lower_c.data\n",
    "\n",
    "print(\"Optimized lambda:\", lambda_lower_optimized, lambda_upper_optimized)\n",
    "print(\"Optimized lower alpha:\", alpha_optimize)\n",
    "\n",
    "print(f\"CROWN lower bound: {obj_fn(lower_x, lower_d, torch.zeros(5,1).float(), G, h, lower_c).squeeze()}, Lagrange lower bound: {obj_fn(lower_x, lower_d, lambda_lower, G, h, lower_c).squeeze()}\")\n",
    "print(f\"CROWN upper bound: {obj_fn(upper_x, upper_d, torch.zeros(5,1).float(), G, h, upper_c).squeeze()}, Lagrange upper bound: {obj_fn(upper_x, upper_d, lambda_upper, G, h, upper_c).squeeze()}\")\n",
    "\n",
    "print(f\"last_lower_loss {last_lower_loss}\\nlast_upper_loss{last_upper_loss}\\nlast_lower_lambda{last_lower_lambda}\\nlast_upper_lambda{last_upper_lambda}\\nlower bound penalth{get_penalty(lambda_lower, G, lower_c, lb=True, scale=1)}\\nupper bound penalty{get_penalty(lambda_upper, G, upper_c, lb=False, scale=1)}\")\n",
    "# assert last_lower_loss <= -7, f\"Last lower loss was {last_lower_loss}\"\n",
    "# assert -1*last_upper_loss >= 1.090909091e+01, f\"Last upper loss was {last_upper_loss}\"\n",
    "\n",
    "p_lower_lambda = torch.clamp(project_onto_lambda(lambda_lower, G, lower_c, 0), min=0.0)\n",
    "p_upper_lambda = torch.clamp(project_onto_lambda(lambda_upper, G, upper_c, 0), max=0.0)\n",
    "print(f\"Last projection of lower bound: {p_lower_lambda} with constraint: {constraint(p_lower_lambda, G, lower_c)}\")\n",
    "print(f\"Last projection of upper bound: {p_upper_lambda} with constraint: {constraint(p_upper_lambda, G, upper_c)}\")\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(f\"Optimization Lambda (converged to {loss_graph[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(f\"\\Lambda_1 (converged to {lambda_vals[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[1,:])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(f\"\\Lambda_2 (converged to {lambda_vals[2,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[2,:])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(f\"\\alpha (converged to {lambda_vals[3,-1]:1.1f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[3,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

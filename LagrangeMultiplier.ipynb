{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking inspiration from this [post](https://stackoverflow.com/questions/77508682/correct-way-to-do-lagrange-dual-optimization-pytorch), we will use the PyTorch Adam Optimizer to solve the Lagrangain dual problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from gurobipy import GRB, quicksum, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an optimizer on the linear system of equations:\n",
    "$$\n",
    "x_0 + x_1 - 5 = 0 (eq.1)\n",
    "$$\n",
    "$$\n",
    "2x_0 - x_1 + 3 = 0 (eq.2)\n",
    "$$\n",
    "where the loss is defined to be:\n",
    "$$\n",
    "(eq.1)^2 + (eq.2)^2\n",
    "$$\n",
    "and the Adam optimizer is being used to minimize this loss. \n",
    "The exact solutions are \n",
    "$$\n",
    "x_0 = \\frac{2}{3}\n",
    "$$\n",
    "$$\n",
    "x_1 = \\frac{13}{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 34.0\n",
      "Step 100, Loss: 20.551959991455078\n",
      "Step 200, Loss: 11.665807723999023\n",
      "Step 300, Loss: 6.137031555175781\n",
      "Step 400, Loss: 2.9643564224243164\n",
      "Step 500, Loss: 1.3036264181137085\n",
      "Step 600, Loss: 0.5183063745498657\n",
      "Step 700, Loss: 0.18529455363750458\n",
      "Step 800, Loss: 0.05931048095226288\n",
      "Step 900, Loss: 0.016934897750616074\n",
      "Step 1000, Loss: 0.004297736566513777\n",
      "Step 1100, Loss: 0.0009654018795117736\n",
      "Step 1200, Loss: 0.00019103451631963253\n",
      "Step 1300, Loss: 3.309983731014654e-05\n",
      "Step 1400, Loss: 4.998842086934019e-06\n",
      "Optimized solution: [0.6665166 4.33259  ]\n"
     ]
    }
   ],
   "source": [
    "# Define your system of equations as a function\n",
    "def equations(x):\n",
    "    eq1 = x[0] + x[1] - 5\n",
    "    eq2 = 2*x[0] - x[1] + 3\n",
    "    return eq1, eq2\n",
    "\n",
    "# Initialize the variables \n",
    "x = torch.tensor([0.,0.], requires_grad=True)\n",
    "\n",
    "# Define the Adam optimizer\n",
    "optimizer = Adam([x], lr=0.01, maximize=False)\n",
    "\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-6\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "    # Compute the system of equations\n",
    "    eq1, eq2 = equations(x)\n",
    "    \n",
    "    # Define the loss as the sum of squared equations\n",
    "    loss = eq1**2 + eq2**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the variables\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an optimizer on the quadratic equation:\n",
    "$$\n",
    "y = x_t^2\n",
    "$$\n",
    "where the loss is defined to be:\n",
    "$$\n",
    "y\n",
    "$$\n",
    "and the Adam optimizer is being used to minimize this loss. \n",
    "The exact solution is\n",
    "$$\n",
    "x_t = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t: tensor([[0.9906]], requires_grad=True)\n",
      "Step 0, Loss: 0.981256365776062\n",
      "Step 100, Loss: 0.04736644774675369\n",
      "Step 200, Loss: 0.00020352439605630934\n",
      "Step 300, Loss: 1.983038089292677e-08\n",
      "Optimized solution: [[9.470147e-05]]\n"
     ]
    }
   ],
   "source": [
    "# test on a simple parabola\n",
    "x_t = torch.rand(1,1, requires_grad=True)\n",
    "print(f\"x_t: {x_t}\")\n",
    "optimizer = torch.optim.Adam([x_t], lr=0.01, maximize=False)\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "\n",
    "    loss = x_t**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x_t.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t: tensor([[0.2674]], requires_grad=True)\n",
      "Step 0, Loss: -0.07150442898273468\n",
      "Optimized solution: [[7.715425e-06]]\n"
     ]
    }
   ],
   "source": [
    "# test on a simple parabola\n",
    "x_t = torch.rand(1,1, requires_grad=True)\n",
    "print(f\"x_t: {x_t}\")\n",
    "optimizer = torch.optim.Adam([x_t], lr=0.01, maximize=True)\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "\n",
    "    loss = -x_t**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x_t.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Gurobi will be needed to validate answers later on, here is a simple example using the Gurobi module.\n",
    "\n",
    "minimize $5x + 4y$\n",
    "\n",
    "subject to\n",
    "$$x + y \\geq 8$$\n",
    "$$2x + y \\geq 10$$\n",
    "$$x + 4y \\geq 11$$\n",
    "$$x \\geq 0$$\n",
    "$$y \\geq 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2024-10-28\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 3 rows, 2 columns and 6 nonzeros\n",
      "Model fingerprint: 0x6c7cdc94\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 4e+00]\n",
      "  Objective range  [4e+00, 5e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [8e+00, 1e+01]\n",
      "Presolve time: 0.00s\n",
      "Presolved: 3 rows, 2 columns, 6 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    0.0000000e+00   1.850000e+01   0.000000e+00      0s\n",
      "       2    3.4000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  3.400000000e+01\n",
      "Objective Function Value: 34.000000\n",
      "x: 2\n",
      "y: 6\n"
     ]
    }
   ],
   "source": [
    "from gurobipy import *\n",
    "opt_mod = Model(name = \"simple_linear_program_1\")\n",
    "\n",
    "# add variables\n",
    "x = opt_mod.addVar(name='x', vtype=GRB.CONTINUOUS, lb=0)\n",
    "y = opt_mod.addVar(name='y', vtype=GRB.CONTINUOUS, lb=0)\n",
    "\n",
    "# set the objective function\n",
    "obj_fn = 5*x + 4*y\n",
    "opt_mod.setObjective(obj_fn, GRB.MINIMIZE)\n",
    "\n",
    "# adding the constraints\n",
    "c1 = opt_mod.addConstr(x + y >= 8, name='c1')\n",
    "c2 = opt_mod.addConstr(2*x + y >= 10, name='c2')\n",
    "c3 = opt_mod.addConstr(x + 4*y >= 11, name='c3')\n",
    "\n",
    "# now optimize the problem and save it to a file\n",
    "opt_mod.optimize()\n",
    "opt_mod.write(\"simpe_linear_model_one.lp\")\n",
    "\n",
    "# output the result\n",
    "print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "# Get values of the decision variables\n",
    "for v in opt_mod.getVars():\n",
    "    print('%s: %g' % (v.VarName, v.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use the Adam optimizer to solve a Lagrangian dual problem. The objective function be to minimized is: \n",
    "$$\n",
    "2x_1 + 4x_2 = 0\n",
    "$$\n",
    "And the constraints are: \n",
    "$$\n",
    "-x_1 - 5 = 0\n",
    "$$\n",
    "$$\n",
    "-x_2 - 5 = 0\n",
    "$$\n",
    "The Lagrangian thus becomes:\n",
    "$$\n",
    "L(x, \\lambda) = 2x_1 + 4x_2 + \\lambda_1(-x_1 - 5) + \\lambda_2(-x_2 - 5)\n",
    "$$\n",
    "\n",
    "For my personal purposes, I do not need to modify $x$, solely $\\lambda$, therefore this is not a dual optimization problem but a simple maximization of $\\lambda$. \n",
    "\n",
    "Thus, assuming $x$ is given and does not violate constraints, we will obtain the gradients:\n",
    "$$\n",
    "\\nabla_{x_1}L(x,\\lambda) = 2 - \\lambda_1 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{x_2}L(x,\\lambda) = 4 - \\lambda_2 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{\\lambda_1}L(x,\\lambda) = -x_1 - 5 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{\\lambda_2}L(x,\\lambda) = -x_2 - 5 = 0\n",
    "$$\n",
    "Giving the exact solutions:\n",
    "$$\n",
    "x_1 = -5\n",
    "$$\n",
    "$$\n",
    "x_2 = -5\n",
    "$$\n",
    "$$\n",
    "\\lambda_1 = 2\n",
    "$$\n",
    "$$\n",
    "\\lambda_2 = 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5. -5.  2.  4.]\n"
     ]
    }
   ],
   "source": [
    "# double checking that these are indeed the exact solutions as I stated above\n",
    "A = np.array([[0,0,-1,0],[0,0,0,-1],[-1,0,0,0],[0,-1,0,0]])\n",
    "b = np.array([-2,-4,5,5])\n",
    "x = np.linalg.solve(A,b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[-1.  0.]\n",
      " [ 0. -1.]]\n",
      "b: [[-5.]\n",
      " [-5.]]\n",
      "c: [[2.]\n",
      " [4.]]\n"
     ]
    }
   ],
   "source": [
    "c = np.array([2,4], dtype='float32').reshape(-1,1)\n",
    "n = 2 # input of dimension 2\n",
    "m = 2 # 2 constraints\n",
    "A = np.array([[-1, 0], [0, -1]], dtype='float32')\n",
    "b = np.array([-5,-5], dtype='float32').reshape(-1, 1)\n",
    "print(f\"A: {A}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[-1.  0.]\n",
      " [ 0. -1.]]\n",
      "b: [[-5.]\n",
      " [-5.]]\n",
      "c: [[2.]\n",
      " [4.]]\n",
      "x_t: [[-5.]\n",
      " [-5.]]\n",
      "Init lagrange_multiplier [[0.873101 ]\n",
      " [0.8700457]]\n",
      "lagrangian shape: torch.Size([])\n",
      "Shape objective torch.Size([1, 1]), Shape constraint torch.Size([2, 1])\n",
      "objective: tensor([[-30.]], grad_fn=<MmBackward0>)\n",
      "constraint: tensor([[0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n",
      "lagrangian: -30.0\n",
      "Step 0, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 340000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m lagrangian\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# update values\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mopt_lagrange\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c_t = torch.tensor(c).float()\n",
    "A_t = torch.tensor(A).float()\n",
    "b_t = torch.tensor(b).float().reshape(-1,1)\n",
    "# x_t = torch.rand(n, 1, requires_grad=True)\n",
    "x_t = torch.tensor([-5.0,-5.0], requires_grad=True).float().reshape(-1,1)\n",
    "print(f\"A: {A_t.detach().numpy()}\")\n",
    "print(f\"b: {b_t.detach().numpy()}\")\n",
    "print(f\"c: {c_t.detach().numpy()}\")\n",
    "print(f\"x_t: {x_t.detach().numpy()}\")\n",
    "\n",
    "_lagrange_multiplier = torch.rand(m,1, requires_grad=True)\n",
    "lagrange_multiplier = torch.nn.functional.softplus(_lagrange_multiplier)\n",
    "print(f\"Init lagrange_multiplier {lagrange_multiplier.detach().numpy()}\")\n",
    "\n",
    "# opt_weights = torch.optim.Adam([x_t], lr=0.1)\n",
    "opt_lagrange = torch.optim.Adam([_lagrange_multiplier], lr=0.1, maximize=True)\n",
    "\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "    \n",
    "\n",
    "    objective = c_t.T @ x_t\n",
    "    constraint = A_t @ x_t + b_t\n",
    "    \n",
    "    lagrange_multiplier = torch.nn.functional.softplus(_lagrange_multiplier)\n",
    "    lagrangian = objective + lagrange_multiplier.T @ constraint\n",
    "    lagrangian = lagrangian.squeeze()\n",
    "    if step == 0:\n",
    "        print(f\"lagrangian shape: {lagrangian.shape}\")\n",
    "        print(f\"Shape objective {objective.shape}, Shape constraint {constraint.shape}\")\n",
    "        print(f\"objective: {objective}\")\n",
    "        print(f\"constraint: {constraint}\")\n",
    "        print(f\"lagrangian: {lagrangian}\")\n",
    "\n",
    "        # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {lagrangian.item()}, lambda_1: {_lagrange_multiplier[0]}, lambda_2: {_lagrange_multiplier[1]}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(lagrangian.item()) < loss_threshold:\n",
    "        break\n",
    "        \n",
    "    # zero the gradient\n",
    "    opt_lagrange.zero_grad()\n",
    "\n",
    "    # compute the gradient\n",
    "    lagrangian.backward()\n",
    "\n",
    "    # update values\n",
    "    opt_lagrange.step()\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: -5.0 is leaf True, x2: -5.0 is leaf True, x3: tensor([0.0566], requires_grad=True) is leaf True, x4: tensor([0.8357], requires_grad=True) is leaf True\n",
      "Optimized x1: -5.0\n",
      "Optimized x2: -5.0\n",
      "Optimized x3: 0.05660974979400635\n",
      "Optimized x4: 0.835723876953125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9e002a050>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Adam\n",
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.tensor(-5.0, requires_grad=False)\n",
    "x2_opt = torch.tensor(-5.0, requires_grad=False)\n",
    "x3_opt = torch.rand(1, requires_grad=True)\n",
    "x4_opt = torch.rand(1, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 1000\n",
    "lr = 0.1\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "opt = torch.optim.Adam([x3_opt, x4_opt], lr=lr, maximize=True)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, 0.98)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt).sum()\n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    y.backward()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    # clip lagrange values\n",
    "    x3_opt.data = torch.clip(x3_opt.data, 0)\n",
    "    x4_opt.data = torch.clip(x4_opt.data, 0)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([0.6116], dtype=torch.float64, requires_grad=True) is leaf True, x2: tensor([0.0295], dtype=torch.float64, requires_grad=True) is leaf True, x3: tensor([0.1955], dtype=torch.float64, requires_grad=True) is leaf True, x4: tensor([0.9048], dtype=torch.float64, requires_grad=True) is leaf True\n",
      "Optimized x1: -4.55081800782109\n",
      "Optimized x2: -5.225714517952417\n",
      "Optimized x3: 0.0\n",
      "Optimized x4: 0.29730698177256015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fda17360160>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Adam\n",
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x2_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x3_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x4_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 10000 + 20\n",
    "lr = 0.1\n",
    "\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "opt_duals = torch.optim.Adam([x3_opt, x4_opt], lr=lr, maximize=True)\n",
    "opt_x = torch.optim.Adadelta([x1_opt, x2_opt], lr=lr, maximize=False)\n",
    "scheduler_dual = torch.optim.lr_scheduler.ExponentialLR(opt_duals, 0.98)\n",
    "scheduler_x = torch.optim.lr_scheduler.ExponentialLR(opt_x, 0.98)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt).sum()\n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt_x.zero_grad(set_to_none=True)\n",
    "    opt_duals.zero_grad(set_to_none=True)\n",
    "    y.backward()\n",
    "\n",
    "    if flip:\n",
    "        opt_x.step()\n",
    "        # if step % 20 == 0:\n",
    "        #     scheduler_x.step()\n",
    "    else:\n",
    "        opt_duals.step()\n",
    "        # if step % 20 == 0:\n",
    "        #     scheduler_dual.step()\n",
    "    \n",
    "    # clip lagrange values\n",
    "    x3_opt.data = torch.clip(x3_opt.data, 0)\n",
    "    x4_opt.data = torch.clip(x4_opt.data, 0)\n",
    "\n",
    "    if step != 0 and step % 60 == 0:\n",
    "        flip = not flip\n",
    "    \n",
    "    if loss_graph[1, step] < -29.5 and loss_graph[1,step] > -30.5 and flip == False:\n",
    "        loss_graph[1, step:] = loss_graph[1,step]\n",
    "        break\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([0.7194], requires_grad=True) is leaf True, x2: tensor([0.2687], requires_grad=True) is leaf True, x3: tensor([0.7701], requires_grad=True) is leaf True, x4: tensor([0.8334], requires_grad=True) is leaf True\n",
      "Optimized x1: -4.999998092651367\n",
      "Optimized x2: -4.999998092651367\n",
      "Optimized x3: 1.9999995231628418\n",
      "Optimized x4: 3.9999990463256836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdffc1509d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1, requires_grad=True)\n",
    "x2_opt = torch.rand(1, requires_grad=True)\n",
    "x3_opt = torch.rand(1, requires_grad=True)\n",
    "x4_opt = torch.rand(1, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "def zero_grad(parameters):\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 1000\n",
    "lr = 0.1\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt)\n",
    "    y.backward()\n",
    "\n",
    "    grad1 = x1_opt.grad if x1_opt.grad is not None else 0.0\n",
    "    grad2 = x2_opt.grad if x2_opt.grad is not None else 0.0\n",
    "    grad3 = x3_opt.grad if x3_opt.grad is not None else 0.0\n",
    "    grad4 = x4_opt.grad if x4_opt.grad is not None else 0.0\n",
    "\n",
    "    loss_graph[1, step] = y.item()\n",
    "    \n",
    "    if flip:\n",
    "        # when this is true, we are minimizing L(x,lambda) w.r.t. x\n",
    "        x1_opt.data = (x1_opt.data + lr*grad3).requires_grad_(True)\n",
    "        x2_opt.data = (x2_opt.data + lr*grad4).requires_grad_(True)        \n",
    "\n",
    "    else:\n",
    "        # when this is false, we are maximizing L(x,lambda) w.r.t. lambda\n",
    "        x3_opt.data = torch.clamp(x3_opt.data + lr*grad1, min=0.0).requires_grad_(True)\n",
    "        x4_opt.data = torch.clamp(x4_opt.data + lr*grad2, min=0.0).requires_grad_(True)\n",
    "\n",
    "    # if step != 0 and (step % 100) == 0:\n",
    "    #     print(f\"Step {step}, Loss: {y.item():.4f}, x1: {x1_opt.detach().numpy()[0]:.4f} x2: {x2_opt.detach().numpy()[0]:.4f} lambda_1: {x3_opt.detach().numpy()[0]:.4f}, lambda_2: {x4_opt.detach().numpy()[0]:.4f}, grads: [{x1_opt.grad.detach().numpy()[0]:.4f}, {x2_opt.grad.detach().numpy()[0]:.4f}, {x3_opt.grad.detach().numpy()[0]:.4f}, {x4_opt.grad.detach().numpy()[0]:.4f}]\")\n",
    "\n",
    "    if step != 0 and (step % 100) == 0:\n",
    "        flip = not flip\n",
    "        \n",
    "    # zero out the gradients\n",
    "    zero_grad([x1_opt, x2_opt, x3_opt, x4_opt])\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x1: -0.8333324193954468\n",
      "Optimized x2: -0.8333352208137512\n",
      "Optimized l1: 1.045896053314209\n",
      "Optimized l2: 0.2871112823486328\n",
      "Optimized l3: 0.0\n",
      "Optimized l4: 1.4137334823608398\n",
      "Optimized l5: 0.5862621068954468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdce8c83550>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1, requires_grad=True)\n",
    "x2_opt = torch.rand(1, requires_grad=True)\n",
    "l1_opt = torch.rand(1, requires_grad=True)\n",
    "l2_opt = torch.rand(1, requires_grad=True)\n",
    "l3_opt = torch.rand(1, requires_grad=True)\n",
    "l4_opt = torch.rand(1, requires_grad=True)\n",
    "l5_opt = torch.rand(1, requires_grad=True)\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, l1, l2, l3, l4, l5):\n",
    "    return x1 + 2*x2 + l1*(x1-2) + l2*(-x1-2) + l3*(x2-2) + l4*(-x2-2) + l5*(-3*x1 - x2 - 5)\n",
    "\n",
    "def zero_grad(parameters):\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 10000\n",
    "lr = 0.01\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, l1_opt, l2_opt, l3_opt, l4_opt, l5_opt)\n",
    "    y.backward()\n",
    "\n",
    "    x_grad1 = x1_opt.grad if x1_opt.grad is not None else 0.0\n",
    "    x_grad2 = x2_opt.grad if x2_opt.grad is not None else 0.0\n",
    "    l_grad1 = l1_opt.grad if l1_opt.grad is not None else 0.0\n",
    "    l_grad2 = l2_opt.grad if l2_opt.grad is not None else 0.0\n",
    "    l_grad3 = l3_opt.grad if l3_opt.grad is not None else 0.0\n",
    "    l_grad4 = l4_opt.grad if l4_opt.grad is not None else 0.0\n",
    "    l_grad5 = l5_opt.grad if l5_opt.grad is not None else 0.0\n",
    "\n",
    "    loss_graph[1, step] = y.item()\n",
    "    \n",
    "    if flip:\n",
    "        # when this is true, we are minimizing L(x,lambda) w.r.t. x\n",
    "        x1_opt.data = (x1_opt.data + lr*(-l_grad1 + l_grad2 + l_grad5)).requires_grad_(True)\n",
    "        x2_opt.data = (x2_opt.data + lr*(-l_grad3 + l_grad4 + l_grad5)).requires_grad_(True)        \n",
    "\n",
    "    else:\n",
    "        # when this is false, we are maximizing L(x,lambda) w.r.t. lambda\n",
    "        l1_opt.data = torch.clamp(l1_opt.data + lr*(-x_grad1), min=0.0).requires_grad_(True)\n",
    "        l2_opt.data = torch.clamp(l2_opt.data + lr*(x_grad1), min=0.0).requires_grad_(True)\n",
    "        l3_opt.data = torch.clamp(l3_opt.data + lr*(-x_grad2), min=0.0).requires_grad_(True)\n",
    "        l4_opt.data = torch.clamp(l4_opt.data + lr*(x_grad2), min=0.0).requires_grad_(True)\n",
    "        l5_opt.data = torch.clamp(l5_opt.data + lr*(x_grad1 + x_grad2), min=0.0).requires_grad_(True)\n",
    "\n",
    "    if step != 0 and (step % 100) == 0:\n",
    "        flip = not flip\n",
    "        \n",
    "    # zero out the gradients\n",
    "    zero_grad([x1_opt, x2_opt, l1_opt, l2_opt, l3_opt, l4_opt, l5_opt])\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "l1_optimized = l1_opt.item()\n",
    "l2_optimized = l2_opt.item()\n",
    "l3_optimized = l3_opt.item()\n",
    "l4_optimized = l4_opt.item()\n",
    "l5_optimized = l5_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized l1:\", l1_optimized)\n",
    "print(\"Optimized l2:\", l2_optimized)\n",
    "print(\"Optimized l3:\", l3_optimized)\n",
    "print(\"Optimized l4:\", l4_optimized)\n",
    "print(\"Optimized l5:\", l5_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am checking the results of the Lagrange problem above by computing the upper and lower bounds on x as well as the minimal and maximal pertubation on each logit output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Model fingerprint: 0xf5f961a3\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [3e+00, 4e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Presolve removed 5 rows and 2 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    1.6000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.600000000e+01\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 2e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -3.0000000e+30   3.000000e+30   3.000000e+00      0s\n",
      "       3   -5.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 3 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -5.000000000e+00\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [3e+00, 4e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    7.0000000e+30   2.000000e+30   7.000000e+00      0s\n",
      "       2    1.6000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.600000000e+01\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 3e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -4.0000000e+30   3.000000e+30   4.000000e+00      0s\n",
      "       3   -5.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 3 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -5.000000000e+00\n",
      "logit 1 is bounded s.t. -5.0 <= z(x) <= 16.0 with lb pertubation (-1.0, -2.0) and ub pertubation (2.0, 2.0)\n",
      "logit 2 is bounded s.t. -5.0 <= z(x) <= 16.0 with lb pertubation (-1.0, -2.0) and ub pertubation (2.0, 2.0)\n"
     ]
    }
   ],
   "source": [
    "W_ub = np.array([[4,3],[4,3]])\n",
    "b_ub = np.array([2,2])\n",
    "W_lb = np.array([[1,2],[1,1]])\n",
    "# b_lb = np.array([1,1])\n",
    "b_lb = np.zeros(2)\n",
    "# using Gurobi to solve the same problem as above\n",
    "opt_mod = Model(name = \"simple_linear_program_2\")\n",
    "\n",
    "# add variables\n",
    "inputs = np.array(list(opt_mod.addVars(W_ub.shape[1], name=\"x\", lb=float(\"-inf\"), ub=float(\"inf\")).values()))\n",
    "\n",
    "# adding the constraints\n",
    "c1 = opt_mod.addConstr(inputs[0] - 2 <= 0, name='c1') # these four constraints are the l_inf norm box constraints\n",
    "c2 = opt_mod.addConstr(-inputs[0] - 2 <= 0, name='c2')\n",
    "c3 = opt_mod.addConstr(inputs[1] - 2 <= 0, name='c3')\n",
    "c4 = opt_mod.addConstr(-inputs[1] - 2 <= 0, name='c4')\n",
    "c5 = opt_mod.addConstr(-3*inputs[0] - inputs[1] - 5 <= 0, name='c5') # this constraint is a line constraint cutting through the box\n",
    "\n",
    "worst_case_inputs_ub = []\n",
    "worst_case_inputs_lb = []\n",
    "upper_bounds = []\n",
    "lower_bounds = []\n",
    "\n",
    "# set the objective function for each logit\n",
    "for idx in range(2*W_ub.shape[0]):\n",
    "    i = idx // 2\n",
    "    if idx % 2 == 0:\n",
    "        obj_fn = quicksum([W_ub[i,j]*inputs[j] for j in range(W_ub.shape[1])]) + b_ub[i]\n",
    "        opt_mod.setObjective(obj_fn, GRB.MAXIMIZE)\n",
    "    else:\n",
    "        obj_fn = quicksum([W_lb[i,j]*inputs[j] for j in range(W_lb.shape[1])]) + b_lb[i]\n",
    "        opt_mod.setObjective(obj_fn, GRB.MINIMIZE)\n",
    "\n",
    "    # now optimize the problem and save it to a file\n",
    "    opt_mod.optimize()\n",
    "    # opt_mod.write(\"scenario_one_upperbound_logit_one.lp\")\n",
    "\n",
    "    # output the result\n",
    "    # print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "    if idx % 2 == 0:\n",
    "        upper_bounds.append(opt_mod.ObjVal)\n",
    "    else:\n",
    "        lower_bounds.append(opt_mod.ObjVal)\n",
    "    # Get values of the decision variables\n",
    "    temp_inputs = []\n",
    "    for v in opt_mod.getVars():\n",
    "        # print('%s: %g' % (v.VarName, v.x))\n",
    "        temp_inputs.append(v.x)\n",
    "\n",
    "    if idx % 2 == 0:\n",
    "        worst_case_inputs_ub.append(tuple(temp_inputs)) # append the worst case input that caused this maximal pertubation\n",
    "    else:\n",
    "        worst_case_inputs_lb.append(tuple(temp_inputs)) # append the worst case input that caused this maximal pertubation\n",
    "    \n",
    "for i in range(W_ub.shape[0]):\n",
    "    print(f\"logit {i + 1} is bounded s.t. {lower_bounds[i]} <= z(x) <= {upper_bounds[i]} with lb pertubation {worst_case_inputs_lb[i]} and ub pertubation {worst_case_inputs_ub[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal Result:\n",
      "       message: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n",
      "       success: True\n",
      "        status: 0\n",
      "           fun: 0.0\n",
      "             x: [ 0.000e+00  0.000e+00]\n",
      "           nit: 0\n",
      "         lower:  residual: [ 0.000e+00  0.000e+00]\n",
      "                marginals: [ 2.000e+00  3.000e+00]\n",
      "         upper:  residual: [       inf        inf]\n",
      "                marginals: [ 0.000e+00  0.000e+00]\n",
      "         eqlin:  residual: []\n",
      "                marginals: []\n",
      "       ineqlin:  residual: [ 1.000e+00  2.000e+00]\n",
      "                marginals: [-0.000e+00 -0.000e+00]\n",
      "\n",
      "Dual Result:\n",
      "       message: The problem is unbounded. (HiGHS Status 10: model_status is Unbounded; primal_status is At upper bound)\n",
      "       success: False\n",
      "        status: 3\n",
      "           fun: None\n",
      "             x: None\n",
      "           nit: 0\n",
      "         lower:  residual: None\n",
      "                marginals: None\n",
      "         upper:  residual: None\n",
      "                marginals: None\n",
      "         eqlin:  residual: None\n",
      "                marginals: None\n",
      "       ineqlin:  residual: None\n",
      "                marginals: None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# Primal linear program coefficients\n",
    "c = np.array([2, 3])\n",
    "A = np.array([[1, -1], [3, 1]])\n",
    "b = np.array([1, 2])\n",
    "\n",
    "# Solve the primal linear program\n",
    "result_primal = linprog(c, A_ub=A, b_ub=b, method='highs')\n",
    "\n",
    "# Display the primal result\n",
    "print(\"Primal Result:\")\n",
    "print(result_primal)\n",
    "\n",
    "# Dual linear program coefficients\n",
    "c_dual = -b  # Coefficients are negated for maximization\n",
    "A_dual = -A.T  # Transpose of A with negation\n",
    "b_dual = c  # Dual variables corresponding to the inequality constraints\n",
    "\n",
    "# Solve the dual linear program\n",
    "result_dual = linprog(c_dual, A_ub=A_dual, b_ub=b_dual, method='highs')\n",
    "\n",
    "# Display the dual result\n",
    "print(\"\\nDual Result:\")\n",
    "print(result_dual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [1.25 0.  ]\n",
      " [2.5  0.  ]\n",
      " [3.75 0.  ]\n",
      " [5.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# potentially look into this for solving the lagrange dual, but it requires the gradients to be determined beforehand\n",
    "from scipy.optimize import fsolve, fmin_l_bfgs_b\n",
    "\n",
    "a = 1\n",
    "nbtests = 5\n",
    "minmu = 0\n",
    "maxmu = 5\n",
    "\n",
    "def lagrange(x, mu):\n",
    "    return x**2 + mu * (np.exp(x) + x - a)\n",
    "\n",
    "def lagrange_grad(x, mu):\n",
    "    grad_x = 2*x + mu * (np.exp(x) + 1)\n",
    "    grad_mu = np.exp(x) + x - a\n",
    "    return grad_x, grad_mu\n",
    "\n",
    "def dual(mu):\n",
    "    x = fsolve(lambda x: lagrange_grad(x, mu)[0], x0=1)\n",
    "    obj_val = lagrange(x, mu)\n",
    "    grad = lagrange_grad(x, mu)[1]\n",
    "    return -1.0*obj_val, -1.0*grad\n",
    "\n",
    "pl = np.empty((nbtests, 2))\n",
    "for i, nu in enumerate(np.linspace(minmu,maxmu,nbtests)):\n",
    "    res = fmin_l_bfgs_b(dual, x0=nu, bounds=[(0,None)], factr=1e6)\n",
    "    mu_opt = res[0]\n",
    "    x_opt = fsolve(lambda x: lagrange_grad(x, mu_opt)[0], x0=1)\n",
    "    pl[i] = [nu, *x_opt]\n",
    "print(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last example, we will deal with a single dimension input and output. The input x will have a pertubation of $\\epsilon=2$ and be fed to a ReLU unit, then be added by 5. This will then be the final output. This first example will have no constraints, and if alpha is tuned correctly, it should be zero s.t. alpha-CROWN produces a lower bound of 0. The actualy lower and upper bound is 5 and 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same example as before but with the restriction that $x\\geq1$. We will add a Lagrange Multiplier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximizing $\\lambda$ after minimizing x. \n",
    "For a ReLU activation function, we may have an example where we wish to minimize $\\sigma(x)$ where $\\sigma$ is the ReLU function. If x is convex s.t. $-2\\leq x \\leq 2$, we may choose the lower bound on the input to be x and we will naturally find that the objection function is 0. The main issue is that ReLU is non-convex in nature, therefore we take inspiration from $\\alpha$-CROWN and use the linear lower bound $\\alpha$x to lower bound the ReLU function. This is the linear constraint that is used in the triangular relaxation of ReLU. Letting $alpha=1$ (note that we must have 0\\leq \\alpha \\leq 1) and adding constraints, $x \\geq -1$ and $x \\leq 2$, we now solve the dual program, $L(x, \\lambda)$. First minimize x and then maximize $\\lambda$. x exists in the space $-2 \\leq x \\leq 2$, therefore we know that the smallest value of x is x=-2. Though this x does not fit our constraints, if $\\lambda$ is optimal, we get the optimal objective value. Note that this minimum value will be -1 and not 0 because of the ReLU relaxation. The true minimum value is 0. \n",
    "\n",
    "## Question\n",
    "Ask Zhang if it is reasonable to choose route 1 where clipping and projection is included in the projection as this leads to tigher bounds yet still valid bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Lambda: tensor([[0.8509],\n",
      "        [0.0250]])\n",
      "Initial \u0007lpha: tensor([[1.]])\n",
      "Optimized lambda: tensor([[0.],\n",
      "        [1.]])\n",
      "Optimized alpha: tensor([[1.]])\n",
      "CROWN lower bound: -2.0, Lagrange lower bound: -1.0000001192092896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb1bf784c10>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.tensor([[-2]]).float()\n",
    "lambda_ = torch.rand(2,1, requires_grad=True)\n",
    "c = torch.tensor([[1]]).float()\n",
    "# c = torch.rand(1,1, requires_grad=True)\n",
    "# with torch.no_grad():\n",
    "#     c.data = torch.clamp(c.data, min=0, max=1)\n",
    "G = torch.tensor([[1],[-1]]).float()\n",
    "h = torch.tensor([[2],[1]]).float()\n",
    "\n",
    "print(f\"Initial Lambda: {lambda_.data}\\nInitial \\alpha: {c}\")\n",
    "\n",
    "def constraint(v, G, c):\n",
    "    return -G.T @ v - c\n",
    "\n",
    "def project_onto_lambda(v, G, c):\n",
    "    # using the formula x = z - A.T(AA.T)^-1(Az-b) to project v onto the set Ax=b\n",
    "    # in this example, A = -G.T, b = c\n",
    "    projection = v + G @ torch.linalg.pinv(G.T@G) @ (-G.T @ v - c)\n",
    "    min_val = torch.min(projection)\n",
    "    if min_val < 0.0:\n",
    "        projection -= min_val\n",
    "    return projection\n",
    "    \n",
    "def obj_fn(x, lambda_, A, b, c):\n",
    "    return c.T@x + lambda_.T@(A@x - b)\n",
    "\n",
    "# Number of optimization steps\n",
    "lr = 0.1\n",
    "num_steps = 20\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "lambda_vals = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "lambda_vals = np.vstack((lambda_vals, np.zeros((3, num_steps))))\n",
    "\n",
    "# opt = torch.optim.Adam([lambda_, c], lr=lr, maximize=True)\n",
    "opt = torch.optim.Adam([lambda_], lr=lr, maximize=True)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, 0.98)\n",
    "\n",
    "## Though this converges much more tightly, it is frowned upon\n",
    "# Optimization loop\n",
    "# for step in range(num_steps):\n",
    "\n",
    "#     # y = obj_fn(x, lambda_, G, h, c)\n",
    "#     c.data = torch.clamp(c.data, min=0, max=1)\n",
    "#     projected_lambda = project_onto_lambda(lambda_, G, c)\n",
    "#     lambda_vals[1:3,step] = projected_lambda.detach().clone().numpy().flatten()\n",
    "#     y = obj_fn(x, projected_lambda, G, h, c)\n",
    "\n",
    "#     loss_graph[1, step] = y.item()\n",
    "\n",
    "#     opt.zero_grad(set_to_none=True)\n",
    "\n",
    "#     y.backward()\n",
    "\n",
    "#     opt.step()\n",
    "#     scheduler.step()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # c.data = torch.clamp(c.data, min=0, max=1)\n",
    "#         lambda_vals[3, step] = c.detach().clone().numpy().flatten()\n",
    "\n",
    "## Projections and constraints should not be included in the Adam optimizer\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    y = obj_fn(x, lambda_, G, h, c)\n",
    "    \n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    y.backward()\n",
    "\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lambda_.data = project_onto_lambda(lambda_, G, c)\n",
    "        lambda_vals[1:3,step] = lambda_.detach().clone().numpy().flatten()\n",
    "        \n",
    "    # with torch.no_grad():\n",
    "    #     c.data = torch.clamp(c.data, min=0, max=1)\n",
    "    #     lambda_vals[3, step] = c.detach().clone().numpy().flatten()\n",
    "\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "# lambda_optimized = lambda_.data\n",
    "lambda_optimized = project_onto_lambda(lambda_, G, c).data\n",
    "alpha_optimize = c.data\n",
    "\n",
    "print(\"Optimized lambda:\", lambda_optimized)\n",
    "print(\"Optimized alpha:\", alpha_optimize)\n",
    "\n",
    "print(f\"CROWN lower bound: {obj_fn(x, torch.zeros(2,1).float(), G, h, c).squeeze()}, Lagrange lower bound: {loss_graph[1,-1]}\")\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(f\"Optimization Lambda (converged to {loss_graph[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(f\"\\Lambda_1 (converged to {lambda_vals[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[1,:])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(f\"\\Lambda_2 (converged to {lambda_vals[2,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[2,:])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(f\"\\alpha (converged to {lambda_vals[3,-1]:1.1f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2024-10-28\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 10 nonzeros\n",
      "Model fingerprint: 0xd3257bed\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-01, 5e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [5e+00, 3e+01]\n",
      "Presolve removed 2 rows and 0 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 3 rows, 2 columns, 6 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -1.5200000e+01   1.216983e+01   0.000000e+00      0s\n",
      "       2   -7.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.01 seconds (0.00 work units)\n",
      "Optimal objective -7.000000000e+00\n",
      "Objective Function Value: -7.000000\n",
      "x[0,0]: -2\n",
      "x[1,0]: -5\n",
      "Printing inequality constraint dual variables:\n",
      " [[ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.62790698]\n",
      " [-0.37209302]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# tightest lower bound with  constraints\n",
    "opt_mod = Model(name=\"lower_bound_model\")\n",
    "\n",
    "# constraints\n",
    "G = np.array([[-2/9, 1],[-6/5, 1], [-5/3, -1], [1/8, -1], [5, 1]])\n",
    "h = np.array([[46/9], [6], [25/3], [19/4], [26]])\n",
    "\n",
    "x = opt_mod.addMVar(shape=(2,1), name='x', lb=float('-inf'), ub=float('inf'))\n",
    "\n",
    "# adding inequality constraints\n",
    "ineq_c = opt_mod.addConstr(G @ x <= h, name='c0')\n",
    "\n",
    "opt_mod.setObjective(x[0,0] + x[1,0], GRB.MINIMIZE)\n",
    "\n",
    "opt_mod.optimize()\n",
    "\n",
    "# output the result\n",
    "print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "# Get values of the decision variables\n",
    "for v in opt_mod.getVars():\n",
    "    print('%s: %g' % (v.VarName, v.x))\n",
    "\n",
    "ineq_pi = ineq_c.Pi\n",
    "print(f\"Printing inequality constraint dual variables:\\n {ineq_pi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 10 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-01, 5e+00]\n",
      "  Objective range  [5e-01, 5e-01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [5e+00, 3e+01]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    5.4545455e+29   2.125000e+30   5.454545e-01      0s\n",
      "       2    1.0909091e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.090909091e+01\n",
      "Objective Function Value: 10.909091\n",
      "x[0,0]: 4\n",
      "x[1,0]: 6\n",
      "Printing inequality constraint dual variables:\n",
      " [[0.41779497]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.12765957]]\n"
     ]
    }
   ],
   "source": [
    "opt_mod.setObjective((12/22)*(x[0,0] + x[1,0] + 10), GRB.MAXIMIZE)\n",
    "\n",
    "opt_mod.optimize()\n",
    "\n",
    "# output the result\n",
    "print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "# Get values of the decision variables\n",
    "for v in opt_mod.getVars():\n",
    "    print('%s: %g' % (v.VarName, v.x))\n",
    "\n",
    "ineq_pi = ineq_c.Pi\n",
    "print(f\"Printing inequality constraint dual variables:\\n {ineq_pi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(constraint(torch.tensor([[0.],[0.], [0.62790698], [0.37209302], [0.]]), G, lower_c).detach().clone().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized lambda: tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.7317],\n",
      "        [0.2558],\n",
      "        [0.0000]]) tensor([[-0.4186],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-0.1353]])\n",
      "CROWN lower bound: -10.0, Lagrange lower bound: -6.4376044273376465\n",
      "CROWN upper bound: 12.0, Lagrange upper bound: 10.83309268951416\n",
      "last_lower_loss -7.443249225616455\n",
      "last_upper_loss-11.264433860778809\n",
      "last_lower_lambda[[0.        ]\n",
      " [0.        ]\n",
      " [0.73877037]\n",
      " [0.25246984]\n",
      " [0.        ]]\n",
      "last_upper_lambda[[-0.42389584]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.1441049 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa7e5b01660>]"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def constraint(v, G, c):\n",
    "    return -G.T @ v - c\n",
    "\n",
    "def project_onto_lambda(v, G, c, d):\n",
    "    # print(constraint(np.linalg.lstsq(-G.detach().clone().numpy().T, c.detach().clone().numpy())[0], G, c))\n",
    "\n",
    "    # using the formula x = z - A.T(AA.T)^-1(Az-b) to project v onto the set Ax=b\n",
    "    # in this example, A = -G.T, b = c\n",
    "    if np.linalg.det(G.T@G) != 0:\n",
    "        projection = v + G @ torch.linalg.inv(G.T@G) @ (-G.T @ v - c)\n",
    "    else:\n",
    "        projection = v + G @ torch.linalg.pinv(G.T@G) @ (-G.T @ v - c)\n",
    "\n",
    "    return projection\n",
    "\n",
    "def obj_fn(x, d, lambda_, A, b, c, ub=False):\n",
    "    if ub:\n",
    "        return -1*(c.T@x + d) + lambda_.T@(A@x - b)\n",
    "    else:\n",
    "        return c.T@x + d + lambda_.T@(A@x - b)\n",
    "    \n",
    "def get_penalty(lambda_, G, c, scale=5, lb=True):\n",
    "    if lb:\n",
    "        return -scale*(torch.sum(torch.abs(constraint(lambda_, G, c))) + torch.sum(torch.abs(torch.clamp(lambda_, max=0.0))))\n",
    "    else:\n",
    "        return -scale*(torch.sum(torch.abs(constraint(lambda_, G, c))) + torch.sum(torch.abs(torch.clamp(lambda_, min=0.0))))\n",
    "\n",
    "lower_x = torch.tensor([[-5], [-5]]).float()\n",
    "upper_x = torch.tensor([[6], [6]]).float()\n",
    "lambda_lower = torch.rand(5,1, requires_grad=True)\n",
    "lambda_upper = torch.rand(5,1, requires_grad=True)\n",
    "lower_c = torch.tensor([[1], [1]]).float()\n",
    "lower_d = torch.tensor([[0]]).float()\n",
    "upper_c = torch.tensor([[12/22],[12/22]]).float()\n",
    "upper_d = torch.tensor([[120/22]]).float()\n",
    "G = torch.tensor([[-2/9, 1],[-6/5, 1], [-5/3, -1], [1/8, -1], [5, 1]]).float()\n",
    "h = torch.tensor([[46/9], [6], [25/3], [19/4], [26]]).float()\n",
    "\n",
    "project_onto_lambda(lambda_lower, G, lower_c,0)\n",
    "\n",
    "# print(f\"Initial Lower Lambda: {lambda_lower.data}\\nInitial Upper Lambda: {lambda_upper.data}\\nInitial lower \\alpha: {lower_c}\\n Initial upper \\alpha: {upper_c}\")\n",
    "\n",
    "# Number of optimization steps\n",
    "lr = 0.1\n",
    "num_steps = 40\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "lambda_vals = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "lambda_vals = np.vstack((lambda_vals, np.zeros((3, num_steps))))\n",
    "\n",
    "# opt = torch.optim.Adam([lambda_, c], lr=lr, maximize=True)\n",
    "opt_lower = torch.optim.Adam([lambda_lower], lr=lr, maximize=True)\n",
    "opt_upper = torch.optim.Adam([lambda_upper], lr=lr, maximize=True)\n",
    "scheduler_lower = torch.optim.lr_scheduler.ExponentialLR(opt_lower, 0.98)\n",
    "scheduler_upper = torch.optim.lr_scheduler.ExponentialLR(opt_upper, 0.98)\n",
    "\n",
    "## Projections and constraints should not be included in the Adam optimizer\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    lower_y = obj_fn(lower_x, lower_d, lambda_lower, G, h, lower_c) + get_penalty(lambda_lower, G, lower_c, lb=True)\n",
    "    upper_y = -1*(obj_fn(upper_x, upper_d, lambda_upper, G, h, upper_c) - get_penalty(lambda_upper, G, upper_c, lb=False))\n",
    "\n",
    "    \n",
    "    loss_graph[1, step] = lower_y.item()\n",
    "\n",
    "    if step == num_steps - 1:\n",
    "        last_lower_loss = lower_y.item()\n",
    "        last_upper_loss = upper_y.item()\n",
    "        last_lower_lambda = lambda_lower.detach().clone().numpy()\n",
    "        last_upper_lambda = lambda_upper.detach().clone().numpy()\n",
    "\n",
    "    opt_lower.zero_grad(set_to_none=True)\n",
    "    opt_upper.zero_grad(set_to_none=True)\n",
    "\n",
    "    lower_y.backward()\n",
    "    upper_y.backward()\n",
    "\n",
    "    opt_lower.step()\n",
    "    opt_upper.step()\n",
    "    scheduler_lower.step()\n",
    "    scheduler_upper.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        lambda_upper.data = torch.clamp(lambda_upper.data, max=0.0)\n",
    "        lambda_lower.data = torch.clamp(lambda_lower.data, min=0.0)\n",
    "        lambda_vals[1:3,step] = lambda_lower.detach().clone().numpy().flatten()[:2]\n",
    "        \n",
    "# lambda_lower.data = project_onto_lambda(lambda_lower.data, G, lower_c,0)\n",
    "lambda_lower_optimized = lambda_lower.data\n",
    "lambda_upper_optimized = lambda_upper.data\n",
    "# alpha_optimize = c.data\n",
    "\n",
    "print(\"Optimized lambda:\", lambda_lower_optimized, lambda_upper_optimized)\n",
    "# print(\"Optimized alpha:\", alpha_optimize)\n",
    "\n",
    "print(f\"CROWN lower bound: {obj_fn(lower_x, lower_d, torch.zeros(5,1).float(), G, h, lower_c).squeeze()}, Lagrange lower bound: {obj_fn(lower_x, lower_d, lambda_lower, G, h, lower_c).squeeze()}\")\n",
    "print(f\"CROWN upper bound: {obj_fn(upper_x, upper_d, torch.zeros(5,1).float(), G, h, upper_c).squeeze()}, Lagrange upper bound: {obj_fn(upper_x, upper_d, lambda_upper, G, h, upper_c).squeeze()}\")\n",
    "\n",
    "print(f\"last_lower_loss {last_lower_loss}\\nlast_upper_loss{last_upper_loss}\\nlast_lower_lambda{last_lower_lambda}\\nlast_upper_lambda{last_upper_lambda}\")\n",
    "assert last_lower_loss <= -7, f\"Last lower loss was {last_lower_loss}\"\n",
    "assert -1*last_upper_loss >= 1.090909091e+01, f\"Last upper loss was {last_upper_loss}\"\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(f\"Optimization Lambda (converged to {loss_graph[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(f\"\\Lambda_1 (converged to {lambda_vals[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[1,:])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(f\"\\Lambda_2 (converged to {lambda_vals[2,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[2,:])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(f\"\\alpha (converged to {lambda_vals[3,-1]:1.1f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[3,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking inspiration from this [post](https://stackoverflow.com/questions/77508682/correct-way-to-do-lagrange-dual-optimization-pytorch), we will use the PyTorch Adam Optimizer to solve the Lagrangain dual problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an optimizer on the linear system of equations:\n",
    "$$\n",
    "x_0 + x_1 - 5 = 0 (eq.1)\n",
    "$$\n",
    "$$\n",
    "2x_0 - x_1 + 3 = 0 (eq.2)\n",
    "$$\n",
    "where the loss is defined to be:\n",
    "$$\n",
    "(eq.1)^2 + (eq.2)^2\n",
    "$$\n",
    "and the Adam optimizer is being used to minimize this loss. \n",
    "The exact solutions are \n",
    "$$\n",
    "x_0 = \\frac{2}{3}\n",
    "$$\n",
    "$$\n",
    "x_1 = \\frac{13}{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 34.0\n",
      "Step 100, Loss: 20.551959991455078\n",
      "Step 200, Loss: 11.665807723999023\n",
      "Step 300, Loss: 6.137031555175781\n",
      "Step 400, Loss: 2.9643564224243164\n",
      "Step 500, Loss: 1.3036264181137085\n",
      "Step 600, Loss: 0.5183063745498657\n",
      "Step 700, Loss: 0.18529455363750458\n",
      "Step 800, Loss: 0.05931048095226288\n",
      "Step 900, Loss: 0.016934897750616074\n",
      "Step 1000, Loss: 0.004297736566513777\n",
      "Step 1100, Loss: 0.0009654018795117736\n",
      "Step 1200, Loss: 0.00019103451631963253\n",
      "Step 1300, Loss: 3.309983731014654e-05\n",
      "Step 1400, Loss: 4.998842086934019e-06\n",
      "Optimized solution: [0.6665166 4.33259  ]\n"
     ]
    }
   ],
   "source": [
    "# Define your system of equations as a function\n",
    "def equations(x):\n",
    "    eq1 = x[0] + x[1] - 5\n",
    "    eq2 = 2*x[0] - x[1] + 3\n",
    "    return eq1, eq2\n",
    "\n",
    "# Initialize the variables\n",
    "x = torch.tensor([0.,0.], requires_grad=True)\n",
    "\n",
    "# Define the Adam optimizer\n",
    "optimizer = Adam([x], lr=0.01, maximize=False)\n",
    "\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-6\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "    # Compute the system of equations\n",
    "    eq1, eq2 = equations(x)\n",
    "    \n",
    "    # Define the loss as the sum of squared equations\n",
    "    loss = eq1**2 + eq2**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the variables\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an optimizer on the quadratic equation:\n",
    "$$\n",
    "y = x_t^2\n",
    "$$\n",
    "where the loss is defined to be:\n",
    "$$\n",
    "y\n",
    "$$\n",
    "and the Adam optimizer is being used to minimize this loss. \n",
    "The exact solution is\n",
    "$$\n",
    "x_t = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t: tensor([[0.9906]], requires_grad=True)\n",
      "Step 0, Loss: 0.981256365776062\n",
      "Step 100, Loss: 0.04736644774675369\n",
      "Step 200, Loss: 0.00020352439605630934\n",
      "Step 300, Loss: 1.983038089292677e-08\n",
      "Optimized solution: [[9.470147e-05]]\n"
     ]
    }
   ],
   "source": [
    "# test on a simple parabola\n",
    "x_t = torch.rand(1,1, requires_grad=True)\n",
    "print(f\"x_t: {x_t}\")\n",
    "optimizer = torch.optim.Adam([x_t], lr=0.01, maximize=False)\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "\n",
    "    loss = x_t**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x_t.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t: tensor([[0.2674]], requires_grad=True)\n",
      "Step 0, Loss: -0.07150442898273468\n",
      "Optimized solution: [[7.715425e-06]]\n"
     ]
    }
   ],
   "source": [
    "# test on a simple parabola\n",
    "x_t = torch.rand(1,1, requires_grad=True)\n",
    "print(f\"x_t: {x_t}\")\n",
    "optimizer = torch.optim.Adam([x_t], lr=0.01, maximize=True)\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "\n",
    "    loss = -x_t**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x_t.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Gurobi will be needed to validate answers later on, here is a simple example using the Gurobi module.\n",
    "\n",
    "minimize $5x + 4y$\n",
    "\n",
    "subject to\n",
    "$$x + y \\geq 8$$\n",
    "$$2x + y \\geq 10$$\n",
    "$$x + 4y \\geq 11$$\n",
    "$$x \\geq 0$$\n",
    "$$y \\geq 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2024-10-28\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 3 rows, 2 columns and 6 nonzeros\n",
      "Model fingerprint: 0x6c7cdc94\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 4e+00]\n",
      "  Objective range  [4e+00, 5e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [8e+00, 1e+01]\n",
      "Presolve time: 0.00s\n",
      "Presolved: 3 rows, 2 columns, 6 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    0.0000000e+00   1.850000e+01   0.000000e+00      0s\n",
      "       2    3.4000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  3.400000000e+01\n",
      "Objective Function Value: 34.000000\n",
      "x: 2\n",
      "y: 6\n"
     ]
    }
   ],
   "source": [
    "from gurobipy import *\n",
    "opt_mod = Model(name = \"simple_linear_program_1\")\n",
    "\n",
    "# add variables\n",
    "x = opt_mod.addVar(name='x', vtype=GRB.CONTINUOUS, lb=0)\n",
    "y = opt_mod.addVar(name='y', vtype=GRB.CONTINUOUS, lb=0)\n",
    "\n",
    "# set the objective function\n",
    "obj_fn = 5*x + 4*y\n",
    "opt_mod.setObjective(obj_fn, GRB.MINIMIZE)\n",
    "\n",
    "# adding the constraints\n",
    "c1 = opt_mod.addConstr(x + y >= 8, name='c1')\n",
    "c2 = opt_mod.addConstr(2*x + y >= 10, name='c2')\n",
    "c3 = opt_mod.addConstr(x + 4*y >= 11, name='c3')\n",
    "\n",
    "# now optimize the problem and save it to a file\n",
    "opt_mod.optimize()\n",
    "opt_mod.write(\"simpe_linear_model_one.lp\")\n",
    "\n",
    "# output the result\n",
    "print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "# Get values of the decision variables\n",
    "for v in opt_mod.getVars():\n",
    "    print('%s: %g' % (v.VarName, v.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use the Adam optimizer to solve a Lagrangian dual problem. The objective function be to minimized is: \n",
    "$$\n",
    "2x_1 + 4x_2 = 0\n",
    "$$\n",
    "And the constraints are: \n",
    "$$\n",
    "-x_1 - 5 = 0\n",
    "$$\n",
    "$$\n",
    "-x_2 - 5 = 0\n",
    "$$\n",
    "The Lagrangian thus becomes:\n",
    "$$\n",
    "L(x, \\lambda) = 2x_1 + 4x_2 + \\lambda_1(-x_1 - 5) + \\lambda_2(-x_2 - 5)\n",
    "$$\n",
    "\n",
    "For my personal purposes, I do not need to modify $x$, solely $\\lambda$, therefore this is not a dual optimization problem but a simple maximization of $\\lambda$. \n",
    "\n",
    "Thus, assuming $x$ is given and does not violate constraints, we will obtain the gradients:\n",
    "$$\n",
    "\\nabla_{x_1}L(x,\\lambda) = 2 - \\lambda_1 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{x_2}L(x,\\lambda) = 4 - \\lambda_2 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{\\lambda_1}L(x,\\lambda) = -x_1 - 5 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{\\lambda_2}L(x,\\lambda) = -x_2 - 5 = 0\n",
    "$$\n",
    "Giving the exact solutions:\n",
    "$$\n",
    "x_1 = -5\n",
    "$$\n",
    "$$\n",
    "x_2 = -5\n",
    "$$\n",
    "$$\n",
    "\\lambda_1 = 2\n",
    "$$\n",
    "$$\n",
    "\\lambda_2 = 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5. -5.  2.  4.]\n"
     ]
    }
   ],
   "source": [
    "# double checking that these are indeed the exact solutions as I stated above\n",
    "A = np.array([[0,0,-1,0],[0,0,0,-1],[-1,0,0,0],[0,-1,0,0]])\n",
    "b = np.array([-2,-4,5,5])\n",
    "x = np.linalg.solve(A,b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[-1.  0.]\n",
      " [ 0. -1.]]\n",
      "b: [[-5.]\n",
      " [-5.]]\n",
      "c: [[2.]\n",
      " [4.]]\n"
     ]
    }
   ],
   "source": [
    "c = np.array([2,4], dtype='float32').reshape(-1,1)\n",
    "n = 2 # input of dimension 2\n",
    "m = 2 # 2 constraints\n",
    "A = np.array([[-1, 0], [0, -1]], dtype='float32')\n",
    "b = np.array([-5,-5], dtype='float32').reshape(-1, 1)\n",
    "print(f\"A: {A}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[-1.  0.]\n",
      " [ 0. -1.]]\n",
      "b: [[-5.]\n",
      " [-5.]]\n",
      "c: [[2.]\n",
      " [4.]]\n",
      "x_t: [[-5.]\n",
      " [-5.]]\n",
      "Init lagrange_multiplier [[0.873101 ]\n",
      " [0.8700457]]\n",
      "lagrangian shape: torch.Size([])\n",
      "Shape objective torch.Size([1, 1]), Shape constraint torch.Size([2, 1])\n",
      "objective: tensor([[-30.]], grad_fn=<MmBackward0>)\n",
      "constraint: tensor([[0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n",
      "lagrangian: -30.0\n",
      "Step 0, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 340000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m lagrangian\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# update values\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mopt_lagrange\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c_t = torch.tensor(c).float()\n",
    "A_t = torch.tensor(A).float()\n",
    "b_t = torch.tensor(b).float().reshape(-1,1)\n",
    "# x_t = torch.rand(n, 1, requires_grad=True)\n",
    "x_t = torch.tensor([-5.0,-5.0], requires_grad=True).float().reshape(-1,1)\n",
    "print(f\"A: {A_t.detach().numpy()}\")\n",
    "print(f\"b: {b_t.detach().numpy()}\")\n",
    "print(f\"c: {c_t.detach().numpy()}\")\n",
    "print(f\"x_t: {x_t.detach().numpy()}\")\n",
    "\n",
    "_lagrange_multiplier = torch.rand(m,1, requires_grad=True)\n",
    "lagrange_multiplier = torch.nn.functional.softplus(_lagrange_multiplier)\n",
    "print(f\"Init lagrange_multiplier {lagrange_multiplier.detach().numpy()}\")\n",
    "\n",
    "# opt_weights = torch.optim.Adam([x_t], lr=0.1)\n",
    "opt_lagrange = torch.optim.Adam([_lagrange_multiplier], lr=0.1, maximize=True)\n",
    "\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "    \n",
    "\n",
    "    objective = c_t.T @ x_t\n",
    "    constraint = A_t @ x_t + b_t\n",
    "    \n",
    "    lagrange_multiplier = torch.nn.functional.softplus(_lagrange_multiplier)\n",
    "    lagrangian = objective + lagrange_multiplier.T @ constraint\n",
    "    lagrangian = lagrangian.squeeze()\n",
    "    if step == 0:\n",
    "        print(f\"lagrangian shape: {lagrangian.shape}\")\n",
    "        print(f\"Shape objective {objective.shape}, Shape constraint {constraint.shape}\")\n",
    "        print(f\"objective: {objective}\")\n",
    "        print(f\"constraint: {constraint}\")\n",
    "        print(f\"lagrangian: {lagrangian}\")\n",
    "\n",
    "        # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {lagrangian.item()}, lambda_1: {_lagrange_multiplier[0]}, lambda_2: {_lagrange_multiplier[1]}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(lagrangian.item()) < loss_threshold:\n",
    "        break\n",
    "        \n",
    "    # zero the gradient\n",
    "    opt_lagrange.zero_grad()\n",
    "\n",
    "    # compute the gradient\n",
    "    lagrangian.backward()\n",
    "\n",
    "    # update values\n",
    "    opt_lagrange.step()\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: -5.0 is leaf True, x2: -5.0 is leaf True, x3: tensor([0.0566], requires_grad=True) is leaf True, x4: tensor([0.8357], requires_grad=True) is leaf True\n",
      "Optimized x1: -5.0\n",
      "Optimized x2: -5.0\n",
      "Optimized x3: 0.05660974979400635\n",
      "Optimized x4: 0.835723876953125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9e002a050>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Adam\n",
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.tensor(-5.0, requires_grad=False)\n",
    "x2_opt = torch.tensor(-5.0, requires_grad=False)\n",
    "x3_opt = torch.rand(1, requires_grad=True)\n",
    "x4_opt = torch.rand(1, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 1000\n",
    "lr = 0.1\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "opt = torch.optim.Adam([x3_opt, x4_opt], lr=lr, maximize=True)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, 0.98)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt).sum()\n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    y.backward()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    # clip lagrange values\n",
    "    x3_opt.data = torch.clip(x3_opt.data, 0)\n",
    "    x4_opt.data = torch.clip(x4_opt.data, 0)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([0.6116], dtype=torch.float64, requires_grad=True) is leaf True, x2: tensor([0.0295], dtype=torch.float64, requires_grad=True) is leaf True, x3: tensor([0.1955], dtype=torch.float64, requires_grad=True) is leaf True, x4: tensor([0.9048], dtype=torch.float64, requires_grad=True) is leaf True\n",
      "Optimized x1: -4.55081800782109\n",
      "Optimized x2: -5.225714517952417\n",
      "Optimized x3: 0.0\n",
      "Optimized x4: 0.29730698177256015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fda17360160>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Adam\n",
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x2_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x3_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x4_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 10000 + 20\n",
    "lr = 0.1\n",
    "\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "opt_duals = torch.optim.Adam([x3_opt, x4_opt], lr=lr, maximize=True)\n",
    "opt_x = torch.optim.Adadelta([x1_opt, x2_opt], lr=lr, maximize=False)\n",
    "scheduler_dual = torch.optim.lr_scheduler.ExponentialLR(opt_duals, 0.98)\n",
    "scheduler_x = torch.optim.lr_scheduler.ExponentialLR(opt_x, 0.98)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt).sum()\n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt_x.zero_grad(set_to_none=True)\n",
    "    opt_duals.zero_grad(set_to_none=True)\n",
    "    y.backward()\n",
    "\n",
    "    if flip:\n",
    "        opt_x.step()\n",
    "        # if step % 20 == 0:\n",
    "        #     scheduler_x.step()\n",
    "    else:\n",
    "        opt_duals.step()\n",
    "        # if step % 20 == 0:\n",
    "        #     scheduler_dual.step()\n",
    "    \n",
    "    # clip lagrange values\n",
    "    x3_opt.data = torch.clip(x3_opt.data, 0)\n",
    "    x4_opt.data = torch.clip(x4_opt.data, 0)\n",
    "\n",
    "    if step != 0 and step % 60 == 0:\n",
    "        flip = not flip\n",
    "    \n",
    "    if loss_graph[1, step] < -29.5 and loss_graph[1,step] > -30.5 and flip == False:\n",
    "        loss_graph[1, step:] = loss_graph[1,step]\n",
    "        break\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([0.7194], requires_grad=True) is leaf True, x2: tensor([0.2687], requires_grad=True) is leaf True, x3: tensor([0.7701], requires_grad=True) is leaf True, x4: tensor([0.8334], requires_grad=True) is leaf True\n",
      "Optimized x1: -4.999998092651367\n",
      "Optimized x2: -4.999998092651367\n",
      "Optimized x3: 1.9999995231628418\n",
      "Optimized x4: 3.9999990463256836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdffc1509d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1, requires_grad=True)\n",
    "x2_opt = torch.rand(1, requires_grad=True)\n",
    "x3_opt = torch.rand(1, requires_grad=True)\n",
    "x4_opt = torch.rand(1, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "def zero_grad(parameters):\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 1000\n",
    "lr = 0.1\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt)\n",
    "    y.backward()\n",
    "\n",
    "    grad1 = x1_opt.grad if x1_opt.grad is not None else 0.0\n",
    "    grad2 = x2_opt.grad if x2_opt.grad is not None else 0.0\n",
    "    grad3 = x3_opt.grad if x3_opt.grad is not None else 0.0\n",
    "    grad4 = x4_opt.grad if x4_opt.grad is not None else 0.0\n",
    "\n",
    "    loss_graph[1, step] = y.item()\n",
    "    \n",
    "    if flip:\n",
    "        # when this is true, we are minimizing L(x,lambda) w.r.t. x\n",
    "        x1_opt.data = (x1_opt.data + lr*grad3).requires_grad_(True)\n",
    "        x2_opt.data = (x2_opt.data + lr*grad4).requires_grad_(True)        \n",
    "\n",
    "    else:\n",
    "        # when this is false, we are maximizing L(x,lambda) w.r.t. lambda\n",
    "        x3_opt.data = torch.clamp(x3_opt.data + lr*grad1, min=0.0).requires_grad_(True)\n",
    "        x4_opt.data = torch.clamp(x4_opt.data + lr*grad2, min=0.0).requires_grad_(True)\n",
    "\n",
    "    # if step != 0 and (step % 100) == 0:\n",
    "    #     print(f\"Step {step}, Loss: {y.item():.4f}, x1: {x1_opt.detach().numpy()[0]:.4f} x2: {x2_opt.detach().numpy()[0]:.4f} lambda_1: {x3_opt.detach().numpy()[0]:.4f}, lambda_2: {x4_opt.detach().numpy()[0]:.4f}, grads: [{x1_opt.grad.detach().numpy()[0]:.4f}, {x2_opt.grad.detach().numpy()[0]:.4f}, {x3_opt.grad.detach().numpy()[0]:.4f}, {x4_opt.grad.detach().numpy()[0]:.4f}]\")\n",
    "\n",
    "    if step != 0 and (step % 100) == 0:\n",
    "        flip = not flip\n",
    "        \n",
    "    # zero out the gradients\n",
    "    zero_grad([x1_opt, x2_opt, x3_opt, x4_opt])\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x1: -0.8333324193954468\n",
      "Optimized x2: -0.8333352208137512\n",
      "Optimized l1: 1.045896053314209\n",
      "Optimized l2: 0.2871112823486328\n",
      "Optimized l3: 0.0\n",
      "Optimized l4: 1.4137334823608398\n",
      "Optimized l5: 0.5862621068954468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdce8c83550>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1, requires_grad=True)\n",
    "x2_opt = torch.rand(1, requires_grad=True)\n",
    "l1_opt = torch.rand(1, requires_grad=True)\n",
    "l2_opt = torch.rand(1, requires_grad=True)\n",
    "l3_opt = torch.rand(1, requires_grad=True)\n",
    "l4_opt = torch.rand(1, requires_grad=True)\n",
    "l5_opt = torch.rand(1, requires_grad=True)\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, l1, l2, l3, l4, l5):\n",
    "    return x1 + 2*x2 + l1*(x1-2) + l2*(-x1-2) + l3*(x2-2) + l4*(-x2-2) + l5*(-3*x1 - x2 - 5)\n",
    "\n",
    "def zero_grad(parameters):\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 10000\n",
    "lr = 0.01\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, l1_opt, l2_opt, l3_opt, l4_opt, l5_opt)\n",
    "    y.backward()\n",
    "\n",
    "    x_grad1 = x1_opt.grad if x1_opt.grad is not None else 0.0\n",
    "    x_grad2 = x2_opt.grad if x2_opt.grad is not None else 0.0\n",
    "    l_grad1 = l1_opt.grad if l1_opt.grad is not None else 0.0\n",
    "    l_grad2 = l2_opt.grad if l2_opt.grad is not None else 0.0\n",
    "    l_grad3 = l3_opt.grad if l3_opt.grad is not None else 0.0\n",
    "    l_grad4 = l4_opt.grad if l4_opt.grad is not None else 0.0\n",
    "    l_grad5 = l5_opt.grad if l5_opt.grad is not None else 0.0\n",
    "\n",
    "    loss_graph[1, step] = y.item()\n",
    "    \n",
    "    if flip:\n",
    "        # when this is true, we are minimizing L(x,lambda) w.r.t. x\n",
    "        x1_opt.data = (x1_opt.data + lr*(-l_grad1 + l_grad2 + l_grad5)).requires_grad_(True)\n",
    "        x2_opt.data = (x2_opt.data + lr*(-l_grad3 + l_grad4 + l_grad5)).requires_grad_(True)        \n",
    "\n",
    "    else:\n",
    "        # when this is false, we are maximizing L(x,lambda) w.r.t. lambda\n",
    "        l1_opt.data = torch.clamp(l1_opt.data + lr*(-x_grad1), min=0.0).requires_grad_(True)\n",
    "        l2_opt.data = torch.clamp(l2_opt.data + lr*(x_grad1), min=0.0).requires_grad_(True)\n",
    "        l3_opt.data = torch.clamp(l3_opt.data + lr*(-x_grad2), min=0.0).requires_grad_(True)\n",
    "        l4_opt.data = torch.clamp(l4_opt.data + lr*(x_grad2), min=0.0).requires_grad_(True)\n",
    "        l5_opt.data = torch.clamp(l5_opt.data + lr*(x_grad1 + x_grad2), min=0.0).requires_grad_(True)\n",
    "\n",
    "    if step != 0 and (step % 100) == 0:\n",
    "        flip = not flip\n",
    "        \n",
    "    # zero out the gradients\n",
    "    zero_grad([x1_opt, x2_opt, l1_opt, l2_opt, l3_opt, l4_opt, l5_opt])\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "l1_optimized = l1_opt.item()\n",
    "l2_optimized = l2_opt.item()\n",
    "l3_optimized = l3_opt.item()\n",
    "l4_optimized = l4_opt.item()\n",
    "l5_optimized = l5_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized l1:\", l1_optimized)\n",
    "print(\"Optimized l2:\", l2_optimized)\n",
    "print(\"Optimized l3:\", l3_optimized)\n",
    "print(\"Optimized l4:\", l4_optimized)\n",
    "print(\"Optimized l5:\", l5_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am checking the results of the Lagrange problem above by computing the upper and lower bounds on x as well as the minimal and maximal pertubation on each logit output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Model fingerprint: 0xf5f961a3\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [3e+00, 4e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Presolve removed 5 rows and 2 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    1.6000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.600000000e+01\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 2e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -3.0000000e+30   3.000000e+30   3.000000e+00      0s\n",
      "       3   -5.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 3 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -5.000000000e+00\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [3e+00, 4e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    7.0000000e+30   2.000000e+30   7.000000e+00      0s\n",
      "       2    1.6000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.600000000e+01\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 3e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -4.0000000e+30   3.000000e+30   4.000000e+00      0s\n",
      "       3   -5.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 3 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -5.000000000e+00\n",
      "logit 1 is bounded s.t. -5.0 <= z(x) <= 16.0 with lb pertubation (-1.0, -2.0) and ub pertubation (2.0, 2.0)\n",
      "logit 2 is bounded s.t. -5.0 <= z(x) <= 16.0 with lb pertubation (-1.0, -2.0) and ub pertubation (2.0, 2.0)\n"
     ]
    }
   ],
   "source": [
    "W_ub = np.array([[4,3],[4,3]])\n",
    "b_ub = np.array([2,2])\n",
    "W_lb = np.array([[1,2],[1,1]])\n",
    "# b_lb = np.array([1,1])\n",
    "b_lb = np.zeros(2)\n",
    "# using Gurobi to solve the same problem as above\n",
    "opt_mod = Model(name = \"simple_linear_program_2\")\n",
    "\n",
    "# add variables\n",
    "inputs = np.array(list(opt_mod.addVars(W_ub.shape[1], name=\"x\", lb=float(\"-inf\"), ub=float(\"inf\")).values()))\n",
    "\n",
    "# adding the constraints\n",
    "c1 = opt_mod.addConstr(inputs[0] - 2 <= 0, name='c1') # these four constraints are the l_inf norm box constraints\n",
    "c2 = opt_mod.addConstr(-inputs[0] - 2 <= 0, name='c2')\n",
    "c3 = opt_mod.addConstr(inputs[1] - 2 <= 0, name='c3')\n",
    "c4 = opt_mod.addConstr(-inputs[1] - 2 <= 0, name='c4')\n",
    "c5 = opt_mod.addConstr(-3*inputs[0] - inputs[1] - 5 <= 0, name='c5') # this constraint is a line constraint cutting through the box\n",
    "\n",
    "worst_case_inputs_ub = []\n",
    "worst_case_inputs_lb = []\n",
    "upper_bounds = []\n",
    "lower_bounds = []\n",
    "\n",
    "# set the objective function for each logit\n",
    "for idx in range(2*W_ub.shape[0]):\n",
    "    i = idx // 2\n",
    "    if idx % 2 == 0:\n",
    "        obj_fn = quicksum([W_ub[i,j]*inputs[j] for j in range(W_ub.shape[1])]) + b_ub[i]\n",
    "        opt_mod.setObjective(obj_fn, GRB.MAXIMIZE)\n",
    "    else:\n",
    "        obj_fn = quicksum([W_lb[i,j]*inputs[j] for j in range(W_lb.shape[1])]) + b_lb[i]\n",
    "        opt_mod.setObjective(obj_fn, GRB.MINIMIZE)\n",
    "\n",
    "    # now optimize the problem and save it to a file\n",
    "    opt_mod.optimize()\n",
    "    # opt_mod.write(\"scenario_one_upperbound_logit_one.lp\")\n",
    "\n",
    "    # output the result\n",
    "    # print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "    if idx % 2 == 0:\n",
    "        upper_bounds.append(opt_mod.ObjVal)\n",
    "    else:\n",
    "        lower_bounds.append(opt_mod.ObjVal)\n",
    "    # Get values of the decision variables\n",
    "    temp_inputs = []\n",
    "    for v in opt_mod.getVars():\n",
    "        # print('%s: %g' % (v.VarName, v.x))\n",
    "        temp_inputs.append(v.x)\n",
    "\n",
    "    if idx % 2 == 0:\n",
    "        worst_case_inputs_ub.append(tuple(temp_inputs)) # append the worst case input that caused this maximal pertubation\n",
    "    else:\n",
    "        worst_case_inputs_lb.append(tuple(temp_inputs)) # append the worst case input that caused this maximal pertubation\n",
    "    \n",
    "for i in range(W_ub.shape[0]):\n",
    "    print(f\"logit {i + 1} is bounded s.t. {lower_bounds[i]} <= z(x) <= {upper_bounds[i]} with lb pertubation {worst_case_inputs_lb[i]} and ub pertubation {worst_case_inputs_ub[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal Result:\n",
      "       message: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n",
      "       success: True\n",
      "        status: 0\n",
      "           fun: 0.0\n",
      "             x: [ 0.000e+00  0.000e+00]\n",
      "           nit: 0\n",
      "         lower:  residual: [ 0.000e+00  0.000e+00]\n",
      "                marginals: [ 2.000e+00  3.000e+00]\n",
      "         upper:  residual: [       inf        inf]\n",
      "                marginals: [ 0.000e+00  0.000e+00]\n",
      "         eqlin:  residual: []\n",
      "                marginals: []\n",
      "       ineqlin:  residual: [ 1.000e+00  2.000e+00]\n",
      "                marginals: [-0.000e+00 -0.000e+00]\n",
      "\n",
      "Dual Result:\n",
      "       message: The problem is unbounded. (HiGHS Status 10: model_status is Unbounded; primal_status is At upper bound)\n",
      "       success: False\n",
      "        status: 3\n",
      "           fun: None\n",
      "             x: None\n",
      "           nit: 0\n",
      "         lower:  residual: None\n",
      "                marginals: None\n",
      "         upper:  residual: None\n",
      "                marginals: None\n",
      "         eqlin:  residual: None\n",
      "                marginals: None\n",
      "       ineqlin:  residual: None\n",
      "                marginals: None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# Primal linear program coefficients\n",
    "c = np.array([2, 3])\n",
    "A = np.array([[1, -1], [3, 1]])\n",
    "b = np.array([1, 2])\n",
    "\n",
    "# Solve the primal linear program\n",
    "result_primal = linprog(c, A_ub=A, b_ub=b, method='highs')\n",
    "\n",
    "# Display the primal result\n",
    "print(\"Primal Result:\")\n",
    "print(result_primal)\n",
    "\n",
    "# Dual linear program coefficients\n",
    "c_dual = -b  # Coefficients are negated for maximization\n",
    "A_dual = -A.T  # Transpose of A with negation\n",
    "b_dual = c  # Dual variables corresponding to the inequality constraints\n",
    "\n",
    "# Solve the dual linear program\n",
    "result_dual = linprog(c_dual, A_ub=A_dual, b_ub=b_dual, method='highs')\n",
    "\n",
    "# Display the dual result\n",
    "print(\"\\nDual Result:\")\n",
    "print(result_dual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [1.25 0.  ]\n",
      " [2.5  0.  ]\n",
      " [3.75 0.  ]\n",
      " [5.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# potentially look into this for solving the lagrange dual, but it requires the gradients to be determined beforehand\n",
    "from scipy.optimize import fsolve, fmin_l_bfgs_b\n",
    "\n",
    "a = 1\n",
    "nbtests = 5\n",
    "minmu = 0\n",
    "maxmu = 5\n",
    "\n",
    "def lagrange(x, mu):\n",
    "    return x**2 + mu * (np.exp(x) + x - a)\n",
    "\n",
    "def lagrange_grad(x, mu):\n",
    "    grad_x = 2*x + mu * (np.exp(x) + 1)\n",
    "    grad_mu = np.exp(x) + x - a\n",
    "    return grad_x, grad_mu\n",
    "\n",
    "def dual(mu):\n",
    "    x = fsolve(lambda x: lagrange_grad(x, mu)[0], x0=1)\n",
    "    obj_val = lagrange(x, mu)\n",
    "    grad = lagrange_grad(x, mu)[1]\n",
    "    return -1.0*obj_val, -1.0*grad\n",
    "\n",
    "pl = np.empty((nbtests, 2))\n",
    "for i, nu in enumerate(np.linspace(minmu,maxmu,nbtests)):\n",
    "    res = fmin_l_bfgs_b(dual, x0=nu, bounds=[(0,None)], factr=1e6)\n",
    "    mu_opt = res[0]\n",
    "    x_opt = fsolve(lambda x: lagrange_grad(x, mu_opt)[0], x0=1)\n",
    "    pl[i] = [nu, *x_opt]\n",
    "print(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last example, we will deal with a single dimension input and output. The input x will have a pertubation of $\\epsilon=2$ and be fed to a ReLU unit, then be added by 5. This will then be the final output. This first example will have no constraints, and if alpha is tuned correctly, it should be zero s.t. alpha-CROWN produces a lower bound of 0. The actualy lower and upper bound is 5 and 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same example as before but with the restriction that $x\\geq1$. We will add a Lagrange Multiplier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximizing $\\lambda$ after minimizing x. \n",
    "For a ReLU activation function, we may have an example where we wish to minimize $\\sigma(x)$ where $\\sigma$ is the ReLU function. If x is convex s.t. $-2\\leq x \\leq 2$, we may choose the lower bound on the input to be x and we will naturally find that the objection function is 0. The main issue is that ReLU is non-convex in nature, therefore we take inspiration from $\\alpha$-CROWN and use the linear lower bound $\\alpha$x to lower bound the ReLU function. This is the linear constraint that is used in the triangular relaxation of ReLU. Letting $alpha=1$ (note that we must have 0\\leq \\alpha \\leq 1) and adding constraints, $x \\geq -1$ and $x \\leq 2$, we now solve the dual program, $L(x, \\lambda)$. First minimize x and then maximize $\\lambda$. x exists in the space $-2 \\leq x \\leq 2$, therefore we know that the smallest value of x is x=-2. Though this x does not fit our constraints, if $\\lambda$ is optimal, we get the optimal objective value. Note that this minimum value will be -1 and not 0 because of the ReLU relaxation. The true minimum value is 0. \n",
    "\n",
    "## Question\n",
    "Ask Zhang if it is reasonable to choose route 1 where clipping and projection is included in the projection as this leads to tigher bounds yet still valid bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Lambda: tensor([[0.2355],\n",
      "        [0.5286]])\n",
      "Initial \u0007lpha: tensor([[0.2135]], requires_grad=True)\n",
      "Optimized lambda: tensor([[0.0000],\n",
      "        [0.2931]])\n",
      "Optimized alpha: tensor([[0.]])\n",
      "CROWN lower bound: 0.0, Lagrange lower bound: -0.29309558868408203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16343/3477208528.py:84: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  lambda_vals[3, step] = c.detach().clone().numpy().flatten()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb369d98cd0>]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.tensor([[-2]]).float()\n",
    "lambda_ = torch.rand(2,1, requires_grad=True)\n",
    "# c = torch.tensor([[1]]).float()\n",
    "c = torch.rand(1,1, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    c.data = torch.clamp(c.data, min=0, max=1)\n",
    "G = torch.tensor([[1],[1]]).float()\n",
    "h = torch.tensor([[2],[-1]]).float()\n",
    "\n",
    "print(f\"Initial Lambda: {lambda_.data}\\nInitial \\alpha: {c}\")\n",
    "\n",
    "def constraint(v, G, c):\n",
    "    return -G.T @ v - c\n",
    "\n",
    "def project_onto_lambda(v, G, c):\n",
    "    # using the formula x = z - A.T(AA.T)^-1(Az-b) to project v onto the set Ax=b\n",
    "    # in this example, A = -G.T, b = c\n",
    "    projection = v + G @ torch.linalg.pinv(G.T@G) @ (-G.T @ v - c)\n",
    "    min_val = torch.min(projection)\n",
    "    if min_val >= 0.0:\n",
    "        return projection\n",
    "    else:\n",
    "        return projection - min_val\n",
    "    \n",
    "def obj_fn(x, lambda_, A, b, c):\n",
    "    return c.T@x + lambda_.T@(A@x - b)\n",
    "\n",
    "# Number of optimization steps\n",
    "lr = 0.1\n",
    "num_steps = 20\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "lambda_vals = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "lambda_vals = np.vstack((lambda_vals, np.zeros((3, num_steps))))\n",
    "\n",
    "opt = torch.optim.Adam([lambda_, c], lr=lr, maximize=True)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, 0.98)\n",
    "\n",
    "## Though this converges much more tightly, it is frowned upon\n",
    "# Optimization loop\n",
    "# for step in range(num_steps):\n",
    "\n",
    "#     # y = obj_fn(x, lambda_, G, h, c)\n",
    "#     c.data = torch.clamp(c.data, min=0, max=1)\n",
    "#     projected_lambda = project_onto_lambda(lambda_, G, c)\n",
    "#     lambda_vals[1:3,step] = projected_lambda.detach().clone().numpy().flatten()\n",
    "#     y = obj_fn(x, projected_lambda, G, h, c)\n",
    "\n",
    "#     loss_graph[1, step] = y.item()\n",
    "\n",
    "#     opt.zero_grad(set_to_none=True)\n",
    "\n",
    "#     y.backward()\n",
    "\n",
    "#     opt.step()\n",
    "#     scheduler.step()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # c.data = torch.clamp(c.data, min=0, max=1)\n",
    "#         lambda_vals[3, step] = c.detach().clone().numpy().flatten()\n",
    "\n",
    "## Projections and constraints should not be included in the Adam optimizer\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    y = obj_fn(x, lambda_, G, h, c)\n",
    "    \n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    y.backward()\n",
    "\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lambda_.data = project_onto_lambda(lambda_, G, c)\n",
    "        lambda_vals[1:3,step] = lambda_.detach().clone().numpy().flatten()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        c.data = torch.clamp(c.data, min=0, max=1)\n",
    "        lambda_vals[3, step] = c.detach().clone().numpy().flatten()\n",
    "\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "# lambda_optimized = lambda_.data\n",
    "lambda_optimized = project_onto_lambda(lambda_, G, c).data\n",
    "alpha_optimize = c.data\n",
    "\n",
    "print(\"Optimized lambda:\", lambda_optimized)\n",
    "print(\"Optimized alpha:\", alpha_optimize)\n",
    "\n",
    "print(f\"CROWN lower bound: {obj_fn(x, torch.zeros(2,1).float(), G, h, c).squeeze()}, Lagrange lower bound: {loss_graph[1,-1]}\")\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(f\"Optimization Lambda (converged to {loss_graph[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(f\"\\Lambda_1 (converged to {lambda_vals[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[1,:])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(f\"\\Lambda_2 (converged to {lambda_vals[2,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[2,:])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(f\"\\alpha (converged to {lambda_vals[3,-1]:1.1f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[3,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

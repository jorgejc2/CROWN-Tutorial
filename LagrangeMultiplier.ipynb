{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking inspiration from this [post](https://stackoverflow.com/questions/77508682/correct-way-to-do-lagrange-dual-optimization-pytorch), we will use the PyTorch Adam Optimizer to solve the Lagrangain dual problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from gurobipy import GRB, quicksum, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an optimizer on the linear system of equations:\n",
    "$$\n",
    "x_0 + x_1 - 5 = 0 (eq.1)\n",
    "$$\n",
    "$$\n",
    "2x_0 - x_1 + 3 = 0 (eq.2)\n",
    "$$\n",
    "where the loss is defined to be:\n",
    "$$\n",
    "(eq.1)^2 + (eq.2)^2\n",
    "$$\n",
    "and the Adam optimizer is being used to minimize this loss. \n",
    "The exact solutions are \n",
    "$$\n",
    "x_0 = \\frac{2}{3}\n",
    "$$\n",
    "$$\n",
    "x_1 = \\frac{13}{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 34.0\n",
      "Step 100, Loss: 20.551959991455078\n",
      "Step 200, Loss: 11.665807723999023\n",
      "Step 300, Loss: 6.137031555175781\n",
      "Step 400, Loss: 2.9643564224243164\n",
      "Step 500, Loss: 1.3036264181137085\n",
      "Step 600, Loss: 0.5183063745498657\n",
      "Step 700, Loss: 0.18529455363750458\n",
      "Step 800, Loss: 0.05931048095226288\n",
      "Step 900, Loss: 0.016934897750616074\n",
      "Step 1000, Loss: 0.004297736566513777\n",
      "Step 1100, Loss: 0.0009654018795117736\n",
      "Step 1200, Loss: 0.00019103451631963253\n",
      "Step 1300, Loss: 3.309983731014654e-05\n",
      "Step 1400, Loss: 4.998842086934019e-06\n",
      "Optimized solution: [0.6665166 4.33259  ]\n"
     ]
    }
   ],
   "source": [
    "# Define your system of equations as a function\n",
    "def equations(x):\n",
    "    eq1 = x[0] + x[1] - 5\n",
    "    eq2 = 2*x[0] - x[1] + 3\n",
    "    return eq1, eq2\n",
    "\n",
    "# Initialize the variables \n",
    "x = torch.tensor([0.,0.], requires_grad=True)\n",
    "\n",
    "# Define the Adam optimizer\n",
    "optimizer = Adam([x], lr=0.01, maximize=False)\n",
    "\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-6\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "    # Compute the system of equations\n",
    "    eq1, eq2 = equations(x)\n",
    "    \n",
    "    # Define the loss as the sum of squared equations\n",
    "    loss = eq1**2 + eq2**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the variables\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an optimizer on the quadratic equation:\n",
    "$$\n",
    "y = x_t^2\n",
    "$$\n",
    "where the loss is defined to be:\n",
    "$$\n",
    "y\n",
    "$$\n",
    "and the Adam optimizer is being used to minimize this loss. \n",
    "The exact solution is\n",
    "$$\n",
    "x_t = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t: tensor([[0.9906]], requires_grad=True)\n",
      "Step 0, Loss: 0.981256365776062\n",
      "Step 100, Loss: 0.04736644774675369\n",
      "Step 200, Loss: 0.00020352439605630934\n",
      "Step 300, Loss: 1.983038089292677e-08\n",
      "Optimized solution: [[9.470147e-05]]\n"
     ]
    }
   ],
   "source": [
    "# test on a simple parabola\n",
    "x_t = torch.rand(1,1, requires_grad=True)\n",
    "print(f\"x_t: {x_t}\")\n",
    "optimizer = torch.optim.Adam([x_t], lr=0.01, maximize=False)\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "\n",
    "    loss = x_t**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x_t.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t: tensor([[0.2674]], requires_grad=True)\n",
      "Step 0, Loss: -0.07150442898273468\n",
      "Optimized solution: [[7.715425e-06]]\n"
     ]
    }
   ],
   "source": [
    "# test on a simple parabola\n",
    "x_t = torch.rand(1,1, requires_grad=True)\n",
    "print(f\"x_t: {x_t}\")\n",
    "optimizer = torch.optim.Adam([x_t], lr=0.01, maximize=True)\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "\n",
    "    loss = -x_t**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x_t.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Gurobi will be needed to validate answers later on, here is a simple example using the Gurobi module.\n",
    "\n",
    "minimize $5x + 4y$\n",
    "\n",
    "subject to\n",
    "$$x + y \\geq 8$$\n",
    "$$2x + y \\geq 10$$\n",
    "$$x + 4y \\geq 11$$\n",
    "$$x \\geq 0$$\n",
    "$$y \\geq 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2024-10-28\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 3 rows, 2 columns and 6 nonzeros\n",
      "Model fingerprint: 0x6c7cdc94\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 4e+00]\n",
      "  Objective range  [4e+00, 5e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [8e+00, 1e+01]\n",
      "Presolve time: 0.00s\n",
      "Presolved: 3 rows, 2 columns, 6 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    0.0000000e+00   1.850000e+01   0.000000e+00      0s\n",
      "       2    3.4000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  3.400000000e+01\n",
      "Objective Function Value: 34.000000\n",
      "x: 2\n",
      "y: 6\n"
     ]
    }
   ],
   "source": [
    "from gurobipy import *\n",
    "opt_mod = Model(name = \"simple_linear_program_1\")\n",
    "\n",
    "# add variables\n",
    "x = opt_mod.addVar(name='x', vtype=GRB.CONTINUOUS, lb=0)\n",
    "y = opt_mod.addVar(name='y', vtype=GRB.CONTINUOUS, lb=0)\n",
    "\n",
    "# set the objective function\n",
    "obj_fn = 5*x + 4*y\n",
    "opt_mod.setObjective(obj_fn, GRB.MINIMIZE)\n",
    "\n",
    "# adding the constraints\n",
    "c1 = opt_mod.addConstr(x + y >= 8, name='c1')\n",
    "c2 = opt_mod.addConstr(2*x + y >= 10, name='c2')\n",
    "c3 = opt_mod.addConstr(x + 4*y >= 11, name='c3')\n",
    "\n",
    "# now optimize the problem and save it to a file\n",
    "opt_mod.optimize()\n",
    "opt_mod.write(\"simpe_linear_model_one.lp\")\n",
    "\n",
    "# output the result\n",
    "print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "# Get values of the decision variables\n",
    "for v in opt_mod.getVars():\n",
    "    print('%s: %g' % (v.VarName, v.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use the Adam optimizer to solve a Lagrangian dual problem. The objective function be to minimized is: \n",
    "$$\n",
    "2x_1 + 4x_2 = 0\n",
    "$$\n",
    "And the constraints are: \n",
    "$$\n",
    "-x_1 - 5 = 0\n",
    "$$\n",
    "$$\n",
    "-x_2 - 5 = 0\n",
    "$$\n",
    "The Lagrangian thus becomes:\n",
    "$$\n",
    "L(x, \\lambda) = 2x_1 + 4x_2 + \\lambda_1(-x_1 - 5) + \\lambda_2(-x_2 - 5)\n",
    "$$\n",
    "\n",
    "For my personal purposes, I do not need to modify $x$, solely $\\lambda$, therefore this is not a dual optimization problem but a simple maximization of $\\lambda$. \n",
    "\n",
    "Thus, assuming $x$ is given and does not violate constraints, we will obtain the gradients:\n",
    "$$\n",
    "\\nabla_{x_1}L(x,\\lambda) = 2 - \\lambda_1 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{x_2}L(x,\\lambda) = 4 - \\lambda_2 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{\\lambda_1}L(x,\\lambda) = -x_1 - 5 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{\\lambda_2}L(x,\\lambda) = -x_2 - 5 = 0\n",
    "$$\n",
    "Giving the exact solutions:\n",
    "$$\n",
    "x_1 = -5\n",
    "$$\n",
    "$$\n",
    "x_2 = -5\n",
    "$$\n",
    "$$\n",
    "\\lambda_1 = 2\n",
    "$$\n",
    "$$\n",
    "\\lambda_2 = 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5. -5.  2.  4.]\n"
     ]
    }
   ],
   "source": [
    "# double checking that these are indeed the exact solutions as I stated above\n",
    "A = np.array([[0,0,-1,0],[0,0,0,-1],[-1,0,0,0],[0,-1,0,0]])\n",
    "b = np.array([-2,-4,5,5])\n",
    "x = np.linalg.solve(A,b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[-1.  0.]\n",
      " [ 0. -1.]]\n",
      "b: [[-5.]\n",
      " [-5.]]\n",
      "c: [[2.]\n",
      " [4.]]\n"
     ]
    }
   ],
   "source": [
    "c = np.array([2,4], dtype='float32').reshape(-1,1)\n",
    "n = 2 # input of dimension 2\n",
    "m = 2 # 2 constraints\n",
    "A = np.array([[-1, 0], [0, -1]], dtype='float32')\n",
    "b = np.array([-5,-5], dtype='float32').reshape(-1, 1)\n",
    "print(f\"A: {A}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[-1.  0.]\n",
      " [ 0. -1.]]\n",
      "b: [[-5.]\n",
      " [-5.]]\n",
      "c: [[2.]\n",
      " [4.]]\n",
      "x_t: [[-5.]\n",
      " [-5.]]\n",
      "Init lagrange_multiplier [[0.873101 ]\n",
      " [0.8700457]]\n",
      "lagrangian shape: torch.Size([])\n",
      "Shape objective torch.Size([1, 1]), Shape constraint torch.Size([2, 1])\n",
      "objective: tensor([[-30.]], grad_fn=<MmBackward0>)\n",
      "constraint: tensor([[0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n",
      "lagrangian: -30.0\n",
      "Step 0, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 340000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m lagrangian\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# update values\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mopt_lagrange\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c_t = torch.tensor(c).float()\n",
    "A_t = torch.tensor(A).float()\n",
    "b_t = torch.tensor(b).float().reshape(-1,1)\n",
    "# x_t = torch.rand(n, 1, requires_grad=True)\n",
    "x_t = torch.tensor([-5.0,-5.0], requires_grad=True).float().reshape(-1,1)\n",
    "print(f\"A: {A_t.detach().numpy()}\")\n",
    "print(f\"b: {b_t.detach().numpy()}\")\n",
    "print(f\"c: {c_t.detach().numpy()}\")\n",
    "print(f\"x_t: {x_t.detach().numpy()}\")\n",
    "\n",
    "_lagrange_multiplier = torch.rand(m,1, requires_grad=True)\n",
    "lagrange_multiplier = torch.nn.functional.softplus(_lagrange_multiplier)\n",
    "print(f\"Init lagrange_multiplier {lagrange_multiplier.detach().numpy()}\")\n",
    "\n",
    "# opt_weights = torch.optim.Adam([x_t], lr=0.1)\n",
    "opt_lagrange = torch.optim.Adam([_lagrange_multiplier], lr=0.1, maximize=True)\n",
    "\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "    \n",
    "\n",
    "    objective = c_t.T @ x_t\n",
    "    constraint = A_t @ x_t + b_t\n",
    "    \n",
    "    lagrange_multiplier = torch.nn.functional.softplus(_lagrange_multiplier)\n",
    "    lagrangian = objective + lagrange_multiplier.T @ constraint\n",
    "    lagrangian = lagrangian.squeeze()\n",
    "    if step == 0:\n",
    "        print(f\"lagrangian shape: {lagrangian.shape}\")\n",
    "        print(f\"Shape objective {objective.shape}, Shape constraint {constraint.shape}\")\n",
    "        print(f\"objective: {objective}\")\n",
    "        print(f\"constraint: {constraint}\")\n",
    "        print(f\"lagrangian: {lagrangian}\")\n",
    "\n",
    "        # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {lagrangian.item()}, lambda_1: {_lagrange_multiplier[0]}, lambda_2: {_lagrange_multiplier[1]}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(lagrangian.item()) < loss_threshold:\n",
    "        break\n",
    "        \n",
    "    # zero the gradient\n",
    "    opt_lagrange.zero_grad()\n",
    "\n",
    "    # compute the gradient\n",
    "    lagrangian.backward()\n",
    "\n",
    "    # update values\n",
    "    opt_lagrange.step()\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: -5.0 is leaf True, x2: -5.0 is leaf True, x3: tensor([0.0566], requires_grad=True) is leaf True, x4: tensor([0.8357], requires_grad=True) is leaf True\n",
      "Optimized x1: -5.0\n",
      "Optimized x2: -5.0\n",
      "Optimized x3: 0.05660974979400635\n",
      "Optimized x4: 0.835723876953125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9e002a050>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Adam\n",
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.tensor(-5.0, requires_grad=False)\n",
    "x2_opt = torch.tensor(-5.0, requires_grad=False)\n",
    "x3_opt = torch.rand(1, requires_grad=True)\n",
    "x4_opt = torch.rand(1, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 1000\n",
    "lr = 0.1\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "opt = torch.optim.Adam([x3_opt, x4_opt], lr=lr, maximize=True)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, 0.98)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt).sum()\n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    y.backward()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    # clip lagrange values\n",
    "    x3_opt.data = torch.clip(x3_opt.data, 0)\n",
    "    x4_opt.data = torch.clip(x4_opt.data, 0)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([0.6116], dtype=torch.float64, requires_grad=True) is leaf True, x2: tensor([0.0295], dtype=torch.float64, requires_grad=True) is leaf True, x3: tensor([0.1955], dtype=torch.float64, requires_grad=True) is leaf True, x4: tensor([0.9048], dtype=torch.float64, requires_grad=True) is leaf True\n",
      "Optimized x1: -4.55081800782109\n",
      "Optimized x2: -5.225714517952417\n",
      "Optimized x3: 0.0\n",
      "Optimized x4: 0.29730698177256015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fda17360160>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Adam\n",
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x2_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x3_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x4_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 10000 + 20\n",
    "lr = 0.1\n",
    "\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "opt_duals = torch.optim.Adam([x3_opt, x4_opt], lr=lr, maximize=True)\n",
    "opt_x = torch.optim.Adadelta([x1_opt, x2_opt], lr=lr, maximize=False)\n",
    "scheduler_dual = torch.optim.lr_scheduler.ExponentialLR(opt_duals, 0.98)\n",
    "scheduler_x = torch.optim.lr_scheduler.ExponentialLR(opt_x, 0.98)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt).sum()\n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt_x.zero_grad(set_to_none=True)\n",
    "    opt_duals.zero_grad(set_to_none=True)\n",
    "    y.backward()\n",
    "\n",
    "    if flip:\n",
    "        opt_x.step()\n",
    "        # if step % 20 == 0:\n",
    "        #     scheduler_x.step()\n",
    "    else:\n",
    "        opt_duals.step()\n",
    "        # if step % 20 == 0:\n",
    "        #     scheduler_dual.step()\n",
    "    \n",
    "    # clip lagrange values\n",
    "    x3_opt.data = torch.clip(x3_opt.data, 0)\n",
    "    x4_opt.data = torch.clip(x4_opt.data, 0)\n",
    "\n",
    "    if step != 0 and step % 60 == 0:\n",
    "        flip = not flip\n",
    "    \n",
    "    if loss_graph[1, step] < -29.5 and loss_graph[1,step] > -30.5 and flip == False:\n",
    "        loss_graph[1, step:] = loss_graph[1,step]\n",
    "        break\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([0.7194], requires_grad=True) is leaf True, x2: tensor([0.2687], requires_grad=True) is leaf True, x3: tensor([0.7701], requires_grad=True) is leaf True, x4: tensor([0.8334], requires_grad=True) is leaf True\n",
      "Optimized x1: -4.999998092651367\n",
      "Optimized x2: -4.999998092651367\n",
      "Optimized x3: 1.9999995231628418\n",
      "Optimized x4: 3.9999990463256836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdffc1509d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1, requires_grad=True)\n",
    "x2_opt = torch.rand(1, requires_grad=True)\n",
    "x3_opt = torch.rand(1, requires_grad=True)\n",
    "x4_opt = torch.rand(1, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "def zero_grad(parameters):\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 1000\n",
    "lr = 0.1\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt)\n",
    "    y.backward()\n",
    "\n",
    "    grad1 = x1_opt.grad if x1_opt.grad is not None else 0.0\n",
    "    grad2 = x2_opt.grad if x2_opt.grad is not None else 0.0\n",
    "    grad3 = x3_opt.grad if x3_opt.grad is not None else 0.0\n",
    "    grad4 = x4_opt.grad if x4_opt.grad is not None else 0.0\n",
    "\n",
    "    loss_graph[1, step] = y.item()\n",
    "    \n",
    "    if flip:\n",
    "        # when this is true, we are minimizing L(x,lambda) w.r.t. x\n",
    "        x1_opt.data = (x1_opt.data + lr*grad3).requires_grad_(True)\n",
    "        x2_opt.data = (x2_opt.data + lr*grad4).requires_grad_(True)        \n",
    "\n",
    "    else:\n",
    "        # when this is false, we are maximizing L(x,lambda) w.r.t. lambda\n",
    "        x3_opt.data = torch.clamp(x3_opt.data + lr*grad1, min=0.0).requires_grad_(True)\n",
    "        x4_opt.data = torch.clamp(x4_opt.data + lr*grad2, min=0.0).requires_grad_(True)\n",
    "\n",
    "    # if step != 0 and (step % 100) == 0:\n",
    "    #     print(f\"Step {step}, Loss: {y.item():.4f}, x1: {x1_opt.detach().numpy()[0]:.4f} x2: {x2_opt.detach().numpy()[0]:.4f} lambda_1: {x3_opt.detach().numpy()[0]:.4f}, lambda_2: {x4_opt.detach().numpy()[0]:.4f}, grads: [{x1_opt.grad.detach().numpy()[0]:.4f}, {x2_opt.grad.detach().numpy()[0]:.4f}, {x3_opt.grad.detach().numpy()[0]:.4f}, {x4_opt.grad.detach().numpy()[0]:.4f}]\")\n",
    "\n",
    "    if step != 0 and (step % 100) == 0:\n",
    "        flip = not flip\n",
    "        \n",
    "    # zero out the gradients\n",
    "    zero_grad([x1_opt, x2_opt, x3_opt, x4_opt])\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x1: -0.8333324193954468\n",
      "Optimized x2: -0.8333352208137512\n",
      "Optimized l1: 1.045896053314209\n",
      "Optimized l2: 0.2871112823486328\n",
      "Optimized l3: 0.0\n",
      "Optimized l4: 1.4137334823608398\n",
      "Optimized l5: 0.5862621068954468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdce8c83550>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1, requires_grad=True)\n",
    "x2_opt = torch.rand(1, requires_grad=True)\n",
    "l1_opt = torch.rand(1, requires_grad=True)\n",
    "l2_opt = torch.rand(1, requires_grad=True)\n",
    "l3_opt = torch.rand(1, requires_grad=True)\n",
    "l4_opt = torch.rand(1, requires_grad=True)\n",
    "l5_opt = torch.rand(1, requires_grad=True)\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, l1, l2, l3, l4, l5):\n",
    "    return x1 + 2*x2 + l1*(x1-2) + l2*(-x1-2) + l3*(x2-2) + l4*(-x2-2) + l5*(-3*x1 - x2 - 5)\n",
    "\n",
    "def zero_grad(parameters):\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 10000\n",
    "lr = 0.01\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, l1_opt, l2_opt, l3_opt, l4_opt, l5_opt)\n",
    "    y.backward()\n",
    "\n",
    "    x_grad1 = x1_opt.grad if x1_opt.grad is not None else 0.0\n",
    "    x_grad2 = x2_opt.grad if x2_opt.grad is not None else 0.0\n",
    "    l_grad1 = l1_opt.grad if l1_opt.grad is not None else 0.0\n",
    "    l_grad2 = l2_opt.grad if l2_opt.grad is not None else 0.0\n",
    "    l_grad3 = l3_opt.grad if l3_opt.grad is not None else 0.0\n",
    "    l_grad4 = l4_opt.grad if l4_opt.grad is not None else 0.0\n",
    "    l_grad5 = l5_opt.grad if l5_opt.grad is not None else 0.0\n",
    "\n",
    "    loss_graph[1, step] = y.item()\n",
    "    \n",
    "    if flip:\n",
    "        # when this is true, we are minimizing L(x,lambda) w.r.t. x\n",
    "        x1_opt.data = (x1_opt.data + lr*(-l_grad1 + l_grad2 + l_grad5)).requires_grad_(True)\n",
    "        x2_opt.data = (x2_opt.data + lr*(-l_grad3 + l_grad4 + l_grad5)).requires_grad_(True)        \n",
    "\n",
    "    else:\n",
    "        # when this is false, we are maximizing L(x,lambda) w.r.t. lambda\n",
    "        l1_opt.data = torch.clamp(l1_opt.data + lr*(-x_grad1), min=0.0).requires_grad_(True)\n",
    "        l2_opt.data = torch.clamp(l2_opt.data + lr*(x_grad1), min=0.0).requires_grad_(True)\n",
    "        l3_opt.data = torch.clamp(l3_opt.data + lr*(-x_grad2), min=0.0).requires_grad_(True)\n",
    "        l4_opt.data = torch.clamp(l4_opt.data + lr*(x_grad2), min=0.0).requires_grad_(True)\n",
    "        l5_opt.data = torch.clamp(l5_opt.data + lr*(x_grad1 + x_grad2), min=0.0).requires_grad_(True)\n",
    "\n",
    "    if step != 0 and (step % 100) == 0:\n",
    "        flip = not flip\n",
    "        \n",
    "    # zero out the gradients\n",
    "    zero_grad([x1_opt, x2_opt, l1_opt, l2_opt, l3_opt, l4_opt, l5_opt])\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "l1_optimized = l1_opt.item()\n",
    "l2_optimized = l2_opt.item()\n",
    "l3_optimized = l3_opt.item()\n",
    "l4_optimized = l4_opt.item()\n",
    "l5_optimized = l5_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized l1:\", l1_optimized)\n",
    "print(\"Optimized l2:\", l2_optimized)\n",
    "print(\"Optimized l3:\", l3_optimized)\n",
    "print(\"Optimized l4:\", l4_optimized)\n",
    "print(\"Optimized l5:\", l5_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am checking the results of the Lagrange problem above by computing the upper and lower bounds on x as well as the minimal and maximal pertubation on each logit output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Model fingerprint: 0xf5f961a3\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [3e+00, 4e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Presolve removed 5 rows and 2 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    1.6000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.600000000e+01\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 2e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -3.0000000e+30   3.000000e+30   3.000000e+00      0s\n",
      "       3   -5.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 3 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -5.000000000e+00\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [3e+00, 4e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    7.0000000e+30   2.000000e+30   7.000000e+00      0s\n",
      "       2    1.6000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.600000000e+01\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 3e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -4.0000000e+30   3.000000e+30   4.000000e+00      0s\n",
      "       3   -5.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 3 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -5.000000000e+00\n",
      "logit 1 is bounded s.t. -5.0 <= z(x) <= 16.0 with lb pertubation (-1.0, -2.0) and ub pertubation (2.0, 2.0)\n",
      "logit 2 is bounded s.t. -5.0 <= z(x) <= 16.0 with lb pertubation (-1.0, -2.0) and ub pertubation (2.0, 2.0)\n"
     ]
    }
   ],
   "source": [
    "W_ub = np.array([[4,3],[4,3]])\n",
    "b_ub = np.array([2,2])\n",
    "W_lb = np.array([[1,2],[1,1]])\n",
    "# b_lb = np.array([1,1])\n",
    "b_lb = np.zeros(2)\n",
    "# using Gurobi to solve the same problem as above\n",
    "opt_mod = Model(name = \"simple_linear_program_2\")\n",
    "\n",
    "# add variables\n",
    "inputs = np.array(list(opt_mod.addVars(W_ub.shape[1], name=\"x\", lb=float(\"-inf\"), ub=float(\"inf\")).values()))\n",
    "\n",
    "# adding the constraints\n",
    "c1 = opt_mod.addConstr(inputs[0] - 2 <= 0, name='c1') # these four constraints are the l_inf norm box constraints\n",
    "c2 = opt_mod.addConstr(-inputs[0] - 2 <= 0, name='c2')\n",
    "c3 = opt_mod.addConstr(inputs[1] - 2 <= 0, name='c3')\n",
    "c4 = opt_mod.addConstr(-inputs[1] - 2 <= 0, name='c4')\n",
    "c5 = opt_mod.addConstr(-3*inputs[0] - inputs[1] - 5 <= 0, name='c5') # this constraint is a line constraint cutting through the box\n",
    "\n",
    "worst_case_inputs_ub = []\n",
    "worst_case_inputs_lb = []\n",
    "upper_bounds = []\n",
    "lower_bounds = []\n",
    "\n",
    "# set the objective function for each logit\n",
    "for idx in range(2*W_ub.shape[0]):\n",
    "    i = idx // 2\n",
    "    if idx % 2 == 0:\n",
    "        obj_fn = quicksum([W_ub[i,j]*inputs[j] for j in range(W_ub.shape[1])]) + b_ub[i]\n",
    "        opt_mod.setObjective(obj_fn, GRB.MAXIMIZE)\n",
    "    else:\n",
    "        obj_fn = quicksum([W_lb[i,j]*inputs[j] for j in range(W_lb.shape[1])]) + b_lb[i]\n",
    "        opt_mod.setObjective(obj_fn, GRB.MINIMIZE)\n",
    "\n",
    "    # now optimize the problem and save it to a file\n",
    "    opt_mod.optimize()\n",
    "    # opt_mod.write(\"scenario_one_upperbound_logit_one.lp\")\n",
    "\n",
    "    # output the result\n",
    "    # print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "    if idx % 2 == 0:\n",
    "        upper_bounds.append(opt_mod.ObjVal)\n",
    "    else:\n",
    "        lower_bounds.append(opt_mod.ObjVal)\n",
    "    # Get values of the decision variables\n",
    "    temp_inputs = []\n",
    "    for v in opt_mod.getVars():\n",
    "        # print('%s: %g' % (v.VarName, v.x))\n",
    "        temp_inputs.append(v.x)\n",
    "\n",
    "    if idx % 2 == 0:\n",
    "        worst_case_inputs_ub.append(tuple(temp_inputs)) # append the worst case input that caused this maximal pertubation\n",
    "    else:\n",
    "        worst_case_inputs_lb.append(tuple(temp_inputs)) # append the worst case input that caused this maximal pertubation\n",
    "    \n",
    "for i in range(W_ub.shape[0]):\n",
    "    print(f\"logit {i + 1} is bounded s.t. {lower_bounds[i]} <= z(x) <= {upper_bounds[i]} with lb pertubation {worst_case_inputs_lb[i]} and ub pertubation {worst_case_inputs_ub[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal Result:\n",
      "       message: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n",
      "       success: True\n",
      "        status: 0\n",
      "           fun: 0.0\n",
      "             x: [ 0.000e+00  0.000e+00]\n",
      "           nit: 0\n",
      "         lower:  residual: [ 0.000e+00  0.000e+00]\n",
      "                marginals: [ 2.000e+00  3.000e+00]\n",
      "         upper:  residual: [       inf        inf]\n",
      "                marginals: [ 0.000e+00  0.000e+00]\n",
      "         eqlin:  residual: []\n",
      "                marginals: []\n",
      "       ineqlin:  residual: [ 1.000e+00  2.000e+00]\n",
      "                marginals: [-0.000e+00 -0.000e+00]\n",
      "\n",
      "Dual Result:\n",
      "       message: The problem is unbounded. (HiGHS Status 10: model_status is Unbounded; primal_status is At upper bound)\n",
      "       success: False\n",
      "        status: 3\n",
      "           fun: None\n",
      "             x: None\n",
      "           nit: 0\n",
      "         lower:  residual: None\n",
      "                marginals: None\n",
      "         upper:  residual: None\n",
      "                marginals: None\n",
      "         eqlin:  residual: None\n",
      "                marginals: None\n",
      "       ineqlin:  residual: None\n",
      "                marginals: None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# Primal linear program coefficients\n",
    "c = np.array([2, 3])\n",
    "A = np.array([[1, -1], [3, 1]])\n",
    "b = np.array([1, 2])\n",
    "\n",
    "# Solve the primal linear program\n",
    "result_primal = linprog(c, A_ub=A, b_ub=b, method='highs')\n",
    "\n",
    "# Display the primal result\n",
    "print(\"Primal Result:\")\n",
    "print(result_primal)\n",
    "\n",
    "# Dual linear program coefficients\n",
    "c_dual = -b  # Coefficients are negated for maximization\n",
    "A_dual = -A.T  # Transpose of A with negation\n",
    "b_dual = c  # Dual variables corresponding to the inequality constraints\n",
    "\n",
    "# Solve the dual linear program\n",
    "result_dual = linprog(c_dual, A_ub=A_dual, b_ub=b_dual, method='highs')\n",
    "\n",
    "# Display the dual result\n",
    "print(\"\\nDual Result:\")\n",
    "print(result_dual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [1.25 0.  ]\n",
      " [2.5  0.  ]\n",
      " [3.75 0.  ]\n",
      " [5.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# potentially look into this for solving the lagrange dual, but it requires the gradients to be determined beforehand\n",
    "from scipy.optimize import fsolve, fmin_l_bfgs_b\n",
    "\n",
    "a = 1\n",
    "nbtests = 5\n",
    "minmu = 0\n",
    "maxmu = 5\n",
    "\n",
    "def lagrange(x, mu):\n",
    "    return x**2 + mu * (np.exp(x) + x - a)\n",
    "\n",
    "def lagrange_grad(x, mu):\n",
    "    grad_x = 2*x + mu * (np.exp(x) + 1)\n",
    "    grad_mu = np.exp(x) + x - a\n",
    "    return grad_x, grad_mu\n",
    "\n",
    "def dual(mu):\n",
    "    x = fsolve(lambda x: lagrange_grad(x, mu)[0], x0=1)\n",
    "    obj_val = lagrange(x, mu)\n",
    "    grad = lagrange_grad(x, mu)[1]\n",
    "    return -1.0*obj_val, -1.0*grad\n",
    "\n",
    "pl = np.empty((nbtests, 2))\n",
    "for i, nu in enumerate(np.linspace(minmu,maxmu,nbtests)):\n",
    "    res = fmin_l_bfgs_b(dual, x0=nu, bounds=[(0,None)], factr=1e6)\n",
    "    mu_opt = res[0]\n",
    "    x_opt = fsolve(lambda x: lagrange_grad(x, mu_opt)[0], x0=1)\n",
    "    pl[i] = [nu, *x_opt]\n",
    "print(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last example, we will deal with a single dimension input and output. The input x will have a pertubation of $\\epsilon=2$ and be fed to a ReLU unit, then be added by 5. This will then be the final output. This first example will have no constraints, and if alpha is tuned correctly, it should be zero s.t. alpha-CROWN produces a lower bound of 0. The actualy lower and upper bound is 5 and 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same example as before but with the restriction that $x\\geq1$. We will add a Lagrange Multiplier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximizing $\\lambda$ after minimizing x. \n",
    "For a ReLU activation function, we may have an example where we wish to minimize $\\sigma(x)$ where $\\sigma$ is the ReLU function. If x is convex s.t. $-2\\leq x \\leq 2$, we may choose the lower bound on the input to be x and we will naturally find that the objection function is 0. The main issue is that ReLU is non-convex in nature, therefore we take inspiration from $\\alpha$-CROWN and use the linear lower bound $\\alpha$x to lower bound the ReLU function. This is the linear constraint that is used in the triangular relaxation of ReLU. Letting $alpha=1$ (note that we must have 0\\leq \\alpha \\leq 1) and adding constraints, $x \\geq -1$ and $x \\leq 2$, we now solve the dual program, $L(x, \\lambda)$. First minimize x and then maximize $\\lambda$. x exists in the space $-2 \\leq x \\leq 2$, therefore we know that the smallest value of x is x=-2. Though this x does not fit our constraints, if $\\lambda$ is optimal, we get the optimal objective value. Note that this minimum value will be -1 and not 0 because of the ReLU relaxation. The true minimum value is 0. \n",
    "\n",
    "## Question\n",
    "Ask Zhang if it is reasonable to choose route 1 where clipping and projection is included in the projection as this leads to tigher bounds yet still valid bounds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Lambda: tensor([[0.8509],\n",
      "        [0.0250]])\n",
      "Initial \u0007lpha: tensor([[1.]])\n",
      "Optimized lambda: tensor([[0.],\n",
      "        [1.]])\n",
      "Optimized alpha: tensor([[1.]])\n",
      "CROWN lower bound: -2.0, Lagrange lower bound: -1.0000001192092896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb1bf784c10>]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.tensor([[-2]]).float()\n",
    "lambda_ = torch.rand(2,1, requires_grad=True)\n",
    "c = torch.tensor([[1]]).float()\n",
    "# c = torch.rand(1,1, requires_grad=True)\n",
    "# with torch.no_grad():\n",
    "#     c.data = torch.clamp(c.data, min=0, max=1)\n",
    "G = torch.tensor([[1],[-1]]).float()\n",
    "h = torch.tensor([[2],[1]]).float()\n",
    "\n",
    "print(f\"Initial Lambda: {lambda_.data}\\nInitial \\alpha: {c}\")\n",
    "\n",
    "def constraint(v, G, c):\n",
    "    return -G.T @ v - c\n",
    "\n",
    "def project_onto_lambda(v, G, c):\n",
    "    # using the formula x = z - A.T(AA.T)^-1(Az-b) to project v onto the set Ax=b\n",
    "    # in this example, A = -G.T, b = c\n",
    "    projection = v + G @ torch.linalg.pinv(G.T@G) @ (-G.T @ v - c)\n",
    "    min_val = torch.min(projection)\n",
    "    if min_val < 0.0:\n",
    "        projection -= min_val\n",
    "    return projection\n",
    "    \n",
    "def obj_fn(x, lambda_, A, b, c):\n",
    "    return c.T@x + lambda_.T@(A@x - b)\n",
    "\n",
    "# Number of optimization steps\n",
    "lr = 0.1\n",
    "num_steps = 20\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "lambda_vals = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "lambda_vals = np.vstack((lambda_vals, np.zeros((3, num_steps))))\n",
    "\n",
    "# opt = torch.optim.Adam([lambda_, c], lr=lr, maximize=True)\n",
    "opt = torch.optim.Adam([lambda_], lr=lr, maximize=True)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, 0.98)\n",
    "\n",
    "## Though this converges much more tightly, it is frowned upon\n",
    "# Optimization loop\n",
    "# for step in range(num_steps):\n",
    "\n",
    "#     # y = obj_fn(x, lambda_, G, h, c)\n",
    "#     c.data = torch.clamp(c.data, min=0, max=1)\n",
    "#     projected_lambda = project_onto_lambda(lambda_, G, c)\n",
    "#     lambda_vals[1:3,step] = projected_lambda.detach().clone().numpy().flatten()\n",
    "#     y = obj_fn(x, projected_lambda, G, h, c)\n",
    "\n",
    "#     loss_graph[1, step] = y.item()\n",
    "\n",
    "#     opt.zero_grad(set_to_none=True)\n",
    "\n",
    "#     y.backward()\n",
    "\n",
    "#     opt.step()\n",
    "#     scheduler.step()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # c.data = torch.clamp(c.data, min=0, max=1)\n",
    "#         lambda_vals[3, step] = c.detach().clone().numpy().flatten()\n",
    "\n",
    "## Projections and constraints should not be included in the Adam optimizer\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    y = obj_fn(x, lambda_, G, h, c)\n",
    "    \n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    y.backward()\n",
    "\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lambda_.data = project_onto_lambda(lambda_, G, c)\n",
    "        lambda_vals[1:3,step] = lambda_.detach().clone().numpy().flatten()\n",
    "        \n",
    "    # with torch.no_grad():\n",
    "    #     c.data = torch.clamp(c.data, min=0, max=1)\n",
    "    #     lambda_vals[3, step] = c.detach().clone().numpy().flatten()\n",
    "\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "# lambda_optimized = lambda_.data\n",
    "lambda_optimized = project_onto_lambda(lambda_, G, c).data\n",
    "alpha_optimize = c.data\n",
    "\n",
    "print(\"Optimized lambda:\", lambda_optimized)\n",
    "print(\"Optimized alpha:\", alpha_optimize)\n",
    "\n",
    "print(f\"CROWN lower bound: {obj_fn(x, torch.zeros(2,1).float(), G, h, c).squeeze()}, Lagrange lower bound: {loss_graph[1,-1]}\")\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(f\"Optimization Lambda (converged to {loss_graph[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(f\"\\Lambda_1 (converged to {lambda_vals[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[1,:])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(f\"\\Lambda_2 (converged to {lambda_vals[2,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[2,:])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(f\"\\alpha (converged to {lambda_vals[3,-1]:1.1f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2024-10-28\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 10 nonzeros\n",
      "Model fingerprint: 0xd3257bed\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-01, 5e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [5e+00, 3e+01]\n",
      "Presolve removed 2 rows and 0 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 3 rows, 2 columns, 6 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -1.5200000e+01   1.216983e+01   0.000000e+00      0s\n",
      "       2   -7.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.01 seconds (0.00 work units)\n",
      "Optimal objective -7.000000000e+00\n",
      "Objective Function Value: -7.000000\n",
      "x[0,0]: -2\n",
      "x[1,0]: -5\n",
      "Printing inequality constraint dual variables:\n",
      " [[ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.62790698]\n",
      " [-0.37209302]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# tightest lower bound with  constraints\n",
    "opt_mod = Model(name=\"lower_bound_model\")\n",
    "\n",
    "# constraints\n",
    "G = np.array([[-2/9, 1],[-6/5, 1], [-5/3, -1], [1/8, -1], [5, 1]])\n",
    "h = np.array([[46/9], [6], [25/3], [19/4], [26]])\n",
    "\n",
    "x = opt_mod.addMVar(shape=(2,1), name='x', lb=float('-inf'), ub=float('inf'))\n",
    "\n",
    "# adding inequality constraints\n",
    "ineq_c = opt_mod.addConstr(G @ x <= h, name='c0')\n",
    "\n",
    "opt_mod.setObjective(x[0,0] + x[1,0], GRB.MINIMIZE)\n",
    "\n",
    "opt_mod.optimize()\n",
    "\n",
    "# output the result\n",
    "print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "# Get values of the decision variables\n",
    "for v in opt_mod.getVars():\n",
    "    print('%s: %g' % (v.VarName, v.x))\n",
    "\n",
    "ineq_pi = ineq_c.Pi\n",
    "print(f\"Printing inequality constraint dual variables:\\n {ineq_pi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 10 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-01, 5e+00]\n",
      "  Objective range  [5e-01, 5e-01]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [5e+00, 3e+01]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    5.4545455e+29   2.125000e+30   5.454545e-01      0s\n",
      "       2    1.0909091e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.090909091e+01\n",
      "Objective Function Value: 10.909091\n",
      "x[0,0]: 4\n",
      "x[1,0]: 6\n",
      "Printing inequality constraint dual variables:\n",
      " [[0.41779497]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.12765957]]\n"
     ]
    }
   ],
   "source": [
    "opt_mod.setObjective((12/22)*(x[0,0] + x[1,0] + 10), GRB.MAXIMIZE)\n",
    "\n",
    "opt_mod.optimize()\n",
    "\n",
    "# output the result\n",
    "print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "# Get values of the decision variables\n",
    "for v in opt_mod.getVars():\n",
    "    print('%s: %g' % (v.VarName, v.x))\n",
    "\n",
    "ineq_pi = ineq_c.Pi\n",
    "print(f\"Printing inequality constraint dual variables:\\n {ineq_pi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(constraint(torch.tensor([[0.],[0.], [0.62790698], [0.37209302], [0.]]), G, lower_c).detach().clone().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: tensor([[0.0010],\n",
      "        [0.0000],\n",
      "        [0.4016],\n",
      "        [0.4034],\n",
      "        [0.0062]])\n",
      "Optimized lambda: tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.6307],\n",
      "        [0.3701],\n",
      "        [0.0000]]) tensor([[-0.4222],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-0.1275]])\n",
      "CROWN lower bound: -10.0, Lagrange lower bound: -6.985259532928467\n",
      "CROWN upper bound: 12.0, Lagrange upper bound: 10.913131713867188\n",
      "last_lower_loss -7.027132034301758\n",
      "last_upper_loss-10.989557266235352\n",
      "last_lower_lambda[[0.       ]\n",
      " [0.       ]\n",
      " [0.629528 ]\n",
      " [0.3664104]\n",
      " [0.       ]]\n",
      "last_upper_lambda[[-0.4187259 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.12556776]]\n",
      "lower bound penalth-0.005712032318115234\n",
      "upper bound penalty-0.006209135055541992\n",
      "w: tensor([[0.0010],\n",
      "        [0.0000],\n",
      "        [0.4016],\n",
      "        [0.4034],\n",
      "        [0.0062]])\n",
      "w: tensor([[0.0005],\n",
      "        [0.0000],\n",
      "        [0.2190],\n",
      "        [0.2200],\n",
      "        [0.0034]])\n",
      "Last projection of lower bound: tensor([[0.0012],\n",
      "        [0.0000],\n",
      "        [0.4989],\n",
      "        [0.5012],\n",
      "        [0.0077]], grad_fn=<ClampBackward1>) with constraint: tensor([[-0.2691],\n",
      "        [-0.0088]], grad_fn=<SubBackward0>)\n",
      "Last projection of upper bound: tensor([[-3.5848e-06],\n",
      "        [-0.0000e+00],\n",
      "        [-1.4816e-03],\n",
      "        [-1.4882e-03],\n",
      "        [-2.2731e-05]], grad_fn=<ClampBackward1>) with constraint: tensor([[-0.5476],\n",
      "        [-0.5484]], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8937/2445090498.py:5: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w = torch.from_numpy(np.linalg.lstsq(-G.detach().clone().numpy().T, c.detach().clone().numpy())[0].reshape(-1,1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f17093d9f60>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def constraint(v, G, c):\n",
    "    return -G.T @ v - c\n",
    "\n",
    "def project_onto_lambda(v, G, c, d):\n",
    "    w = torch.from_numpy(np.linalg.lstsq(-G.detach().clone().numpy().T, c.detach().clone().numpy())[0].reshape(-1,1))\n",
    "    min_w_entry = torch.min(w)\n",
    "    if min_w_entry < 0:\n",
    "        w = w - min_w_entry\n",
    "    print(f\"w: {w}\")\n",
    "    # print(constraint(w, G, c))\n",
    "    projection = ((w.T@v)/(w.T@w))*w\n",
    "\n",
    "    # using the formula x = z - A.T(AA.T)^-1(Az-b) to project v onto the set Ax=b\n",
    "    # in this example, A = -G.T, b = c\n",
    "    # if np.linalg.det(G.T@G) != 0:\n",
    "    #     projection = v + G @ torch.linalg.inv(G.T@G) @ (-G.T @ v - c)\n",
    "    # else:\n",
    "    #     projection = v + G @ torch.linalg.pinv(G.T@G) @ (-G.T @ v - c)\n",
    "\n",
    "    return projection\n",
    "\n",
    "def obj_fn(x, d, lambda_, A, b, c, ub=False):\n",
    "    if ub:\n",
    "        return -1*(c.T@x + d) + lambda_.T@(A@x - b)\n",
    "    else:\n",
    "        return c.T@x + d + lambda_.T@(A@x - b)\n",
    "    \n",
    "def get_penalty(lambda_, G, c, scale=5, lb=True):\n",
    "    if lb:\n",
    "        return -scale*(torch.sum(torch.abs(constraint(lambda_, G, c))) + torch.sum(torch.abs(torch.clamp(lambda_, max=0.0))))\n",
    "    else:\n",
    "        return -scale*(torch.sum(torch.abs(constraint(lambda_, G, c))) + torch.sum(torch.abs(torch.clamp(lambda_, min=0.0))))\n",
    "\n",
    "lower_x = torch.tensor([[-5], [-5]]).float()\n",
    "upper_x = torch.tensor([[6], [6]]).float()\n",
    "lambda_lower = torch.rand(5,1, requires_grad=True)\n",
    "lambda_upper = torch.rand(5,1, requires_grad=True)\n",
    "lower_c = torch.tensor([[1], [1]]).float()\n",
    "lower_d = torch.tensor([[0]]).float()\n",
    "upper_c = torch.tensor([[12/22],[12/22]]).float()\n",
    "upper_d = torch.tensor([[120/22]]).float()\n",
    "G = torch.tensor([[-2/9, 1],[-6/5, 1], [-5/3, -1], [1/8, -1], [5, 1]]).float()\n",
    "h = torch.tensor([[46/9], [6], [25/3], [19/4], [26]]).float()\n",
    "\n",
    "# print(f\"Initial Lower Lambda: {lambda_lower.data}\\nInitial Upper Lambda: {lambda_upper.data}\\nInitial lower \\alpha: {lower_c}\\n Initial upper \\alpha: {upper_c}\")\n",
    "\n",
    "# Number of optimization steps\n",
    "lr = 0.1\n",
    "num_steps = 100\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "lambda_vals = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "lambda_vals = np.vstack((lambda_vals, np.zeros((3, num_steps))))\n",
    "\n",
    "# opt = torch.optim.Adam([lambda_, c], lr=lr, maximize=True)\n",
    "opt_lower = torch.optim.Adam([lambda_lower], lr=lr, maximize=True)\n",
    "opt_upper = torch.optim.Adam([lambda_upper], lr=lr, maximize=True)\n",
    "scheduler_lower = torch.optim.lr_scheduler.ExponentialLR(opt_lower, 0.98)\n",
    "scheduler_upper = torch.optim.lr_scheduler.ExponentialLR(opt_upper, 0.98)\n",
    "\n",
    "## Projections and constraints should not be included in the Adam optimizer\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    lower_y = obj_fn(lower_x, lower_d, lambda_lower, G, h, lower_c) + get_penalty(lambda_lower, G, lower_c, lb=True)\n",
    "    upper_y = -1*(obj_fn(upper_x, upper_d, lambda_upper, G, h, upper_c) - get_penalty(lambda_upper, G, upper_c, lb=False))\n",
    "\n",
    "    \n",
    "    loss_graph[1, step] = lower_y.item()\n",
    "\n",
    "    if step == num_steps - 1:\n",
    "        last_lower_loss = lower_y.item()\n",
    "        last_upper_loss = upper_y.item()\n",
    "        last_lower_lambda = lambda_lower.detach().clone().numpy()\n",
    "        last_upper_lambda = lambda_upper.detach().clone().numpy()\n",
    "\n",
    "    opt_lower.zero_grad(set_to_none=True)\n",
    "    opt_upper.zero_grad(set_to_none=True)\n",
    "\n",
    "    lower_y.backward()\n",
    "    upper_y.backward()\n",
    "\n",
    "    opt_lower.step()\n",
    "    opt_upper.step()\n",
    "    scheduler_lower.step()\n",
    "    scheduler_upper.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        lambda_upper.data = torch.clamp(lambda_upper.data, max=0.0)\n",
    "        lambda_lower.data = torch.clamp(lambda_lower.data, min=0.0)\n",
    "        lambda_vals[1:3,step] = lambda_lower.detach().clone().numpy().flatten()[:2]\n",
    "        \n",
    "# lambda_lower.data = project_onto_lambda(lambda_lower.data, G, lower_c,0)\n",
    "lambda_lower_optimized = lambda_lower.data\n",
    "lambda_upper_optimized = lambda_upper.data\n",
    "# alpha_optimize = c.data\n",
    "\n",
    "print(\"Optimized lambda:\", lambda_lower_optimized, lambda_upper_optimized)\n",
    "# print(\"Optimized alpha:\", alpha_optimize)\n",
    "\n",
    "print(f\"CROWN lower bound: {obj_fn(lower_x, lower_d, torch.zeros(5,1).float(), G, h, lower_c).squeeze()}, Lagrange lower bound: {obj_fn(lower_x, lower_d, lambda_lower, G, h, lower_c).squeeze()}\")\n",
    "print(f\"CROWN upper bound: {obj_fn(upper_x, upper_d, torch.zeros(5,1).float(), G, h, upper_c).squeeze()}, Lagrange upper bound: {obj_fn(upper_x, upper_d, lambda_upper, G, h, upper_c).squeeze()}\")\n",
    "\n",
    "print(f\"last_lower_loss {last_lower_loss}\\nlast_upper_loss{last_upper_loss}\\nlast_lower_lambda{last_lower_lambda}\\nlast_upper_lambda{last_upper_lambda}\\nlower bound penalth{get_penalty(lambda_lower, G, lower_c, lb=True, scale=1)}\\nupper bound penalty{get_penalty(lambda_upper, G, upper_c, lb=False, scale=1)}\")\n",
    "assert last_lower_loss <= -7, f\"Last lower loss was {last_lower_loss}\"\n",
    "assert -1*last_upper_loss >= 1.090909091e+01, f\"Last upper loss was {last_upper_loss}\"\n",
    "\n",
    "p_lower_lambda = torch.clamp(project_onto_lambda(lambda_lower, G, lower_c, 0), min=0.0)\n",
    "p_upper_lambda = torch.clamp(project_onto_lambda(lambda_upper, G, upper_c, 0), max=0.0)\n",
    "print(f\"Last projection of lower bound: {p_lower_lambda} with constraint: {constraint(p_lower_lambda, G, lower_c)}\")\n",
    "print(f\"Last projection of upper bound: {p_upper_lambda} with constraint: {constraint(p_upper_lambda, G, upper_c)}\")\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(f\"Optimization Lambda (converged to {loss_graph[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(f\"\\Lambda_1 (converged to {lambda_vals[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[1,:])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(f\"\\Lambda_2 (converged to {lambda_vals[2,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[2,:])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(f\"\\alpha (converged to {lambda_vals[3,-1]:1.1f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add alpha as an optimizable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized lambda: tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]]) tensor([[-0.4202],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [ 0.0000],\n",
      "        [-0.1314]])\n",
      "Optimized lower alpha: tensor([[0.],\n",
      "        [0.]])\n",
      "CROWN lower bound: 0.0, Lagrange lower bound: 0.0\n",
      "CROWN upper bound: 12.0, Lagrange upper bound: 10.87269401550293\n",
      "last_lower_loss 0.0\n",
      "last_upper_loss-10.921361923217773\n",
      "last_lower_lambda[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "last_upper_lambda[[-0.4172243 ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [-0.12740195]]\n",
      "lower bound penalth-0.0\n",
      "upper bound penalty-0.02439779043197632\n",
      "w: tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "w: tensor([[0.0005],\n",
      "        [0.0000],\n",
      "        [0.2190],\n",
      "        [0.2200],\n",
      "        [0.0034]])\n",
      "Last projection of lower bound: tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], grad_fn=<ClampBackward1>) with constraint: tensor([[nan],\n",
      "        [nan]], grad_fn=<SubBackward0>)\n",
      "Last projection of upper bound: tensor([[-3.6522e-06],\n",
      "        [-0.0000e+00],\n",
      "        [-1.5095e-03],\n",
      "        [-1.5162e-03],\n",
      "        [-2.3158e-05]], grad_fn=<ClampBackward1>) with constraint: tensor([[-0.5477],\n",
      "        [-0.5485]], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8937/3868372827.py:5: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  w = torch.from_numpy(np.linalg.lstsq(-G.detach().clone().numpy().T, c.detach().clone().numpy())[0].reshape(-1,1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1708dda7d0>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:240: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/jorgejc2/.local/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:203: RuntimeWarning: Glyph 7 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def constraint(v, G, c):\n",
    "    return -G.T @ v - c\n",
    "\n",
    "def project_onto_lambda(v, G, c, d):\n",
    "    w = torch.from_numpy(np.linalg.lstsq(-G.detach().clone().numpy().T, c.detach().clone().numpy())[0].reshape(-1,1))\n",
    "    min_w_entry = torch.min(w)\n",
    "    if min_w_entry < 0:\n",
    "        w = w - min_w_entry\n",
    "    print(f\"w: {w}\")\n",
    "    # print(constraint(w, G, c))\n",
    "    projection = ((w.T@v)/(w.T@w))*w\n",
    "\n",
    "    # using the formula x = z - A.T(AA.T)^-1(Az-b) to project v onto the set Ax=b\n",
    "    # in this example, A = -G.T, b = c\n",
    "    # if np.linalg.det(G.T@G) != 0:\n",
    "    #     projection = v + G @ torch.linalg.inv(G.T@G) @ (-G.T @ v - c)\n",
    "    # else:\n",
    "    #     projection = v + G @ torch.linalg.pinv(G.T@G) @ (-G.T @ v - c)\n",
    "\n",
    "    return projection\n",
    "\n",
    "def obj_fn(x, d, lambda_, A, b, c, ub=False):\n",
    "    if ub:\n",
    "        return -1*(c.T@x + d) + lambda_.T@(A@x - b)\n",
    "    else:\n",
    "        return c.T@x + d + lambda_.T@(A@x - b)\n",
    "    \n",
    "def get_penalty(lambda_, G, c, scale=5, lb=True):\n",
    "    if lb:\n",
    "        return -scale*(torch.sum(torch.abs(constraint(lambda_, G, c))) + torch.sum(torch.abs(torch.clamp(lambda_, max=0.0))))\n",
    "    else:\n",
    "        return -scale*(torch.sum(torch.abs(constraint(lambda_, G, c))) + torch.sum(torch.abs(torch.clamp(lambda_, min=0.0))))\n",
    "\n",
    "lower_x = torch.tensor([[-5], [-5]]).float()\n",
    "upper_x = torch.tensor([[6], [6]]).float()\n",
    "lambda_lower = torch.rand(5,1, requires_grad=True)\n",
    "lambda_upper = torch.rand(5,1, requires_grad=True)\n",
    "# lower_c = torch.tensor([[1], [1]]).float()\n",
    "lower_c = torch.rand(2,1,requires_grad=True)\n",
    "lower_d = torch.tensor([[0]]).float()\n",
    "upper_c = torch.tensor([[12/22],[12/22]]).float()\n",
    "upper_d = torch.tensor([[120/22]]).float()\n",
    "G = torch.tensor([[-2/9, 1],[-6/5, 1], [-5/3, -1], [1/8, -1], [5, 1]]).float()\n",
    "h = torch.tensor([[46/9], [6], [25/3], [19/4], [26]]).float()\n",
    "\n",
    "# print(f\"Initial Lower Lambda: {lambda_lower.data}\\nInitial Upper Lambda: {lambda_upper.data}\\nInitial lower \\alpha: {lower_c}\\n Initial upper \\alpha: {upper_c}\")\n",
    "\n",
    "# Number of optimization steps\n",
    "lr = 0.1\n",
    "num_steps = 100\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "lambda_vals = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "lambda_vals = np.vstack((lambda_vals, np.zeros((3, num_steps))))\n",
    "\n",
    "# opt = torch.optim.Adam([lambda_, c], lr=lr, maximize=True)\n",
    "opt_lower = torch.optim.Adam([lambda_lower, lower_c], lr=lr, maximize=True)\n",
    "opt_upper = torch.optim.Adam([lambda_upper], lr=lr, maximize=True)\n",
    "scheduler_lower = torch.optim.lr_scheduler.ExponentialLR(opt_lower, 0.98)\n",
    "scheduler_upper = torch.optim.lr_scheduler.ExponentialLR(opt_upper, 0.98)\n",
    "\n",
    "## Projections and constraints should not be included in the Adam optimizer\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "    \n",
    "    lower_y = obj_fn(lower_x, lower_d, lambda_lower, G, h, lower_c) + get_penalty(lambda_lower, G, lower_c, lb=True)\n",
    "    upper_y = -1*(obj_fn(upper_x, upper_d, lambda_upper, G, h, upper_c) - get_penalty(lambda_upper, G, upper_c, lb=False))\n",
    "\n",
    "    \n",
    "    loss_graph[1, step] = lower_y.item()\n",
    "\n",
    "    if step == num_steps - 1:\n",
    "        last_lower_loss = lower_y.item()\n",
    "        last_upper_loss = upper_y.item()\n",
    "        last_lower_lambda = lambda_lower.detach().clone().numpy()\n",
    "        last_upper_lambda = lambda_upper.detach().clone().numpy()\n",
    "\n",
    "    opt_lower.zero_grad(set_to_none=True)\n",
    "    opt_upper.zero_grad(set_to_none=True)\n",
    "\n",
    "    lower_y.backward()\n",
    "    upper_y.backward()\n",
    "\n",
    "    opt_lower.step()\n",
    "    opt_upper.step()\n",
    "    scheduler_lower.step()\n",
    "    scheduler_upper.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        lower_c.data = torch.clamp(lower_c.data, min=0.0, max=1.0)\n",
    "        \n",
    "        lambda_upper.data = torch.clamp(lambda_upper.data, max=0.0)\n",
    "        lambda_lower.data = torch.clamp(lambda_lower.data, min=0.0)\n",
    "        lambda_vals[1:3,step] = lambda_lower.detach().clone().numpy().flatten()[:2]\n",
    "        \n",
    "# lambda_lower.data = project_onto_lambda(lambda_lower.data, G, lower_c,0)\n",
    "lambda_lower_optimized = lambda_lower.data\n",
    "lambda_upper_optimized = lambda_upper.data\n",
    "alpha_optimize = lower_c.data\n",
    "\n",
    "print(\"Optimized lambda:\", lambda_lower_optimized, lambda_upper_optimized)\n",
    "print(\"Optimized lower alpha:\", alpha_optimize)\n",
    "\n",
    "print(f\"CROWN lower bound: {obj_fn(lower_x, lower_d, torch.zeros(5,1).float(), G, h, lower_c).squeeze()}, Lagrange lower bound: {obj_fn(lower_x, lower_d, lambda_lower, G, h, lower_c).squeeze()}\")\n",
    "print(f\"CROWN upper bound: {obj_fn(upper_x, upper_d, torch.zeros(5,1).float(), G, h, upper_c).squeeze()}, Lagrange upper bound: {obj_fn(upper_x, upper_d, lambda_upper, G, h, upper_c).squeeze()}\")\n",
    "\n",
    "print(f\"last_lower_loss {last_lower_loss}\\nlast_upper_loss{last_upper_loss}\\nlast_lower_lambda{last_lower_lambda}\\nlast_upper_lambda{last_upper_lambda}\\nlower bound penalth{get_penalty(lambda_lower, G, lower_c, lb=True, scale=1)}\\nupper bound penalty{get_penalty(lambda_upper, G, upper_c, lb=False, scale=1)}\")\n",
    "# assert last_lower_loss <= -7, f\"Last lower loss was {last_lower_loss}\"\n",
    "assert -1*last_upper_loss >= 1.090909091e+01, f\"Last upper loss was {last_upper_loss}\"\n",
    "\n",
    "p_lower_lambda = torch.clamp(project_onto_lambda(lambda_lower, G, lower_c, 0), min=0.0)\n",
    "p_upper_lambda = torch.clamp(project_onto_lambda(lambda_upper, G, upper_c, 0), max=0.0)\n",
    "print(f\"Last projection of lower bound: {p_lower_lambda} with constraint: {constraint(p_lower_lambda, G, lower_c)}\")\n",
    "print(f\"Last projection of upper bound: {p_upper_lambda} with constraint: {constraint(p_upper_lambda, G, upper_c)}\")\n",
    "\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(f\"Optimization Lambda (converged to {loss_graph[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(f\"\\Lambda_1 (converged to {lambda_vals[1,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[1,:])\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(f\"\\Lambda_2 (converged to {lambda_vals[2,-1]:.3f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[2,:])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(f\"\\alpha (converged to {lambda_vals[3,-1]:1.1f})\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], lambda_vals[3,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

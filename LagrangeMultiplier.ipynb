{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking inspiration from this [post](https://stackoverflow.com/questions/77508682/correct-way-to-do-lagrange-dual-optimization-pytorch), we will use the PyTorch Adam Optimizer to solve the Lagrangain dual problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an optimizer on the linear system of equations:\n",
    "$$\n",
    "x_0 + x_1 - 5 = 0 (eq.1)\n",
    "$$\n",
    "$$\n",
    "2x_0 - x_1 + 3 = 0 (eq.2)\n",
    "$$\n",
    "where the loss is defined to be:\n",
    "$$\n",
    "(eq.1)^2 + (eq.2)^2\n",
    "$$\n",
    "and the Adam optimizer is being used to minimize this loss. \n",
    "The exact solutions are \n",
    "$$\n",
    "x_0 = \\frac{2}{3}\n",
    "$$\n",
    "$$\n",
    "x_1 = \\frac{13}{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss: 34.0\n",
      "Step 100, Loss: 20.551959991455078\n",
      "Step 200, Loss: 11.665807723999023\n",
      "Step 300, Loss: 6.137031555175781\n",
      "Step 400, Loss: 2.9643564224243164\n",
      "Step 500, Loss: 1.3036264181137085\n",
      "Step 600, Loss: 0.5183063745498657\n",
      "Step 700, Loss: 0.18529455363750458\n",
      "Step 800, Loss: 0.05931048095226288\n",
      "Step 900, Loss: 0.016934897750616074\n",
      "Step 1000, Loss: 0.004297736566513777\n",
      "Step 1100, Loss: 0.0009654018795117736\n",
      "Step 1200, Loss: 0.00019103451631963253\n",
      "Step 1300, Loss: 3.309983731014654e-05\n",
      "Step 1400, Loss: 4.998842086934019e-06\n",
      "Optimized solution: [0.6665166 4.33259  ]\n"
     ]
    }
   ],
   "source": [
    "# Define your system of equations as a function\n",
    "def equations(x):\n",
    "    eq1 = x[0] + x[1] - 5\n",
    "    eq2 = 2*x[0] - x[1] + 3\n",
    "    return eq1, eq2\n",
    "\n",
    "# Initialize the variables\n",
    "x = torch.tensor([0.,0.], requires_grad=True)\n",
    "\n",
    "# Define the Adam optimizer\n",
    "optimizer = Adam([x], lr=0.01, maximize=False)\n",
    "\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-6\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "    # Compute the system of equations\n",
    "    eq1, eq2 = equations(x)\n",
    "    \n",
    "    # Define the loss as the sum of squared equations\n",
    "    loss = eq1**2 + eq2**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the variables\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an optimizer on the quadratic equation:\n",
    "$$\n",
    "y = x_t^2\n",
    "$$\n",
    "where the loss is defined to be:\n",
    "$$\n",
    "y\n",
    "$$\n",
    "and the Adam optimizer is being used to minimize this loss. \n",
    "The exact solution is\n",
    "$$\n",
    "x_t = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t: tensor([[0.9906]], requires_grad=True)\n",
      "Step 0, Loss: 0.981256365776062\n",
      "Step 100, Loss: 0.04736644774675369\n",
      "Step 200, Loss: 0.00020352439605630934\n",
      "Step 300, Loss: 1.983038089292677e-08\n",
      "Optimized solution: [[9.470147e-05]]\n"
     ]
    }
   ],
   "source": [
    "# test on a simple parabola\n",
    "x_t = torch.rand(1,1, requires_grad=True)\n",
    "print(f\"x_t: {x_t}\")\n",
    "optimizer = torch.optim.Adam([x_t], lr=0.01, maximize=False)\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "\n",
    "    loss = x_t**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x_t.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_t: tensor([[0.2674]], requires_grad=True)\n",
      "Step 0, Loss: -0.07150442898273468\n",
      "Optimized solution: [[7.715425e-06]]\n"
     ]
    }
   ],
   "source": [
    "# test on a simple parabola\n",
    "x_t = torch.rand(1,1, requires_grad=True)\n",
    "print(f\"x_t: {x_t}\")\n",
    "optimizer = torch.optim.Adam([x_t], lr=0.01, maximize=True)\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "\n",
    "    loss = -x_t**2\n",
    "\n",
    "    # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(loss.item()) < loss_threshold:\n",
    "        break\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    step += 1\n",
    "\n",
    "# The optimized values for the variables\n",
    "solution = x_t.detach().numpy()\n",
    "print(\"Optimized solution:\", solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Gurobi will be needed to validate answers later on, here is a simple example using the Gurobi module.\n",
    "\n",
    "minimize $5x + 4y$\n",
    "\n",
    "subject to\n",
    "$$x + y \\geq 8$$\n",
    "$$2x + y \\geq 10$$\n",
    "$$x + 4y \\geq 11$$\n",
    "$$x \\geq 0$$\n",
    "$$y \\geq 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2024-10-28\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 3 rows, 2 columns and 6 nonzeros\n",
      "Model fingerprint: 0x6c7cdc94\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 4e+00]\n",
      "  Objective range  [4e+00, 5e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [8e+00, 1e+01]\n",
      "Presolve time: 0.00s\n",
      "Presolved: 3 rows, 2 columns, 6 nonzeros\n",
      "\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    0.0000000e+00   1.850000e+01   0.000000e+00      0s\n",
      "       2    3.4000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  3.400000000e+01\n",
      "Objective Function Value: 34.000000\n",
      "x: 2\n",
      "y: 6\n"
     ]
    }
   ],
   "source": [
    "from gurobipy import *\n",
    "opt_mod = Model(name = \"simple_linear_program_1\")\n",
    "\n",
    "# add variables\n",
    "x = opt_mod.addVar(name='x', vtype=GRB.CONTINUOUS, lb=0)\n",
    "y = opt_mod.addVar(name='y', vtype=GRB.CONTINUOUS, lb=0)\n",
    "\n",
    "# set the objective function\n",
    "obj_fn = 5*x + 4*y\n",
    "opt_mod.setObjective(obj_fn, GRB.MINIMIZE)\n",
    "\n",
    "# adding the constraints\n",
    "c1 = opt_mod.addConstr(x + y >= 8, name='c1')\n",
    "c2 = opt_mod.addConstr(2*x + y >= 10, name='c2')\n",
    "c3 = opt_mod.addConstr(x + 4*y >= 11, name='c3')\n",
    "\n",
    "# now optimize the problem and save it to a file\n",
    "opt_mod.optimize()\n",
    "opt_mod.write(\"simpe_linear_model_one.lp\")\n",
    "\n",
    "# output the result\n",
    "print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "# Get values of the decision variables\n",
    "for v in opt_mod.getVars():\n",
    "    print('%s: %g' % (v.VarName, v.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to use the Adam optimizer to solve a Lagrangian dual problem. The objective function be to minimized is: \n",
    "$$\n",
    "2x_1 + 4x_2 = 0\n",
    "$$\n",
    "And the constraints are: \n",
    "$$\n",
    "-x_1 - 5 = 0\n",
    "$$\n",
    "$$\n",
    "-x_2 - 5 = 0\n",
    "$$\n",
    "The Lagrangian thus becomes:\n",
    "$$\n",
    "L(x, \\lambda) = 2x_1 + 4x_2 + \\lambda_1(-x_1 - 5) + \\lambda_2(-x_2 - 5)\n",
    "$$\n",
    "\n",
    "For my personal purposes, I do not need to modify $x$, solely $\\lambda$, therefore this is not a dual optimization problem but a simple maximization of $\\lambda$. \n",
    "\n",
    "Thus, assuming $x$ is given and does not violate constraints, we will obtain the gradients:\n",
    "$$\n",
    "\\nabla_{x_1}L(x,\\lambda) = 2 - \\lambda_1 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{x_2}L(x,\\lambda) = 4 - \\lambda_2 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{\\lambda_1}L(x,\\lambda) = -x_1 - 5 = 0\n",
    "$$\n",
    "$$\n",
    "\\nabla_{\\lambda_2}L(x,\\lambda) = -x_2 - 5 = 0\n",
    "$$\n",
    "Giving the exact solutions:\n",
    "$$\n",
    "x_1 = -5\n",
    "$$\n",
    "$$\n",
    "x_2 = -5\n",
    "$$\n",
    "$$\n",
    "\\lambda_1 = 2\n",
    "$$\n",
    "$$\n",
    "\\lambda_2 = 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5. -5.  2.  4.]\n"
     ]
    }
   ],
   "source": [
    "# double checking that these are indeed the exact solutions as I stated above\n",
    "A = np.array([[0,0,-1,0],[0,0,0,-1],[-1,0,0,0],[0,-1,0,0]])\n",
    "b = np.array([-2,-4,5,5])\n",
    "x = np.linalg.solve(A,b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[-1.  0.]\n",
      " [ 0. -1.]]\n",
      "b: [[-5.]\n",
      " [-5.]]\n",
      "c: [[2.]\n",
      " [4.]]\n"
     ]
    }
   ],
   "source": [
    "c = np.array([2,4], dtype='float32').reshape(-1,1)\n",
    "n = 2 # input of dimension 2\n",
    "m = 2 # 2 constraints\n",
    "A = np.array([[-1, 0], [0, -1]], dtype='float32')\n",
    "b = np.array([-5,-5], dtype='float32').reshape(-1, 1)\n",
    "print(f\"A: {A}\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"c: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [[-1.  0.]\n",
      " [ 0. -1.]]\n",
      "b: [[-5.]\n",
      " [-5.]]\n",
      "c: [[2.]\n",
      " [4.]]\n",
      "x_t: [[-5.]\n",
      " [-5.]]\n",
      "Init lagrange_multiplier [[0.873101 ]\n",
      " [0.8700457]]\n",
      "lagrangian shape: torch.Size([])\n",
      "Shape objective torch.Size([1, 1]), Shape constraint torch.Size([2, 1])\n",
      "objective: tensor([[-30.]], grad_fn=<MmBackward0>)\n",
      "constraint: tensor([[0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n",
      "lagrangian: -30.0\n",
      "Step 0, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 1900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 2900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 3900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 4900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 5900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 6900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 7900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 8900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 9900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 10900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 11900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 12900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 13900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 14900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 15900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 16900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 17900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 18900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 19900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 20900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 21900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 22900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 23900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 24900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 25900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 26900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 27900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 28900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 29900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 30900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 31900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 32900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 33900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 34900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 35900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 36900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 37900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 38900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 39900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 40900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 41900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 42900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 43900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 44900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 45900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 46900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 47900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 48900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 49900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 50900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 51900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 52900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 53900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 54900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 55900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 56900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 57900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 58900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 59900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 60900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 61900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 62900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 63900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 64900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 65900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 66900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 67900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 68900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 69900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 70900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 71900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 72900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 73900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 74900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 75900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 76900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 77900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 78900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 79900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 80900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 81900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 82900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 83900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 84900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 85900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 86900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 87900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 88900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 89900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 90900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 91900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 92900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 93900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 94900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 95900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 96900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 97900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 98900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 99900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 100900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 101900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 102900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 103900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 104900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 105900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 106900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 107900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 108900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 109900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 110900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 111900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 112900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 113900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 114900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 115900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 116900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 117900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 118900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 119900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 120900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 121900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 122900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 123900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 124900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 125900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 126900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 127900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 128900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 129900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 130900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 131900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 132900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 133900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 134900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 135900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 136900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 137900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 138900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 139900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 140900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 141900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 142900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 143900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 144900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 145900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 146900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 147900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 148900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 149900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 150900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 151900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 152900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 153900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 154900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 155900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 156900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 157900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 158900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 159900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 160900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 161900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 162900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 163900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 164900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 165900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 166900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 167900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 168900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 169900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 170900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 171900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 172900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 173900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 174900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 175900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 176900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 177900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 178900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 179900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 180900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 181900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 182900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 183900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 184900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 185900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 186900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 187900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 188900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 189900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 190900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 191900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 192900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 193900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 194900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 195900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 196900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 197900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 198900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 199900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 200900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 201900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 202900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 203900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 204900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 205900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 206900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 207900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 208900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 209900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 210900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 211900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 212900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 213900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 214900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 215900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 216900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 217900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 218900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 219900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 220900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 221900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 222900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 223900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 224900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 225900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 226900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 227900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 228900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 229900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 230900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 231900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 232900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 233900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 234900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 235900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 236900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 237900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 238900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 239900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 240900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 241900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 242900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 243900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 244900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 245900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 246900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 247900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 248900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 249900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 250900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 251900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 252900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 253900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 254900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 255900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 256900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 257900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 258900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 259900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 260900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 261900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 262900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 263900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 264900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 265900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 266900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 267900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 268900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 269900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 270900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 271900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 272900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 273900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 274900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 275900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 276900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 277900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 278900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 279900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 280900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 281900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 282900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 283900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 284900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 285900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 286900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 287900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 288900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 289900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 290900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 291900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 292900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 293900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 294900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 295900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 296900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 297900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 298900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 299900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 300900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 301900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 302900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 303900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 304900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 305900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 306900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 307900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 308900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 309900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 310900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 311900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 312900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 313900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 314900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 315900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 316900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 317900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 318900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 319900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 320900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 321900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 322900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 323900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 324900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 325900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 326900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 327900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 328900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 329900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 330900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 331900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 332900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 333900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 334900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 335900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 336900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 337900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 338900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339100, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339200, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339300, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339400, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339500, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339600, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339700, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339800, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 339900, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n",
      "Step 340000, Loss: -30.0, lambda_1: tensor([0.3324], grad_fn=<SelectBackward0>), lambda_2: tensor([0.3272], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m lagrangian\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# update values\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[43mopt_lagrange\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c_t = torch.tensor(c).float()\n",
    "A_t = torch.tensor(A).float()\n",
    "b_t = torch.tensor(b).float().reshape(-1,1)\n",
    "# x_t = torch.rand(n, 1, requires_grad=True)\n",
    "x_t = torch.tensor([-5.0,-5.0], requires_grad=True).float().reshape(-1,1)\n",
    "print(f\"A: {A_t.detach().numpy()}\")\n",
    "print(f\"b: {b_t.detach().numpy()}\")\n",
    "print(f\"c: {c_t.detach().numpy()}\")\n",
    "print(f\"x_t: {x_t.detach().numpy()}\")\n",
    "\n",
    "_lagrange_multiplier = torch.rand(m,1, requires_grad=True)\n",
    "lagrange_multiplier = torch.nn.functional.softplus(_lagrange_multiplier)\n",
    "print(f\"Init lagrange_multiplier {lagrange_multiplier.detach().numpy()}\")\n",
    "\n",
    "# opt_weights = torch.optim.Adam([x_t], lr=0.1)\n",
    "opt_lagrange = torch.optim.Adam([_lagrange_multiplier], lr=0.1, maximize=True)\n",
    "\n",
    "# Set a threshold for the loss\n",
    "loss_threshold = 1e-8\n",
    "\n",
    "# Optimization loop\n",
    "step = 0\n",
    "while True:\n",
    "    \n",
    "\n",
    "    objective = c_t.T @ x_t\n",
    "    constraint = A_t @ x_t + b_t\n",
    "    \n",
    "    lagrange_multiplier = torch.nn.functional.softplus(_lagrange_multiplier)\n",
    "    lagrangian = objective + lagrange_multiplier.T @ constraint\n",
    "    lagrangian = lagrangian.squeeze()\n",
    "    if step == 0:\n",
    "        print(f\"lagrangian shape: {lagrangian.shape}\")\n",
    "        print(f\"Shape objective {objective.shape}, Shape constraint {constraint.shape}\")\n",
    "        print(f\"objective: {objective}\")\n",
    "        print(f\"constraint: {constraint}\")\n",
    "        print(f\"lagrangian: {lagrangian}\")\n",
    "\n",
    "        # Print the loss at each step\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}, Loss: {lagrangian.item()}, lambda_1: {_lagrange_multiplier[0]}, lambda_2: {_lagrange_multiplier[1]}\")\n",
    "\n",
    "    # Check if the loss is below the threshold\n",
    "    if abs(lagrangian.item()) < loss_threshold:\n",
    "        break\n",
    "        \n",
    "    # zero the gradient\n",
    "    opt_lagrange.zero_grad()\n",
    "\n",
    "    # compute the gradient\n",
    "    lagrangian.backward()\n",
    "\n",
    "    # update values\n",
    "    opt_lagrange.step()\n",
    "\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: -5.0 is leaf True, x2: -5.0 is leaf True, x3: tensor([0.0566], requires_grad=True) is leaf True, x4: tensor([0.8357], requires_grad=True) is leaf True\n",
      "Optimized x1: -5.0\n",
      "Optimized x2: -5.0\n",
      "Optimized x3: 0.05660974979400635\n",
      "Optimized x4: 0.835723876953125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc9e002a050>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Adam\n",
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.tensor(-5.0, requires_grad=False)\n",
    "x2_opt = torch.tensor(-5.0, requires_grad=False)\n",
    "x3_opt = torch.rand(1, requires_grad=True)\n",
    "x4_opt = torch.rand(1, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 1000\n",
    "lr = 0.1\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "opt = torch.optim.Adam([x3_opt, x4_opt], lr=lr, maximize=True)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, 0.98)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt).sum()\n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    y.backward()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    # clip lagrange values\n",
    "    x3_opt.data = torch.clip(x3_opt.data, 0)\n",
    "    x4_opt.data = torch.clip(x4_opt.data, 0)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([0.6116], dtype=torch.float64, requires_grad=True) is leaf True, x2: tensor([0.0295], dtype=torch.float64, requires_grad=True) is leaf True, x3: tensor([0.1955], dtype=torch.float64, requires_grad=True) is leaf True, x4: tensor([0.9048], dtype=torch.float64, requires_grad=True) is leaf True\n",
      "Optimized x1: -4.55081800782109\n",
      "Optimized x2: -5.225714517952417\n",
      "Optimized x3: 0.0\n",
      "Optimized x4: 0.29730698177256015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fda17360160>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using Adam\n",
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x2_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x3_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "x4_opt = torch.rand(1,dtype=torch.float64, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 10000 + 20\n",
    "lr = 0.1\n",
    "\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "opt_duals = torch.optim.Adam([x3_opt, x4_opt], lr=lr, maximize=True)\n",
    "opt_x = torch.optim.Adadelta([x1_opt, x2_opt], lr=lr, maximize=False)\n",
    "scheduler_dual = torch.optim.lr_scheduler.ExponentialLR(opt_duals, 0.98)\n",
    "scheduler_x = torch.optim.lr_scheduler.ExponentialLR(opt_x, 0.98)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt).sum()\n",
    "    loss_graph[1, step] = y.item()\n",
    "\n",
    "    opt_x.zero_grad(set_to_none=True)\n",
    "    opt_duals.zero_grad(set_to_none=True)\n",
    "    y.backward()\n",
    "\n",
    "    if flip:\n",
    "        opt_x.step()\n",
    "        # if step % 20 == 0:\n",
    "        #     scheduler_x.step()\n",
    "    else:\n",
    "        opt_duals.step()\n",
    "        # if step % 20 == 0:\n",
    "        #     scheduler_dual.step()\n",
    "    \n",
    "    # clip lagrange values\n",
    "    x3_opt.data = torch.clip(x3_opt.data, 0)\n",
    "    x4_opt.data = torch.clip(x4_opt.data, 0)\n",
    "\n",
    "    if step != 0 and step % 60 == 0:\n",
    "        flip = not flip\n",
    "    \n",
    "    if loss_graph[1, step] < -29.5 and loss_graph[1,step] > -30.5 and flip == False:\n",
    "        loss_graph[1, step:] = loss_graph[1,step]\n",
    "        break\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([0.7194], requires_grad=True) is leaf True, x2: tensor([0.2687], requires_grad=True) is leaf True, x3: tensor([0.7701], requires_grad=True) is leaf True, x4: tensor([0.8334], requires_grad=True) is leaf True\n",
      "Optimized x1: -4.999998092651367\n",
      "Optimized x2: -4.999998092651367\n",
      "Optimized x3: 1.9999995231628418\n",
      "Optimized x4: 3.9999990463256836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdffc1509d0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1, requires_grad=True)\n",
    "x2_opt = torch.rand(1, requires_grad=True)\n",
    "x3_opt = torch.rand(1, requires_grad=True)\n",
    "x4_opt = torch.rand(1, requires_grad=True)\n",
    "print(f\"x1: {x1_opt} is leaf {x1_opt.is_leaf}, x2: {x2_opt} is leaf {x2_opt.is_leaf}, x3: {x3_opt} is leaf {x3_opt.is_leaf}, x4: {x4_opt} is leaf {x4_opt.is_leaf}\")\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, x3, x4):\n",
    "    return 2*x1 + 4*x2 + x3*(-x1 - 5) + x4*(-x2 - 5)\n",
    "\n",
    "def zero_grad(parameters):\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 1000\n",
    "lr = 0.1\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, x3_opt, x4_opt)\n",
    "    y.backward()\n",
    "\n",
    "    grad1 = x1_opt.grad if x1_opt.grad is not None else 0.0\n",
    "    grad2 = x2_opt.grad if x2_opt.grad is not None else 0.0\n",
    "    grad3 = x3_opt.grad if x3_opt.grad is not None else 0.0\n",
    "    grad4 = x4_opt.grad if x4_opt.grad is not None else 0.0\n",
    "\n",
    "    loss_graph[1, step] = y.item()\n",
    "    \n",
    "    if flip:\n",
    "        # when this is true, we are minimizing L(x,lambda) w.r.t. x\n",
    "        x1_opt.data = (x1_opt.data + lr*grad3).requires_grad_(True)\n",
    "        x2_opt.data = (x2_opt.data + lr*grad4).requires_grad_(True)        \n",
    "\n",
    "    else:\n",
    "        # when this is false, we are maximizing L(x,lambda) w.r.t. lambda\n",
    "        x3_opt.data = torch.clamp(x3_opt.data + lr*grad1, min=0.0).requires_grad_(True)\n",
    "        x4_opt.data = torch.clamp(x4_opt.data + lr*grad2, min=0.0).requires_grad_(True)\n",
    "\n",
    "    # if step != 0 and (step % 100) == 0:\n",
    "    #     print(f\"Step {step}, Loss: {y.item():.4f}, x1: {x1_opt.detach().numpy()[0]:.4f} x2: {x2_opt.detach().numpy()[0]:.4f} lambda_1: {x3_opt.detach().numpy()[0]:.4f}, lambda_2: {x4_opt.detach().numpy()[0]:.4f}, grads: [{x1_opt.grad.detach().numpy()[0]:.4f}, {x2_opt.grad.detach().numpy()[0]:.4f}, {x3_opt.grad.detach().numpy()[0]:.4f}, {x4_opt.grad.detach().numpy()[0]:.4f}]\")\n",
    "\n",
    "    if step != 0 and (step % 100) == 0:\n",
    "        flip = not flip\n",
    "        \n",
    "    # zero out the gradients\n",
    "    zero_grad([x1_opt, x2_opt, x3_opt, x4_opt])\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "x3_optimized = x3_opt.item()\n",
    "x4_optimized = x4_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized x3:\", x3_optimized)\n",
    "print(\"Optimized x4:\", x4_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda (should converge to -30)\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized x1: -0.8333324193954468\n",
      "Optimized x2: -0.8333352208137512\n",
      "Optimized l1: 1.045896053314209\n",
      "Optimized l2: 0.2871112823486328\n",
      "Optimized l3: 0.0\n",
      "Optimized l4: 1.4137334823608398\n",
      "Optimized l5: 0.5862621068954468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdce8c83550>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the parameters you want to optimize\n",
    "x1_opt = torch.rand(1, requires_grad=True)\n",
    "x2_opt = torch.rand(1, requires_grad=True)\n",
    "l1_opt = torch.rand(1, requires_grad=True)\n",
    "l2_opt = torch.rand(1, requires_grad=True)\n",
    "l3_opt = torch.rand(1, requires_grad=True)\n",
    "l4_opt = torch.rand(1, requires_grad=True)\n",
    "l5_opt = torch.rand(1, requires_grad=True)\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(x1, x2, l1, l2, l3, l4, l5):\n",
    "    return x1 + 2*x2 + l1*(x1-2) + l2*(-x1-2) + l3*(x2-2) + l4*(-x2-2) + l5*(-3*x1 - x2 - 5)\n",
    "\n",
    "def zero_grad(parameters):\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "# Number of optimization steps\n",
    "num_steps = 10000\n",
    "lr = 0.01\n",
    "flip = True\n",
    "\n",
    "loss_graph = np.array([i for i in range(num_steps)])\n",
    "loss_graph = np.vstack((loss_graph, np.zeros(num_steps)))\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Compute the objective function\n",
    "    y = objective_function(x1_opt, x2_opt, l1_opt, l2_opt, l3_opt, l4_opt, l5_opt)\n",
    "    y.backward()\n",
    "\n",
    "    x_grad1 = x1_opt.grad if x1_opt.grad is not None else 0.0\n",
    "    x_grad2 = x2_opt.grad if x2_opt.grad is not None else 0.0\n",
    "    l_grad1 = l1_opt.grad if l1_opt.grad is not None else 0.0\n",
    "    l_grad2 = l2_opt.grad if l2_opt.grad is not None else 0.0\n",
    "    l_grad3 = l3_opt.grad if l3_opt.grad is not None else 0.0\n",
    "    l_grad4 = l4_opt.grad if l4_opt.grad is not None else 0.0\n",
    "    l_grad5 = l5_opt.grad if l5_opt.grad is not None else 0.0\n",
    "\n",
    "    loss_graph[1, step] = y.item()\n",
    "    \n",
    "    if flip:\n",
    "        # when this is true, we are minimizing L(x,lambda) w.r.t. x\n",
    "        x1_opt.data = (x1_opt.data + lr*(-l_grad1 + l_grad2 + l_grad5)).requires_grad_(True)\n",
    "        x2_opt.data = (x2_opt.data + lr*(-l_grad3 + l_grad4 + l_grad5)).requires_grad_(True)        \n",
    "\n",
    "    else:\n",
    "        # when this is false, we are maximizing L(x,lambda) w.r.t. lambda\n",
    "        l1_opt.data = torch.clamp(l1_opt.data + lr*(-x_grad1), min=0.0).requires_grad_(True)\n",
    "        l2_opt.data = torch.clamp(l2_opt.data + lr*(x_grad1), min=0.0).requires_grad_(True)\n",
    "        l3_opt.data = torch.clamp(l3_opt.data + lr*(-x_grad2), min=0.0).requires_grad_(True)\n",
    "        l4_opt.data = torch.clamp(l4_opt.data + lr*(x_grad2), min=0.0).requires_grad_(True)\n",
    "        l5_opt.data = torch.clamp(l5_opt.data + lr*(x_grad1 + x_grad2), min=0.0).requires_grad_(True)\n",
    "\n",
    "    if step != 0 and (step % 100) == 0:\n",
    "        flip = not flip\n",
    "        \n",
    "    # zero out the gradients\n",
    "    zero_grad([x1_opt, x2_opt, l1_opt, l2_opt, l3_opt, l4_opt, l5_opt])\n",
    "\n",
    "# The optimized values for x3 and x4\n",
    "x1_optimized = x1_opt.item()\n",
    "x2_optimized = x2_opt.item()\n",
    "l1_optimized = l1_opt.item()\n",
    "l2_optimized = l2_opt.item()\n",
    "l3_optimized = l3_opt.item()\n",
    "l4_optimized = l4_opt.item()\n",
    "l5_optimized = l5_opt.item()\n",
    "\n",
    "print(\"Optimized x1:\", x1_optimized)\n",
    "print(\"Optimized x2:\", x2_optimized)\n",
    "print(\"Optimized l1:\", l1_optimized)\n",
    "print(\"Optimized l2:\", l2_optimized)\n",
    "print(\"Optimized l3:\", l3_optimized)\n",
    "print(\"Optimized l4:\", l4_optimized)\n",
    "print(\"Optimized l5:\", l5_optimized)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title(\"Dual Optimization of x and lambda\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"L(x,lambda)\")\n",
    "plt.plot(loss_graph[0,:], loss_graph[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am checking the results of the Lagrange problem above by computing the upper and lower bounds on x as well as the minimal and maximal pertubation on each logit output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Model fingerprint: 0xf5f961a3\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [3e+00, 4e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Presolve removed 5 rows and 2 columns\n",
      "Presolve time: 0.00s\n",
      "Presolve: All rows and columns removed\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    1.6000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 0 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.600000000e+01\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 2e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -3.0000000e+30   3.000000e+30   3.000000e+00      0s\n",
      "       3   -5.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 3 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -5.000000000e+00\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [3e+00, 4e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    7.0000000e+30   2.000000e+30   7.000000e+00      0s\n",
      "       2    1.6000000e+01   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 2 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective  1.600000000e+01\n",
      "Gurobi Optimizer version 10.0.3 build v10.0.3rc0 (linux64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 20 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 5 rows, 2 columns and 6 nonzeros\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 3e+00]\n",
      "  Objective range  [1e+00, 3e+00]\n",
      "  Bounds range     [0e+00, 0e+00]\n",
      "  RHS range        [2e+00, 5e+00]\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0   -4.0000000e+30   3.000000e+30   4.000000e+00      0s\n",
      "       3   -5.0000000e+00   0.000000e+00   0.000000e+00      0s\n",
      "\n",
      "Solved in 3 iterations and 0.00 seconds (0.00 work units)\n",
      "Optimal objective -5.000000000e+00\n",
      "logit 1 is bounded s.t. -5.0 <= z(x) <= 16.0 with lb pertubation (-1.0, -2.0) and ub pertubation (2.0, 2.0)\n",
      "logit 2 is bounded s.t. -5.0 <= z(x) <= 16.0 with lb pertubation (-1.0, -2.0) and ub pertubation (2.0, 2.0)\n"
     ]
    }
   ],
   "source": [
    "W_ub = np.array([[4,3],[4,3]])\n",
    "b_ub = np.array([2,2])\n",
    "W_lb = np.array([[1,2],[1,1]])\n",
    "# b_lb = np.array([1,1])\n",
    "b_lb = np.zeros(2)\n",
    "# using Gurobi to solve the same problem as above\n",
    "opt_mod = Model(name = \"simple_linear_program_2\")\n",
    "\n",
    "# add variables\n",
    "inputs = np.array(list(opt_mod.addVars(W_ub.shape[1], name=\"x\", lb=float(\"-inf\"), ub=float(\"inf\")).values()))\n",
    "\n",
    "# adding the constraints\n",
    "c1 = opt_mod.addConstr(inputs[0] - 2 <= 0, name='c1') # these four constraints are the l_inf norm box constraints\n",
    "c2 = opt_mod.addConstr(-inputs[0] - 2 <= 0, name='c2')\n",
    "c3 = opt_mod.addConstr(inputs[1] - 2 <= 0, name='c3')\n",
    "c4 = opt_mod.addConstr(-inputs[1] - 2 <= 0, name='c4')\n",
    "c5 = opt_mod.addConstr(-3*inputs[0] - inputs[1] - 5 <= 0, name='c5') # this constraint is a line constraint cutting through the box\n",
    "\n",
    "worst_case_inputs_ub = []\n",
    "worst_case_inputs_lb = []\n",
    "upper_bounds = []\n",
    "lower_bounds = []\n",
    "\n",
    "# set the objective function for each logit\n",
    "for idx in range(2*W_ub.shape[0]):\n",
    "    i = idx // 2\n",
    "    if idx % 2 == 0:\n",
    "        obj_fn = quicksum([W_ub[i,j]*inputs[j] for j in range(W_ub.shape[1])]) + b_ub[i]\n",
    "        opt_mod.setObjective(obj_fn, GRB.MAXIMIZE)\n",
    "    else:\n",
    "        obj_fn = quicksum([W_lb[i,j]*inputs[j] for j in range(W_lb.shape[1])]) + b_lb[i]\n",
    "        opt_mod.setObjective(obj_fn, GRB.MINIMIZE)\n",
    "\n",
    "    # now optimize the problem and save it to a file\n",
    "    opt_mod.optimize()\n",
    "    # opt_mod.write(\"scenario_one_upperbound_logit_one.lp\")\n",
    "\n",
    "    # output the result\n",
    "    # print('Objective Function Value: %f' % opt_mod.ObjVal)\n",
    "    if idx % 2 == 0:\n",
    "        upper_bounds.append(opt_mod.ObjVal)\n",
    "    else:\n",
    "        lower_bounds.append(opt_mod.ObjVal)\n",
    "    # Get values of the decision variables\n",
    "    temp_inputs = []\n",
    "    for v in opt_mod.getVars():\n",
    "        # print('%s: %g' % (v.VarName, v.x))\n",
    "        temp_inputs.append(v.x)\n",
    "\n",
    "    if idx % 2 == 0:\n",
    "        worst_case_inputs_ub.append(tuple(temp_inputs)) # append the worst case input that caused this maximal pertubation\n",
    "    else:\n",
    "        worst_case_inputs_lb.append(tuple(temp_inputs)) # append the worst case input that caused this maximal pertubation\n",
    "    \n",
    "for i in range(W_ub.shape[0]):\n",
    "    print(f\"logit {i + 1} is bounded s.t. {lower_bounds[i]} <= z(x) <= {upper_bounds[i]} with lb pertubation {worst_case_inputs_lb[i]} and ub pertubation {worst_case_inputs_ub[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primal Result:\n",
      "       message: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n",
      "       success: True\n",
      "        status: 0\n",
      "           fun: 0.0\n",
      "             x: [ 0.000e+00  0.000e+00]\n",
      "           nit: 0\n",
      "         lower:  residual: [ 0.000e+00  0.000e+00]\n",
      "                marginals: [ 2.000e+00  3.000e+00]\n",
      "         upper:  residual: [       inf        inf]\n",
      "                marginals: [ 0.000e+00  0.000e+00]\n",
      "         eqlin:  residual: []\n",
      "                marginals: []\n",
      "       ineqlin:  residual: [ 1.000e+00  2.000e+00]\n",
      "                marginals: [-0.000e+00 -0.000e+00]\n",
      "\n",
      "Dual Result:\n",
      "       message: The problem is unbounded. (HiGHS Status 10: model_status is Unbounded; primal_status is At upper bound)\n",
      "       success: False\n",
      "        status: 3\n",
      "           fun: None\n",
      "             x: None\n",
      "           nit: 0\n",
      "         lower:  residual: None\n",
      "                marginals: None\n",
      "         upper:  residual: None\n",
      "                marginals: None\n",
      "         eqlin:  residual: None\n",
      "                marginals: None\n",
      "       ineqlin:  residual: None\n",
      "                marginals: None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# Primal linear program coefficients\n",
    "c = np.array([2, 3])\n",
    "A = np.array([[1, -1], [3, 1]])\n",
    "b = np.array([1, 2])\n",
    "\n",
    "# Solve the primal linear program\n",
    "result_primal = linprog(c, A_ub=A, b_ub=b, method='highs')\n",
    "\n",
    "# Display the primal result\n",
    "print(\"Primal Result:\")\n",
    "print(result_primal)\n",
    "\n",
    "# Dual linear program coefficients\n",
    "c_dual = -b  # Coefficients are negated for maximization\n",
    "A_dual = -A.T  # Transpose of A with negation\n",
    "b_dual = c  # Dual variables corresponding to the inequality constraints\n",
    "\n",
    "# Solve the dual linear program\n",
    "result_dual = linprog(c_dual, A_ub=A_dual, b_ub=b_dual, method='highs')\n",
    "\n",
    "# Display the dual result\n",
    "print(\"\\nDual Result:\")\n",
    "print(result_dual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [1.25 0.  ]\n",
      " [2.5  0.  ]\n",
      " [3.75 0.  ]\n",
      " [5.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# potentially look into this for solving the lagrange dual, but it requires the gradients to be determined beforehand\n",
    "from scipy.optimize import fsolve, fmin_l_bfgs_b\n",
    "\n",
    "a = 1\n",
    "nbtests = 5\n",
    "minmu = 0\n",
    "maxmu = 5\n",
    "\n",
    "def lagrange(x, mu):\n",
    "    return x**2 + mu * (np.exp(x) + x - a)\n",
    "\n",
    "def lagrange_grad(x, mu):\n",
    "    grad_x = 2*x + mu * (np.exp(x) + 1)\n",
    "    grad_mu = np.exp(x) + x - a\n",
    "    return grad_x, grad_mu\n",
    "\n",
    "def dual(mu):\n",
    "    x = fsolve(lambda x: lagrange_grad(x, mu)[0], x0=1)\n",
    "    obj_val = lagrange(x, mu)\n",
    "    grad = lagrange_grad(x, mu)[1]\n",
    "    return -1.0*obj_val, -1.0*grad\n",
    "\n",
    "pl = np.empty((nbtests, 2))\n",
    "for i, nu in enumerate(np.linspace(minmu,maxmu,nbtests)):\n",
    "    res = fmin_l_bfgs_b(dual, x0=nu, bounds=[(0,None)], factr=1e6)\n",
    "    mu_opt = res[0]\n",
    "    x_opt = fsolve(lambda x: lagrange_grad(x, mu_opt)[0], x0=1)\n",
    "    pl[i] = [nu, *x_opt]\n",
    "print(pl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

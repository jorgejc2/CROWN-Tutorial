{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from gurobipy import GRB, quicksum, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagrangian Dual of LP with Bound Constraints\n",
    "The linear program that will be tackled is of the form:\n",
    "\n",
    "minimize $c^Tx$\n",
    "\n",
    "subject to $Ax\\geq b,-e\\leq x \\leq e, (||x||_\\infty \\leq 1)$\n",
    "\n",
    "The problem to solve is: \n",
    "\n",
    "$\\phi(\\lambda) = \\inf_{-e\\leq x \\leq e} L(x, \\lambda) = \\inf_{-e \\leq x \\leq e} [(c - A^T \\lambda )^T x + b^T \\lambda ]$\n",
    "\n",
    "In the equation above, if $(c - A^T\\lambda)_j \\leq 0, x_j = 1$, otherwise $x_j = -1$, where j denotes the jth inequality constraint. \n",
    "\n",
    "Therefore the Lagrangian dual is:\n",
    "\n",
    "maximize $b^T\\lambda - ||c - A^T\\lambda||_1$\n",
    "\n",
    "subject to $\\lambda \\in \\mathbb{R}^m$\n",
    "\n",
    "For the examples below, we will use the above setup including the bounds on x. To obtain the optimal Lagrange multipliers, we use the PyTorch Adam optimizer to maximimize the Lagrangian dual, and then obtain the lower bound by plugging in these multipliers into the dual objective at the end. \n",
    "\n",
    "The only difference in each example is the inequality constraints. \n",
    "\n",
    "Our variables are defined as: \n",
    "\n",
    "$$x = \\begin{pmatrix}x_1\\\\x_2\\end{pmatrix}$$\n",
    "$$c \\begin{pmatrix}1\\\\1\\end{pmatrix}$$\n",
    "$$\\lambda \\in \\mathbb{R}^1$$\n",
    "$$A \\in \\mathbb{R}^{1x2}$$\n",
    "$$b \\in \\mathbb{R}^1$$\n",
    "\n",
    "Naturally, this means the objective function to minimize is $x_1 + x_2$ subject to some constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langrange_dual_fn (e_x, A, c, b, lambda_):\n",
    "    x = torch.abs(e_x).float() # ensure x is positive\n",
    "    return -1*torch.abs((c - A.T*lambda_).T@x) + b.T*lambda_\n",
    "\n",
    "def maximizing_fn (A, b, c, lambda_):\n",
    "    return b.T @ lambda_ - torch.norm(c-A.T*lambda_, p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 100\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "$-x_1 - x_2 \\leq 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized lambda: \n",
      "tensor([[0.9961]])\n",
      "Lower bound: \n",
      "tensor([[-0.5059]])\n"
     ]
    }
   ],
   "source": [
    "A_t = torch.tensor([[1.,1.]])\n",
    "c_t = torch.tensor([[1.],[1.]])\n",
    "b_t = torch.tensor([[-0.5]])\n",
    "x_t = torch.tensor([[1.],[1.]])\n",
    "lambda_ = torch.rand(1,1, requires_grad=True)\n",
    "\n",
    "opt = torch.optim.Adam([lambda_], lr=lr, maximize=True)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    y = maximizing_fn(A_t, b_t, c_t, lambda_)\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    y.backward()\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lambda_.data = torch.clamp(lambda_.data, min=0.0)\n",
    "\n",
    "lambda_optimized = lambda_.data\n",
    "print(f\"Optimized lambda: \\n{lambda_.data}\")\n",
    "print(f\"Lower bound: \\n{langrange_dual_fn(x_t, A_t, c_t, b_t, lambda_).data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "$-x_1 - x_2 \\geq 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized lambda: \n",
      "tensor([[0.]])\n",
      "Lower bound: \n",
      "tensor([[-2.]])\n"
     ]
    }
   ],
   "source": [
    "A_t = torch.tensor([[-1.,-1.]])\n",
    "c_t = torch.tensor([[1.],[1.]])\n",
    "b_t = torch.tensor([[0.5]])\n",
    "x_t = torch.tensor([[1.],[1.]])\n",
    "lambda_ = torch.rand(1,1, requires_grad=True)\n",
    "\n",
    "opt = torch.optim.Adam([lambda_], lr=lr, maximize=True)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    y = maximizing_fn(A_t, b_t, c_t, lambda_)\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    y.backward()\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lambda_.data = torch.clamp(lambda_.data, min=0.0)\n",
    "\n",
    "lambda_optimized = lambda_.data\n",
    "print(f\"Optimized lambda: \\n{lambda_.data}\")\n",
    "print(f\"Lower bound: \\n{langrange_dual_fn(x_t, A_t, c_t, b_t, lambda_).data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "$x_1 + x_2 \\geq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized lambda: \n",
      "tensor([[0.9874]])\n",
      "Lower bound: \n",
      "tensor([[-0.0251]])\n"
     ]
    }
   ],
   "source": [
    "A_t = torch.tensor([[1.,1.]])\n",
    "c_t = torch.tensor([[1.],[1.]])\n",
    "b_t = torch.tensor([[0.]])\n",
    "x_t = torch.tensor([[1.],[1.]])\n",
    "lambda_ = torch.rand(1,1, requires_grad=True)\n",
    "\n",
    "opt = torch.optim.Adam([lambda_], lr=lr, maximize=True)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    y = maximizing_fn(A_t, b_t, c_t, lambda_)\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    y.backward()\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lambda_.data = torch.clamp(lambda_.data, min=0.0)\n",
    "\n",
    "lambda_optimized = lambda_.data\n",
    "print(f\"Optimized lambda: \\n{lambda_.data}\")\n",
    "print(f\"Lower bound: \\n{langrange_dual_fn(x_t, A_t, c_t, b_t, lambda_).data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4\n",
    "$x_1 + x_2 \\geq 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized lambda: \n",
      "tensor([[1.0078]])\n",
      "Lower bound: \n",
      "tensor([[0.9922]])\n"
     ]
    }
   ],
   "source": [
    "A_t = torch.tensor([[1.,1.]])\n",
    "c_t = torch.tensor([[1.],[1.]])\n",
    "b_t = torch.tensor([[1.]])\n",
    "x_t = torch.tensor([[1.],[1.]])\n",
    "lambda_ = torch.rand(1,1, requires_grad=True)\n",
    "\n",
    "opt = torch.optim.Adam([lambda_], lr=lr, maximize=True)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    y = maximizing_fn(A_t, b_t, c_t, lambda_)\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    y.backward()\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lambda_.data = torch.clamp(lambda_.data, min=0.0)\n",
    "\n",
    "lambda_optimized = lambda_.data\n",
    "print(f\"Optimized lambda: \\n{lambda_.data}\")\n",
    "print(f\"Lower bound: \\n{langrange_dual_fn(x_t, A_t, c_t, b_t, lambda_).data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5\n",
    "$x_1 + x_2 \\geq -1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized lambda: \n",
      "tensor([[0.9983]])\n",
      "Lower bound: \n",
      "tensor([[-1.0017]])\n"
     ]
    }
   ],
   "source": [
    "A_t = torch.tensor([[1.,1.]])\n",
    "c_t = torch.tensor([[1.],[1.]])\n",
    "b_t = torch.tensor([[-1.]])\n",
    "x_t = torch.tensor([[1.],[1.]])\n",
    "lambda_ = torch.rand(1,1, requires_grad=True)\n",
    "\n",
    "opt = torch.optim.Adam([lambda_], lr=lr, maximize=True)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    y = maximizing_fn(A_t, b_t, c_t, lambda_)\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    y.backward()\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lambda_.data = torch.clamp(lambda_.data, min=0.0)\n",
    "\n",
    "lambda_optimized = lambda_.data\n",
    "print(f\"Optimized lambda: \\n{lambda_.data}\")\n",
    "print(f\"Lower bound: \\n{langrange_dual_fn(x_t, A_t, c_t, b_t, lambda_).data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
